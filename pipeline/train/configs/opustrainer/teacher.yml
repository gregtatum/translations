# Train a teacher.

datasets:
  original: {dataset0} # Original parallel corpus
  backtranslated: {dataset1} # Back-translated data


# In train.py, one of these schedules will be used to configure the final OpusTrainer
# config file.
curriculum:
  # Train on a mix until early stopping
  # (useful for clean back-translated data produced by a strong model)
  one-stage:
    stages:
      - train
    train:
      - original 0.7
      - backtranslated 0.3
      - until original inf
  
  # Train in two stages. The first stage is using a mix of back translated data and
  # the original parallel data. Then after 2 epochs, switch to only the original corpus.
  two-stage:
    stages:
      - pretrain
      - finetune

    # Train until the model sees two epochs of back-translated corpus
    pretrain:
      - original 0.6
      - backtranslated 0.4
      - until backtranslated 2

    # Fine-tuning only on original clean corpus until the early stopping
    finetune:
      - original 1.0
      - until original inf


# The pipeline/train/train.py file will configure the modifiers based on the language's
# scripts and what features they support using the values below.
modifiers:
  # The common modifiers are used in every script type.
  common:
    # Insert new sentences composed form Unicode noise
    - Noise: 0.0005
      min_word_length: 2 # Minimum word length for each word in the noisy sentence
      max_word_length: 5 # Maximum word length for each word in the noisy sentence
      max_words: 6 # Maximum number of words in each noisy sentence

    # Generate inline noise (emojis etc.) matching positions in source and target sentences using alignments
    # no spm_vocab argument -> alignments will be removed from Marian input
    # we don't use alignments for teacher training
    # Tags modifier has to be the last one to remove the alignments
    - Tags: 0.005
      custom_detok_src: "icu:{src}"
      custom_detok_trg: "icu:{trg}"
      augment: 1
      tag: 0

  # The casing modifiers work on both the src and trg sentence. However, we only want to
  # apply the casing modifiers if the src language's script supports casing. This is
  # because the model would be confused on arbitrary trg casing when the src side gives
  # no indication that the casing should be different.
  bicameral_src:
    # boost upper case a little as we see that the models underperform on upper case dataset on evaluation
    - UpperCase: 0.07 # Apply randomly to 7% of sentences
    - TitleCase: 0.05

  # A Phonemic is one that doesn't use logographic features such as ideographs. Hangul
  # is excluded here due to it's featural nature of generating characters from features,
  # which won't play nicely with the way typos are actually introduced into the language.
  phonemic_src:
    # Introduce artificial typos in the source text.
    - Typos: 0.05

# The random seed must be different for different teacher models so that the results
# of the models will be different.
seed: {seed}

# src sentence + trg sentence + token alignments
num_fields: 3
