taskcluster
.
├── config.yml
├── configs
│   ├── config.ci.yml
│   └── config.prod.yml
├── docker
│   ├── base
│   │   ├── Dockerfile
│   │   └── known_hosts
│   ├── inference
│   │   ├── Dockerfile
│   │   └── intel-mkl.list
│   ├── test
│   │   └── Dockerfile
│   ├── toolchain-build
│   │   ├── Dockerfile
│   │   └── intel-mkl.list
│   └── train
│       └── Dockerfile
├── kinds
│   ├── alignments-backtranslated
│   │   └── kind.yml
│   ├── alignments-original
│   │   └── kind.yml
│   ├── alignments-student
│   │   └── kind.yml
│   ├── all-pipeline
│   │   └── kind.yml
│   ├── all-pr-pipeline
│   │   └── kind.yml
│   ├── analyze-corpus
│   │   └── kind.yml
│   ├── analyze-mono
│   │   └── kind.yml
│   ├── bicleaner
│   │   └── kind.yml
│   ├── bicleaner-model
│   │   └── kind.yml
│   ├── cefilter
│   │   └── kind.yml
│   ├── clean-corpus
│   │   └── kind.yml
│   ├── clean-mono
│   │   └── kind.yml
│   ├── collect-corpus
│   │   └── kind.yml
│   ├── collect-mono-src
│   │   └── kind.yml
│   ├── collect-mono-trg
│   │   └── kind.yml
│   ├── continuation-corpus
│   │   └── kind.yml
│   ├── continuation-model
│   │   └── kind.yml
│   ├── continuation-vocab
│   │   └── kind.yml
│   ├── dataset
│   │   └── kind.yml
│   ├── docker-image
│   │   └── kind.yml
│   ├── evaluate
│   │   └── kind.yml
│   ├── evaluate-quantized
│   │   └── kind.yml
│   ├── evaluate-teacher-ensemble
│   │   └── kind.yml
│   ├── export
│   │   └── kind.yml
│   ├── extract-best
│   │   └── kind.yml
│   ├── fetch
│   │   ├── kind.yml
│   │   ├── python.yml
│   │   └── toolchains.yml
│   ├── finetune-student
│   │   └── kind.yml
│   ├── inference
│   │   └── kind.yml
│   ├── merge-corpus
│   │   └── kind.yml
│   ├── merge-devset
│   │   └── kind.yml
│   ├── merge-mono
│   │   └── kind.yml
│   ├── merge-translated
│   │   └── kind.yml
│   ├── quantize
│   │   └── kind.yml
│   ├── score
│   │   └── kind.yml
│   ├── shortlist
│   │   └── kind.yml
│   ├── split-corpus
│   │   └── kind.yml
│   ├── split-mono-src
│   │   └── kind.yml
│   ├── split-mono-trg
│   │   └── kind.yml
│   ├── tests
│   │   └── kind.yml
│   ├── toolchain
│   │   └── kind.yml
│   ├── train-backwards
│   │   └── kind.yml
│   ├── train-student
│   │   └── kind.yml
│   ├── train-teacher
│   │   └── kind.yml
│   ├── train-vocab
│   │   └── kind.yml
│   ├── translate-corpus
│   │   └── kind.yml
│   ├── translate-mono-src
│   │   └── kind.yml
│   └── translate-mono-trg
│       └── kind.yml
├── pyproject.toml
├── scripts
│   ├── pipeline
│   │   ├── __init__.py
│   │   ├── __pycache__
│   │   │   └── train_taskcluster.cpython-310.pyc
│   │   ├── train-taskcluster.sh
│   │   └── train_taskcluster.py
│   └── toolchain
│       ├── browsermt.patch
│       ├── build-cuda-toolkit.sh
│       ├── build-extract-lex.sh
│       ├── build-fast-align.sh
│       ├── build-hunspell.sh
│       ├── build-kenlm.sh
│       ├── build-marian.sh
│       └── build-preprocess.sh
├── test
│   ├── conftest.py
│   ├── params
│   │   ├── large-lt-en.yml
│   │   └── small-ru-en.yml
│   ├── test_cleaning_params.py
│   ├── test_default_params.py
│   ├── test_target_stage.py
│   ├── test_training_continuation_backwards.py
│   └── test_training_continuation_teacher.py
├── translations_taskgraph
│   ├── __init__.py
│   ├── __pycache__
│   │   ├── __init__.cpython-310.pyc
│   │   ├── __init__.cpython-312.pyc
│   │   ├── parameters.cpython-310.pyc
│   │   ├── parameters.cpython-312.pyc
│   │   ├── target_tasks.cpython-310.pyc
│   │   ├── target_tasks.cpython-312.pyc
│   │   ├── task.cpython-310.pyc
│   │   └── training_config.cpython-310.pyc
│   ├── actions
│   │   ├── __init__.py
│   │   ├── __pycache__
│   │   │   ├── __init__.cpython-310.pyc
│   │   │   ├── __init__.cpython-312.pyc
│   │   │   ├── rebuild_docker_images_and_toolchains.cpython-310.pyc
│   │   │   ├── rebuild_docker_images_and_toolchains.cpython-312.pyc
│   │   │   ├── train.cpython-310.pyc
│   │   │   └── train.cpython-312.pyc
│   │   ├── rebuild_docker_images_and_toolchains.py
│   │   └── train.py
│   ├── parameters.py
│   ├── target_tasks.py
│   ├── transforms
│   │   ├── __pycache__
│   │   │   ├── cached_tasks.cpython-310.pyc
│   │   │   ├── cached_tasks.cpython-312.pyc
│   │   │   ├── cast_to.cpython-310.pyc
│   │   │   ├── cast_to.cpython-312.pyc
│   │   │   ├── cast_two.cpython-310.pyc
│   │   │   ├── continuation.cpython-310.pyc
│   │   │   ├── corpus_continuation.cpython-310.pyc
│   │   │   ├── dependency_dummies.cpython-310.pyc
│   │   │   ├── dependency_dummies.cpython-312.pyc
│   │   │   ├── find_upstreams.cpython-310.pyc
│   │   │   ├── find_upstreams.cpython-312.pyc
│   │   │   ├── from_datasets.cpython-310.pyc
│   │   │   ├── from_datasets.cpython-312.pyc
│   │   │   ├── marian_args.cpython-310.pyc
│   │   │   ├── marian_args.cpython-312.pyc
│   │   │   ├── skip_unless_inference_changed.cpython-310.pyc
│   │   │   ├── skip_unless_inference_changed.cpython-312.pyc
│   │   │   ├── skip_unless_pipeline_changed.cpython-310.pyc
│   │   │   ├── skip_unless_pipeline_changed.cpython-312.pyc
│   │   │   ├── training_continuation.cpython-310.pyc
│   │   │   ├── training_continuation.cpython-312.pyc
│   │   │   ├── worker_selection.cpython-310.pyc
│   │   │   └── worker_selection.cpython-312.pyc
│   │   ├── cached_tasks.py
│   │   ├── cast_to.py
│   │   ├── continuation.py
│   │   ├── dependency_dummies.py
│   │   ├── find_upstreams.py
│   │   ├── from_datasets.py
│   │   ├── marian_args.py
│   │   ├── skip_unless_inference_changed.py
│   │   ├── skip_unless_pipeline_changed.py
│   │   ├── training_continuation.py
│   │   └── worker_selection.py
│   └── util
│       ├── __init__.py
│       ├── __pycache__
│       │   ├── __init__.cpython-310.pyc
│       │   ├── __init__.cpython-312.pyc
│       │   ├── dataclass_helpers.cpython-310.pyc
│       │   ├── dataset_helpers.cpython-310.pyc
│       │   ├── dataset_helpers.cpython-312.pyc
│       │   ├── dict_helpers.cpython-310.pyc
│       │   ├── dict_helpers.cpython-312.pyc
│       │   ├── serializable.cpython-310.pyc
│       │   ├── substitution.cpython-310.pyc
│       │   └── substitution.cpython-312.pyc
│       ├── dataset_helpers.py
│       ├── dict_helpers.py
│       └── substitution.py
└── translations_taskgraph.egg-info
    ├── PKG-INFO
    ├── SOURCES.txt
    ├── dependency_links.txt
    └── top_level.txt

71 directories, 153 files

scripts/pipeline/__init__.py



scripts/pipeline/train-taskcluster.sh

#!/bin/bash

set -x
set -euo pipefail

pushd `dirname $0`/../../.. &>/dev/null
VCS_ROOT=$(pwd)
popd &>/dev/null

if [ "$#" -lt 10 ]; then
    echo "Usage: $0 <model_type> <training_type> <src_locale> <trg_locale> <train_set_prefix> <validation_set_prefix> <artifacts> <best_model_metric> <alignments> <pretrained_model_mode> <pretrained_model_type> [extra_marian_args...]"
    exit 1
fi

model_type=$1
training_type=$2
src=$3
trg=$4
train_set_prefixes=$5
validation_set_prefix=$6
artifacts=$7
best_model_metric=$8
alignments=$9
seed=${10}
teacher_mode=${11}
student_model=${12}
pretrained_model_mode=${13}
pretrained_model_type=${14}
extra_marian_args=( "${@:15}" )

if [ "$pretrained_model_mode" != "use" ]; then
    # MOZ_FETCHES_DIR is not required for the "use" pretrained model mode
    [[ -v MOZ_FETCHES_DIR ]] || { echo "MOZ_FETCHES_DIR is not set"; exit 1; }
fi

case "$pretrained_model_mode" in
    "use")
        echo "The training mode is 'use', using existing model without further training."
        exit 0
        ;;
    "continue"|"init"|"None")
        if [ "$pretrained_model_mode" == "None" ]; then
            # In any non-pretrained mode this file is pulled from an upstream
            # task. We copy it over to the artifacts directory earlier to
            # ensure that it is published even if the task is interrupted
            # (eg: by a spot termination in GCP). This makes resuming training
            # easier.
            mkdir -p "$TASK_WORKDIR/artifacts"
            cp "$MOZ_FETCHES_DIR/vocab.spm" "$TASK_WORKDIR/artifacts/vocab.spm"
        fi

        if [ "$pretrained_model_mode" == "init" ]; then
            extra_marian_args+=("--pretrained-model" "$TASK_WORKDIR/artifacts/final.model.npz.best-$best_model_metric.npz" "--no-restore-corpus")
        fi
        python3 $VCS_ROOT/pipeline/train/train.py \
        --model_type "$model_type" \
        --student_model "$student_model" \
        --training_type "$training_type" \
        --src "$src" \
        --trg "$trg" \
        --train_set_prefixes "$train_set_prefixes" \
        --validation_set_prefix "$validation_set_prefix" \
        --artifacts "$artifacts" \
        --vocab "$TASK_WORKDIR/artifacts/vocab.spm" \
        --best_model_metric "$best_model_metric" \
        --alignments "$alignments" \
        --seed "$seed" \
        --teacher_mode "$teacher_mode" \
        --gpus "$GPUS" \
        --marian_dir "$MARIAN" \
        --workspace "$WORKSPACE" \
        -- "${extra_marian_args[@]}"
        ;;
esac


scripts/pipeline/train_taskcluster.py

#!/usr/bin/env python3

import logging
import os
import os.path
import requests
import subprocess
import sys

TRAINING_SCRIPT = os.path.join(os.path.dirname(__file__), "train-taskcluster.sh")
CONTINUATION_ARTIFACTS = {
    "config.opustrainer.yml",
    "config.opustrainer.yml.state",
    "devset.out",
    "model.npz",
    "model.npz.best-bleu-detok.npz",
    "model.npz.best-bleu-detok.npz.decoder.yml",
    "model.npz.best-ce-mean-words.npz",
    "model.npz.best-ce-mean-words.npz.decoder.yml",
    "model.npz.best-chrf.npz",
    "model.npz.best-chrf.npz.decoder.yml",
    "model.npz.decoder.yml",
    "model.npz.optimizer.npz",
    "model.npz.progress.yml",
    "model.npz.yml",
    "opustrainer.log",
    "train.log",
    "valid.log",
    "vocab.spm",
}


ARTIFACTS_URL = "{root_url}/api/queue/v1/task/{task_id}/runs/{run_id}/artifacts"
ARTIFACT_URL = "{root_url}/api/queue/v1/task/{task_id}/runs/{run_id}/artifacts/{artifact_name}"
# TODO: consolidate everything in train.py or at least do not rely on the argument names and the number of them in the Taskcluster part
# TODO: https://github.com/mozilla/translations/issues/607
# The argument number where pretrained model mode is expected.
# This is 1-indexed, not 0-indexed, so it should line up with the argument
# number this is fetched in in train-taskcluster.sh
PRETRAINED_MODEL_MODE_ARG_NUMBER = 13
# Nothing special about 17...just a number plucked out of thin air that
# should be distinct enough to retry on.
DOWNLOAD_ERROR_EXIT_CODE = 17


def main(args):
    logging.basicConfig(level=logging.INFO)

    script_args = list(args)
    task_id = os.environ["TASK_ID"]
    run_id = int(os.environ["RUN_ID"])
    root_url = os.environ["TASKCLUSTER_ROOT_URL"]
    # Must line up with where model_dir is in `train-taskcluster.sh` while that script
    # still exists.
    model_dir = script_args[6]
    pretrained_model_mode = None
    if len(args) >= PRETRAINED_MODEL_MODE_ARG_NUMBER:
        pretrained_model_mode = script_args[PRETRAINED_MODEL_MODE_ARG_NUMBER - 1]

    if not os.path.exists(model_dir):
        os.makedirs(model_dir)

    if run_id > 0:
        logging.info("run_id > 0, attempting to resume training from an earlier run...")
        prev_run_id = run_id - 1

        while prev_run_id >= 0:
            try:
                resp = requests.get(
                    ARTIFACTS_URL.format(root_url=root_url, task_id=task_id, run_id=prev_run_id)
                )
                resp.raise_for_status()
            except Exception:
                logging.exception("Caught exception, exiting with distinct code...")
                sys.exit(DOWNLOAD_ERROR_EXIT_CODE)

            run_artifacts = set([os.path.basename(a["name"]) for a in resp.json()["artifacts"]])

            resumable = True
            if run_artifacts.issuperset(CONTINUATION_ARTIFACTS):
                logging.info(
                    f"Run {prev_run_id} appears to have the artifacts we need! Downloading them..."
                )
            else:
                logging.info(f"Run {prev_run_id} is missing some necessary artifacts...")
                resumable = False

            if resumable:
                for artifact in resp.json()["artifacts"]:
                    # Skip Taskcluster logs - we only care about artifacts that the training tools create.
                    if artifact["name"].startswith("public/log"):
                        continue
                    out_name = os.path.basename(artifact["name"])
                    logging.info(f"Fetching {artifact['name']}...")

                    r = requests.get(
                        ARTIFACT_URL.format(
                            root_url=root_url,
                            task_id=task_id,
                            run_id=prev_run_id,
                            artifact_name=artifact["name"],
                        ),
                        stream=True,
                    )
                    if 400 <= r.status_code <= 500:
                        logging.exception(
                            f"Got 4xx error for {artifact['name']}, run {run_id} is not resumable..."
                        )
                        resumable = False
                        break
                    elif r.status_code >= 500:
                        logging.exception("Caught exception, exiting with distinct code...")
                        sys.exit(DOWNLOAD_ERROR_EXIT_CODE)

                    with open(os.path.join(model_dir, out_name), "wb+") as fd:
                        for chunk in r.iter_content(chunk_size=8192):
                            fd.write(chunk)

            if resumable:
                # We successfully downloaded all the artifacts from a previous run. Override
                # the pretrained model mode and we're done!
                pretrained_model_mode = "continue"
                break
            else:
                # We weren't able to get all of the necessary artifacts; try the next previous run
                prev_run_id -= 1

    if pretrained_model_mode:
        if len(script_args) < PRETRAINED_MODEL_MODE_ARG_NUMBER:
            script_args.append(pretrained_model_mode)
        else:
            script_args[PRETRAINED_MODEL_MODE_ARG_NUMBER - 1] = pretrained_model_mode
    subprocess.run([TRAINING_SCRIPT, *script_args], check=True)


if __name__ == "__main__":
    main(sys.argv[1:])


scripts/toolchain/build-cuda-toolkit.sh

#!/bin/bash
set -e
set -x

# CUDA installers do not have a silent mode of operation that allows to run
# them without also installing GPU drivers and other unnecessary things.
# Instead, we extract the raw contents of the installer, and then combine
# the extracted contents into a tarball.

export CUDA_INSTALLER=$MOZ_FETCHES_DIR/cuda-source.run

TARFILE=$UPLOAD_DIR/cuda-toolkit.tar

chmod +x $CUDA_INSTALLER
# This installer advertises a `--extract` option which put
# the contents in a directory of our choosing...but it doesn't
# work when run under alpine linux docker containers. Instead,
# we can use these secret options, which will extract to `pkg`
# in the current working directory. The files we care about
# will end up in `pkg/builds`.
EXTRACT_DIR="$(pwd)/cuda-toolkit"

# On the off chance that two different cuda-toolkit toolchain tasks run
# on the same worker, the second one will end up with an existing toolkit
# as a starting point, which will end up packaging both versions of the
# toolkit at in the subsequent tarball. This confuses downstream tasks, which
# may pick up the wrong version of the toolkit.
rm -rf $EXTRACT_DIR

# it complains on compiler version check on Ubuntu 22 for cuda toolkit 11.2. overriding helps
$CUDA_INSTALLER --toolkit --toolkitpath=$EXTRACT_DIR --silent --override

tar --zstd -cf $TARFILE.zst cuda-toolkit


scripts/toolchain/build-extract-lex.sh

#!/bin/bash
set -e
set -x

EXTRACT_LEX_DIR=$MOZ_FETCHES_DIR/extract-lex

build_dir=$(mktemp -d)
cd $build_dir
cmake $EXTRACT_LEX_DIR
make -j$(nproc)

cd $build_dir
chmod +x extract_lex
tar --zstd -cf $UPLOAD_DIR/extract_lex.tar.zst extract_lex


scripts/toolchain/build-fast-align.sh

#!/bin/bash
set -e
set -x

FAST_ALIGN_DIR=$MOZ_FETCHES_DIR/fast_align

build_dir=$(mktemp -d)
cd $build_dir
cmake $FAST_ALIGN_DIR
make -j$(nproc)

cd "${build_dir}"
chmod +x fast_align atools
tar -c fast_align atools | zstd > $UPLOAD_DIR/fast-align.tar.zst


scripts/toolchain/build-hunspell.sh

#!/bin/bash
set -e
set -x

HUNSPELL_DIR=$MOZ_FETCHES_DIR/hunspell

cd $HUNSPELL_DIR
python3 setup.py bdist_wheel
whl=$(ls dist/*.whl)

cp $whl $UPLOAD_DIR/


scripts/toolchain/build-kenlm.sh

#!/bin/bash
set -e
set -x

KENLM_DIR=$MOZ_FETCHES_DIR/kenlm-source

# TODO: I don't think we actually need the C++ stuff? just the python module
# build_dir=$(mktemp -d)
# cd $build_dir
# cmake $KENLM_DIR -DKENLM_MAX_ORDER=7
# make -j$(nproc)

cd $KENLM_DIR
# Install these separately so they will install as wheels.
# Using `--build-option` below disables wheels even for dependencies.
pip install setuptools wheel cmake
MAX_ORDER=7 python3 setup.py bdist_wheel
find .
cp $KENLM_DIR/dist/kenlm-0.0.0-cp310-cp310-linux_x86_64.whl $UPLOAD_DIR/


scripts/toolchain/build-marian.sh

#!/bin/bash
set -e
set -x

pushd `dirname $0` &>/dev/null
MY_DIR=$(pwd)
popd &>/dev/null

patch=${1:-none}
use_gpu=${2:-true}

export MARIAN_DIR=$MOZ_FETCHES_DIR/marian-source
export CUDA_DIR=$MOZ_FETCHES_DIR/cuda-toolkit

if [ "$patch" != "none" ]; then
  patch -d ${MARIAN_DIR} -p1 < ${MY_DIR}/${patch}
fi

# TODO: consider not calling out to this since it's such a simple script...
bash $VCS_PATH/pipeline/setup/compile-marian.sh "${MARIAN_DIR}/build" "$(nproc)" "${use_gpu}"

cd $MARIAN_DIR/build
tar -cf $UPLOAD_DIR/marian.tar \
  "marian" \
  "marian-decoder" \
  "marian-scorer" \
  "marian-conv" \
  "marian-vocab" \
  "spm_train" \
  "spm_encode" \
  "spm_export_vocab"

if [ -f "${MARIAN_DIR}/scripts/alphas/extract_stats.py" ]; then
  cd "${MARIAN_DIR}/scripts/alphas"
  tar -rf $UPLOAD_DIR/marian.tar extract_stats.py
fi

zstd --rm $UPLOAD_DIR/marian.tar


scripts/toolchain/build-preprocess.sh

#!/bin/bash
set -e
set -x

PREPROCESS_DIR=$MOZ_FETCHES_DIR/preprocess

build_dir=$(mktemp -d)
cd $build_dir
cmake $PREPROCESS_DIR -DBUILD_TYPE=Release
make -j$(nproc)

cd $build_dir/bin
chmod +x dedupe
tar --zstd -cf $UPLOAD_DIR/dedupe.tar.zst dedupe


test/conftest.py

from copy import deepcopy
import pytest
import requests_mock
from typing import Any, Dict, Generator, List, Protocol

from taskgraph.generator import TaskGraph, TaskGraphGenerator
from taskgraph.parameters import Parameters, parameters_loader
from translations_taskgraph.util.substitution import substitute


class CreateTgg(Protocol):
    def __call__(
        self, parameters: Parameters | None = None, overrides: dict | None = None
    ) -> TaskGraphGenerator:
        ...


# These fixtures are largely cribbed from Gecko:
# https://searchfox.org/mozilla-central/source/taskcluster/test
@pytest.fixture(scope="session")
def create_tgg():
    def inner(
        parameters: Parameters | None = None, overrides: dict | None = None
    ) -> TaskGraphGenerator:
        params = parameters_loader(parameters, strict=False, overrides=overrides)
        return TaskGraphGenerator(None, params)

    return inner


@pytest.fixture(scope="module")
def mock_requests() -> Generator[requests_mock.Mocker, None, None]:
    with requests_mock.Mocker() as m:
        yield m


# Scoping this at the module level means that each module will only generate
# a taskgraph one time, no matter how many tests are within it. This is
# beneficial for performance reasons, but forces any tests that need distinct
# parameters to be moved to their own modules.
@pytest.fixture(scope="module")
def tgg(request: pytest.FixtureRequest, create_tgg: CreateTgg) -> TaskGraphGenerator:
    if not hasattr(request.module, "PARAMS"):
        pytest.fail("'tgg' fixture requires a module-level 'PARAMS' variable")

    return create_tgg(overrides=request.module.PARAMS)


@pytest.fixture(scope="module")
def full_task_graph(tgg: TaskGraphGenerator) -> TaskGraph:
    return tgg.full_task_graph


@pytest.fixture(scope="module")
def target_task_graph(tgg: TaskGraphGenerator) -> TaskGraph:
    return tgg.target_task_graph


@pytest.fixture(scope="module")
def target_task_set(tgg: TaskGraphGenerator) -> TaskGraph:
    return tgg.target_task_set


@pytest.fixture(scope="module")
def optimized_task_graph(
    request: pytest.FixtureRequest, mock_requests: requests_mock.Mocker, tgg: TaskGraphGenerator
) -> TaskGraph:
    for resp in getattr(request.module, "MOCK_REQUESTS", {}):
        responses: List[Dict[str, Any]] = deepcopy(resp["responses"])
        digests = {}
        # This is a bit of a terrible hack, but it allows for cached task digests
        # to be substituted into mocked API responses, which is needed to test
        # the optimized and/or morphed task graph. Cached task digests are
        # generated as part of earlier phases, so there's no sensible way for
        # them to defined concretely at the same time as other parts of the
        # MOCK_REQUESTS.
        for label, key in resp.get("substitute_digest", {}).items():
            digests[key] = tgg.full_task_set[label].attributes["cached_task"]["digest"]
        responses = substitute(responses, **digests)
        mock_requests.request(
            resp["method"],
            resp["url"],
            responses,
        )

    return tgg.optimized_task_graph


test/test_cleaning_params.py

from copy import deepcopy

from taskgraph.taskgraph import TaskGraph

from translations_taskgraph.parameters import get_ci_training_config

PARAMS = deepcopy(get_ci_training_config())


def test_monocleaner_params(full_task_graph: TaskGraph):
    tasks = {t.label: t for t in full_task_graph.tasks.values()}

    assert (
        float(
            tasks["clean-mono-news-crawl-ru-news_2008-mono-src"].task["payload"]["command"][-1][
                -3:
            ]
        )
        == PARAMS["training_config"]["experiment"]["monocleaner"]["mono-src"][
            "dataset-thresholds"
        ]["news-crawl_news_2008"]
    )
    assert (
        float(
            tasks["clean-mono-news-crawl-en-news_2007-mono-trg"].task["payload"]["command"][-1][
                -3:
            ]
        )
        == PARAMS["training_config"]["experiment"]["monocleaner"]["mono-trg"]["default-threshold"]
    )
    assert (
        float(
            tasks["clean-mono-opus-ru-tldr-pages_v2023-08-29-mono-src"].task["payload"]["command"][
                -1
            ][-3:]
        )
        == PARAMS["training_config"]["experiment"]["monocleaner"]["mono-src"][
            "dataset-thresholds"
        ]["opus_tldr-pages_v2023-08-29"]
    )
    assert (
        float(
            tasks["clean-mono-opus-en-tldr-pages_v2023-08-29-mono-trg"].task["payload"]["command"][
                -1
            ][-3:]
        )
        == PARAMS["training_config"]["experiment"]["monocleaner"]["mono-trg"][
            "dataset-thresholds"
        ]["opus_tldr-pages_v2023-08-29"]
    )


def test_bicleaner_params(full_task_graph: TaskGraph):
    tasks = {t.label: t for t in full_task_graph.tasks.values()}

    assert (
        str(PARAMS["training_config"]["experiment"]["bicleaner"]["default-threshold"])
        in tasks["bicleaner-ai-mtdata-Tilde-airbaltic-1-eng-rus-ru-en"].task["payload"]["command"][
            -1
        ][-1][-50:]
    )
    assert (
        str(
            PARAMS["training_config"]["experiment"]["bicleaner"]["dataset-thresholds"][
                "opus_ada83_v1"
            ]
        )
        in tasks["bicleaner-ai-opus-ada83_v1-ru-en"].task["payload"]["command"][-1][-1][-50:]
    )


test/test_default_params.py

from copy import deepcopy

from taskgraph.taskgraph import TaskGraph

from translations_taskgraph.parameters import get_ci_training_config

PARAMS = deepcopy(get_ci_training_config())
PARAMS["target_tasks_method"] = "train-target-tasks"

MOCK_REQUESTS = [
    {
        "substitute_digest": {
            "docker-image-base": "digest_base",
            "docker-image-inference": "digest_inference",
            "docker-image-test": "digest_test",
            "docker-image-toolchain-build": "digest_toolchain",
            "docker-image-train": "digest_train",
        },
        "method": "POST",
        "url": "https://firefox-ci-tc.services.mozilla.com/api/index/v1/tasks/indexes",
        "responses": [
            {
                "json": {
                    "tasks": [
                        {
                            "namespace": "translations.cache.level-3.docker-images.v2.base.hash.{digest_base}",
                            "taskId": "docker-image-base",
                        },
                        {
                            "namespace": "translations.cache.level-3.docker-images.v2.inference.hash.{digest_inference}",
                            "taskId": "docker-image-inference",
                        },
                        {
                            "namespace": "translations.cache.level-3.docker-images.v2.test.hash.{digest_test}",
                            "taskId": "docker-image-test",
                        },
                        {
                            "namespace": "translations.cache.level-3.docker-images.v2.toolchain-build.hash.{digest_toolchain}",
                            "taskId": "docker-image-toolchain-build",
                        },
                        {
                            "namespace": "translations.cache.level-3.docker-images.v2.train.hash.{digest_train}",
                            "taskId": "docker-image-train",
                        },
                    ],
                },
                "status_code": 200,
            },
        ],
    },
    {
        "method": "POST",
        "url": "https://firefox-ci-tc.services.mozilla.com/api/queue/v1/tasks/status",
        "responses": [
            {
                "json": {
                    "statuses": [
                        {
                            "status": {
                                "state": "completed",
                                "expires": "3024-08-21T22:37:28.781Z",
                            },
                            "taskId": "docker-image-base",
                        },
                        {
                            "status": {
                                "state": "completed",
                                "expires": "3024-08-21T22:37:28.781Z",
                            },
                            "taskId": "docker-image-inference",
                        },
                        {
                            "status": {
                                "state": "completed",
                                "expires": "3024-08-21T22:37:28.781Z",
                            },
                            "taskId": "docker-image-test",
                        },
                        {
                            "status": {
                                "state": "completed",
                                "expires": "3024-08-21T22:37:28.781Z",
                            },
                            "taskId": "docker-image-toolchain-build",
                        },
                        {
                            "status": {
                                "state": "completed",
                                "expires": "3024-08-21T22:37:28.781Z",
                            },
                            "taskId": "docker-image-train",
                        },
                    ],
                },
                "status_code": 200,
            },
        ],
    },
]


def test_last_task_is_targeted(target_task_set: TaskGraph):
    """Ensure that the last task in the pipeline is targeted by default"""
    assert any([task == "all-pipeline-ru-en-1" for task in target_task_set.tasks])


def test_cached_tasks_optimized_away(optimized_task_graph: TaskGraph):
    """Ensure that any tasks found in a cache route are _not_ present
    in the optimized graph (ie: they will not be scheduled)."""
    for task in optimized_task_graph.tasks.values():
        assert not task.label.startswith("docker-image")


test/test_target_stage.py

from copy import deepcopy

from taskgraph.taskgraph import TaskGraph

from translations_taskgraph.parameters import get_ci_training_config

PARAMS = deepcopy(get_ci_training_config())
PARAMS["target_tasks_method"] = "train-target-tasks"
PARAMS["training_config"]["target-stage"] = "train-teacher"


def test_nothing_downstream_of_target(target_task_graph: TaskGraph):
    # despite being called `reverse_links_dict`, this actually
    # gives us a dict where we can find tasks _downstream_ of
    # each task by label
    links = target_task_graph.graph.reverse_links_dict()
    for task in target_task_graph.graph.nodes:
        if task.startswith("train-teacher"):
            assert links[task] == set()


test/test_training_continuation_backwards.py

from copy import deepcopy

from taskgraph.taskgraph import TaskGraph

from translations_taskgraph.parameters import get_ci_training_config

PARAMS = deepcopy(get_ci_training_config())
PARAMS["target_tasks_method"] = "train-target-tasks"
PARAMS["training_config"]["continuation"]["models"] = {
    "train-backwards": {
        "mode": "use",
        "type": "default",
        "url": "https://storage.googleapis.com/releng-translations-dev/models/ru-en/better-teacher/student",
    },
}

MOCK_REQUESTS = [
    {
        "method": "POST",
        "url": "https://firefox-ci-tc.services.mozilla.com/api/index/v1/tasks/indexes",
        "responses": [{"json": {"tasks": []}}],
    },
    {
        "method": "POST",
        "url": "https://firefox-ci-tc.services.mozilla.com/api/queue/v1/tasks/status",
        "responses": [{"json": {"statuses": []}}],
    },
]


def test_artifact_mounts(full_task_graph: TaskGraph):
    task = [t for t in full_task_graph.tasks.values() if t.label == "train-backwards-ru-en"][0]
    # No need to bother looking for _all_ files (we'd just duplicate
    # the full list if we did that...), but we verify that one file
    # is well formed.
    mounted_files = {m["file"]: m for m in task.task["payload"]["mounts"] if "file" in m}
    assert mounted_files["./artifacts/model.npz"]["content"] == {
        "url": "https://storage.googleapis.com/releng-translations-dev/models/ru-en/better-teacher/student/model.npz",
    }


def test_no_eval_tasks(optimized_task_graph: TaskGraph):
    """Ensure evaluate tasks for train-backwards aren't targeted.
    See https://github.com/mozilla/translations/issues/628"""
    eval_tasks = [
        task.label
        for task in optimized_task_graph.tasks.values()
        if task.label.startswith("evaluate-backward")
    ]
    assert len(eval_tasks) == 0


test/test_training_continuation_teacher.py

from copy import deepcopy

from taskgraph.taskgraph import TaskGraph

from translations_taskgraph.parameters import get_ci_training_config

PARAMS = deepcopy(get_ci_training_config(None))
PARAMS["target_tasks_method"] = "train-target-tasks"
PARAMS["training_config"]["continuation"]["models"] = {
    "train-teacher": {
        "mode": "use",
        "type": "default",
        "urls": [
            "https://storage.googleapis.com/releng-translations-dev/models/ru-en/better-teacher/student"
        ],
    },
}

MOCK_REQUESTS = [
    {
        "method": "POST",
        "url": "https://firefox-ci-tc.services.mozilla.com/api/index/v1/tasks/indexes",
        "responses": [{"json": {"tasks": []}}],
    },
    {
        "method": "POST",
        "url": "https://firefox-ci-tc.services.mozilla.com/api/queue/v1/tasks/status",
        "responses": [{"json": {"statuses": []}}],
    },
]


def test_artifact_mounts(full_task_graph: TaskGraph):
    task = [t for t in full_task_graph.tasks.values() if t.label == "train-teacher-ru-en-1"][0]
    # No need to bother looking for _all_ files (we'd just duplicate
    # the full list if we did that...), but we verify that one file
    # is well formed.
    mounted_files = {m["file"]: m for m in task.task["payload"]["mounts"] if "file" in m}
    assert mounted_files["./artifacts/model.npz"]["content"] == {
        "url": "https://storage.googleapis.com/releng-translations-dev/models/ru-en/better-teacher/student/model.npz",
    }


def test_no_eval_tasks(optimized_task_graph: TaskGraph):
    """Ensure evaluate tasks for train-teacher aren't targeted.
    See https://github.com/mozilla/translations/issues/628"""
    eval_tasks = [
        task.label
        for task in optimized_task_graph.tasks.values()
        if task.label.startswith("evaluate-teacher")
    ]
    assert len(eval_tasks) == 0


translations_taskgraph/__init__.py

from importlib import import_module


def register(graph_config):
    _import_modules(
        [
            "actions.train",
            "actions.rebuild_docker_images_and_toolchains",
            "parameters",
            "target_tasks",
        ]
    )


def _import_modules(modules):
    for module in modules:
        import_module(".{}".format(module), package=__name__)


translations_taskgraph/actions/__init__.py



translations_taskgraph/actions/rebuild_docker_images_and_toolchains.py

# This Source Code Form is subject to the terms of the Mozilla Public
# License, v. 2.0. If a copy of the MPL was not distributed with this
# file, You can obtain one at http://mozilla.org/MPL/2.0/.

from taskgraph.actions.registry import register_callback_action
from taskgraph.actions.util import create_tasks, fetch_graph_and_labels


@register_callback_action(
    name="rebuild-docker-images-and-toolchains",
    title="Rebuild Docker Images and Toolchains",
    symbol="images-and-toolchains",
    description="Create docker-image and toolchain tasks to rebuild their artifacts.",
    order=1000,
    context=[],
)
def rebuild_docker_images_and_toolchains_action(
    parameters, graph_config, input, task_group_id, task_id
):
    decision_task_id, full_task_graph, label_to_task_id = fetch_graph_and_labels(
        parameters, graph_config, task_group_id=task_group_id
    )
    tasks_to_create = [
        label
        for label, task in full_task_graph.tasks.items()
        if task.kind == "docker-image" or task.kind == "fetch" or task.kind == "toolchain"
    ]
    if tasks_to_create:
        create_tasks(
            graph_config,
            tasks_to_create,
            full_task_graph,
            label_to_task_id,
            parameters,
            decision_task_id,
        )


translations_taskgraph/actions/train.py

# This Source Code Form is subject to the terms of the Mozilla Public
# License, v. 2.0. If a copy of the MPL was not distributed with this
# file, You can obtain one at http://mozilla.org/MPL/2.0/.
import json
import logging

from taskgraph.actions.registry import register_callback_action
from taskgraph.decision import taskgraph_decision
from taskgraph.parameters import Parameters
from taskgraph.taskgraph import TaskGraph
from taskgraph.util.taskcluster import get_ancestors, get_artifact

from translations_taskgraph.parameters import get_ci_training_config

logger = logging.getLogger(__name__)

TRAIN_ON_PROJECTS = (
    "https://github.com/mozilla/translations",
    "https://github.com/mozilla-releng/staging-firefox-translations-training",
)

WORKER_CLASSES = (
    # Regular, on-demand GCP instances
    "gcp-standard",
    # Spot instances in GCP
    "gcp-spot",
)


def can_train(parameters):
    return parameters["head_repository"] in TRAIN_ON_PROJECTS or (
        parameters["base_repository"] in TRAIN_ON_PROJECTS
        and parameters["tasks_for"].startswith("github-pull-request")
    )


defaults = get_ci_training_config()["training_config"]


def validate_model_continuation(params):
    pretrained_models = params["training_config"].get("continuation", {}).get("models", {})
    teacher = pretrained_models.get("teacher")
    if teacher:
        teacher_ensemble = params["training_config"]["experiment"]["teacher-ensemble"]
        if len(teacher["urls"]) != teacher_ensemble:
            raise Exception(
                f"The experiment's 'teacher-ensemble' ({teacher_ensemble}) "
                f"does not match the number of provided model 'urls' ({len(teacher['urls'])}) "
                f"for the pretrained 'train-teacher' ensemble."
            )


@register_callback_action(
    name="train",
    title="Train",
    symbol="train",
    description="Initiate part or all of the training pipeline",
    cb_name="train",
    permission="train",
    order=500,
    context=[],
    available=can_train,
    schema=lambda graph_config: {
        "type": "object",
        "properties": {
            "previous_group_ids": {
                "type": "array",
                "description": """Optional: an array of taskIds of decision or action
tasks from the previous group(s) to use to populate our `previous_group_kinds`.
Tasks specified here will be used as long as their label matches a needed task, and that
task is upstream of `start-stage`. (That is to say: even if a task from one of these groups
has a cache digest that doesn't match what the downstream task wants, it will still be used. This
can be used for quick iteration of functionality where the quality of the outputs is not important.)""",
                "items": {
                    "type": "string",
                },
            },
            "start-stage": {
                "type": "string",
                "description": """Optional: The stage of the pipeline to begin at, provided replacements
can be found for tasks upstream of this stage. Usually used in conjunction with `previous_group_ids`
which allows for specifying task group ids to fetch existing tasks from.""",
                "default": "",
                # We need to allow for no stage to be specified, in additional to all of the
                # valid stages.
                "enum": graph_config["valid-stages"] + [""],
            },
            "target-stage": {
                "type": "string",
                "description": """The stage of the pipeline to run until
(any stages this choice depends on will be automatically included).""",
                "default": defaults["target-stage"],
                "enum": graph_config["valid-stages"],
            },
            "wandb-publication": {
                "type": "boolean",
                "description": """Enable publication to Weights and Biases""",
                "default": True,
            },
            "experiment": {
                "type": "object",
                "default": defaults["experiment"],
                "properties": {
                    "name": {
                        "type": "string",
                        "description": "A name for the experiment",
                    },
                    "src": {
                        "type": "string",
                        "description": "The src locale to train",
                    },
                    "trg": {
                        "type": "string",
                        "description": "The trg locale to train",
                    },
                    "teacher-ensemble": {
                        "type": "number",
                        "description": "Number of teachers to train",
                    },
                    "teacher-mode": {
                        "type": "string",
                        "description": "Teacher training mode",
                        "enum": ["one-stage", "two-stage"],
                        "default": "two-stage",
                    },
                    "teacher-decoder": {
                        "type": "string",
                        "description": "Translate with either Marian or CTranslate2",
                        "enum": ["marian", "ctranslate2"],
                        "default": "marian",
                    },
                    "student-model": {
                        "type": "string",
                        "description": "Student model configuration",
                        "enum": ["tiny", "base"],
                        "default": "tiny",
                    },
                    "mono-max-sentences-src": {
                        "type": "object",
                        "default": defaults["experiment"]["mono-max-sentences-src"],
                        "properties": {
                            "total": {
                                "type": "number",
                                "description": "limits for total src dataset",
                            },
                            "per-dataset": {
                                "type": "number",
                                "description": "limits per downloaded src dataset",
                            },
                        },
                    },
                    "mono-max-sentences-trg": {
                        "type": "object",
                        "default": defaults["experiment"]["mono-max-sentences-trg"],
                        "properties": {
                            "total": {
                                "type": "number",
                                "description": "limits for total trg dataset",
                            },
                            "per-dataset": {
                                "type": "number",
                                "description": "limits per downloaded trg dataset",
                            },
                        },
                    },
                    "spm-sample-size": {
                        "type": "number",
                        "description": "vocabularly training sample size",
                    },
                    "spm-vocab-size": {
                        "type": "number",
                        "description": "size of the vocabularly, can be reduced for testing",
                    },
                    "best-model": {
                        "type": "string",
                        "description": "best model to use for training",
                    },
                    "use-opuscleaner": {
                        "type": "string",
                        "description": "use OpusCleaner to clean corpus",
                        "enum": ["true", "false"],
                    },
                    "opuscleaner-mode": {
                        "type": "string",
                        "description": "indicates whether to use dataset specific configs",
                        "enum": ["custom", "defaults"],
                        "default": "defaults",
                    },
                    "bicleaner": {
                        "properties": {
                            "default-threshold": {
                                "type": "number",
                                "description": "bicleaner threshold",
                            },
                            "dataset-thresholds": {
                                "type": "object",
                                "additionalProperties": {
                                    "type": "number",
                                },
                            },
                        },
                        "required": [
                            "default-threshold",
                        ],
                    },
                    "monocleaner": {
                        "properties": {
                            "mono-src": {
                                "type": "object",
                                "properties": {
                                    "default-threshold": {
                                        "type": "number",
                                        "description": "default monocleaner threshold for source language",
                                    },
                                    "dataset-thresholds": {
                                        "type": "object",
                                        "additionalProperties": {
                                            "type": "number",
                                        },
                                    },
                                },
                                "required": [
                                    "default-threshold",
                                ],
                            },
                            "mono-trg": {
                                "type": "object",
                                "properties": {
                                    "default-threshold": {
                                        "type": "number",
                                        "description": "default monocleaner threshold for target language",
                                    },
                                    "dataset-thresholds": {
                                        "type": "object",
                                        "additionalProperties": {
                                            "type": "number",
                                        },
                                    },
                                },
                                "required": [
                                    "default-threshold",
                                ],
                            },
                        },
                        "required": [
                            "mono-src",
                            "mono-trg",
                        ],
                    },
                },
                "required": [
                    "name",
                    "src",
                    "trg",
                    "bicleaner",
                ],
            },
            "marian-args": {
                "type": "object",
                "default": defaults["marian-args"],
                "properties": {
                    "training-backward": {
                        "type": "object",
                        "additionalProperties": {
                            "type": "string",
                        },
                    },
                    "training-teacher": {
                        "type": "object",
                        "additionalProperties": {
                            "type": "string",
                        },
                    },
                    "training-student": {
                        "type": "object",
                        "additionalProperties": {
                            "type": "string",
                        },
                    },
                    "training-student-finetuned": {
                        "type": "object",
                        "additionalProperties": {
                            "type": "string",
                        },
                    },
                    "decoding-backward": {
                        "type": "object",
                        "additionalProperties": {
                            "type": "string",
                        },
                    },
                    "decoding-teacher": {
                        "type": "object",
                        "additionalProperties": {
                            "type": "string",
                        },
                    },
                },
            },
            "datasets": {
                "type": "object",
                "default": defaults["datasets"],
                "description": "The datasets to train with",
                "properties": {
                    "train": {
                        "type": "array",
                        "description": "Parallel training corpus",
                        "items": {
                            "type": "string",
                            # TODO
                            # "enum": []
                        },
                    },
                    "devtest": {
                        "type": "array",
                        "description": "datasets to merge for validation while training",
                        "items": {
                            "type": "string",
                            # TODO
                            # "enum": []
                        },
                    },
                    "test": {
                        "type": "array",
                        "description": "datasets for evaluation",
                        "items": {
                            "type": "string",
                            # TODO
                            # "enum": []
                        },
                    },
                    "mono-src": {
                        "type": "array",
                        "description": """
monolingual datasets (ex. paracrawl-mono_paracrawl8, commoncrawl_wmt16, news-crawl_news.2020)
to be translated by the teacher model
""",
                        "items": {
                            "type": "string",
                            # TODO
                            # "enum": []
                        },
                    },
                    "mono-trg": {
                        "type": "array",
                        "description": """
to be translated by the backward model to augment teacher corpus with back-translations
""",
                        "items": {
                            "type": "string",
                            # TODO
                            # "enum": []
                        },
                    },
                },
            },
            "continuation": {
                "type": "object",
                "default": {},
                "description": "Continue training from existing artifacts",
                "additionalProperties": False,
                "properties": {
                    "vocab": {
                        "type": "object",
                        "additionalProperties": False,
                        "properties": {
                            "src": {"type": "string", "format": "uri"},
                            "trg": {"type": "string", "format": "uri"},
                        },
                        "required": ["src", "trg"],
                    },
                    "corpora": {
                        "type": "object",
                        "additionalProperties": False,
                        "properties": {
                            "student-distillation": {
                                "type": "object",
                                "additionalProperties": False,
                                "properties": {
                                    "src": {"type": "string", "format": "uri"},
                                    "trg": {"type": "string", "format": "uri"},
                                    "tok-src": {"type": "string", "format": "uri"},
                                    "tok-trg": {"type": "string", "format": "uri"},
                                    "alignments": {"type": "string", "format": "uri"},
                                },
                                "required": ["src", "trg"],
                            },
                            "backtranslations": {
                                "type": "object",
                                "additionalProperties": False,
                                "properties": {
                                    "src": {"type": "string", "format": "uri"},
                                    "trg": {"type": "string", "format": "uri"},
                                    "tok-src": {"type": "string", "format": "uri"},
                                    "tok-trg": {"type": "string", "format": "uri"},
                                    "alignments": {"type": "string", "format": "uri"},
                                },
                                "required": ["src", "trg"],
                            },
                            "original-parallel": {
                                "type": "object",
                                "additionalProperties": False,
                                "properties": {
                                    "src": {"type": "string", "format": "uri"},
                                    "trg": {"type": "string", "format": "uri"},
                                    "tok-src": {"type": "string", "format": "uri"},
                                    "tok-trg": {"type": "string", "format": "uri"},
                                    "alignments": {"type": "string", "format": "uri"},
                                },
                                "required": ["src", "trg"],
                            },
                        },
                    },
                    # We are using urls because pretrained-models should be flexible enough
                    # to point at model (ensembles) that are not in taskcluster.
                    # Models could be in a long-term storage bucket, or we may use
                    # pretrained models hosted elsewhere.
                    "models": {
                        "type": "object",
                        "additionalProperties": False,
                        "properties": {
                            "train-teacher": {
                                "type": "object",
                                "properties": {
                                    "urls": {
                                        "type": "array",
                                        "items": {"type": "string", "format": "uri"},
                                        "minItems": 1,
                                    },
                                    "mode": {
                                        "type": "string",
                                        "enum": ["continue", "init", "use"],
                                    },
                                    "type": {
                                        "type": "string",
                                        "enum": ["default", "opusmt"],
                                    },
                                },
                                "required": ["urls", "mode", "type"],
                            },
                            "train-backwards": {
                                "type": "object",
                                "properties": {
                                    "url": {"type": "string"},
                                    "mode": {
                                        "type": "string",
                                        "enum": ["continue", "init", "use"],
                                    },
                                    "type": {
                                        "type": "string",
                                        "enum": ["default", "opusmt"],
                                    },
                                },
                                "required": ["url", "mode", "type"],
                            },
                        },
                    },
                },
            },
            "taskcluster": {
                "type": "object",
                "default": defaults["taskcluster"],
                "description": "Taskcluster-specific pipeline configuration, eg: chunking",
                "properties": {
                    "split-chunks": {
                        "type": "number",
                        "description": "The number of chunks (parallel jobs) to use in `split` steps",
                    },
                    "worker-classes": {
                        "type": "object",
                        "description": "The class of workers to use for this training, by kind",
                        "additionalProperties": {
                            "type": "string",
                            # TODO: add snakepit type(s) when they are brought online
                            "enum": ["gcp-standard", "gcp-spot"],
                        },
                    },
                },
            },
        },
        "required": [
            "target-stage",
            "datasets",
            "experiment",
            "marian-args",
        ],
    },
)
def train_action(parameters, graph_config, input, task_group_id, task_id):
    # TODO: Add a whack load of verification here. Things such as:
    # - datasets all exist
    # - locale pair exists for each dataset
    # - stage is valid
    # etc.

    parameters = dict(parameters)

    start_stage = input.pop("start-stage", None)
    if start_stage:
        if "previous_group_ids" not in input:
            raise Exception(
                "'previous_group_ids' is required to use 'start-stage' (otherwise we can't skip earlier tasks)"
            )

        previous_group_ids = input.pop("previous_group_ids")

        # First, we create one big graph out of all of the tasks from the specified group IDs.
        label_to_task_id = {}
        combined_full_task_graph = {}
        for graph_id in previous_group_ids:
            label_to_task_id.update(get_artifact(graph_id, "public/label-to-taskid.json"))
            full_task_graph = get_artifact(graph_id, "public/full-task-graph.json")
            combined_full_task_graph.update(full_task_graph)
        _, combined_full_task_graph = TaskGraph.from_json(combined_full_task_graph)

        # Next, we find the task id(s) corresponding of the tasks that match the stage
        # we want to start at.
        start_task_ids = []
        for label, task in combined_full_task_graph.tasks.items():
            if task.attributes.get("stage") == start_stage:
                start_task_ids.append(label_to_task_id[label])

        # Finally, we walk up the graph from our starting point and add any tasks found
        # as `existing_tasks`. These map task labels (eg: train-backwards-ru-en) to
        # task ids, and will be used instead of scheduling new tasks for any tasks with
        # an identical name.
        # As of taskgraph 13.0 `get_ancestors` returns taskids -> labels
        # `existing_tasks` needs the opposite
        parameters["existing_tasks"] = {v: k for k, v in get_ancestors(start_task_ids).items()}

    # Override the `existing_tasks` explicitly provided in the action's input
    existing_tasks = input.pop("existing_tasks", {})

    # Find and log `overridden_existing_tasks`
    overridden_existing_tasks = {
        existing_task: parameters["existing_tasks"][existing_task]
        for existing_task in existing_tasks.keys()
        if existing_task in parameters["existing_tasks"]
    }

    if overridden_existing_tasks:
        logger.info(
            f"Old values for `overridden_existing_tasks`: {json.dumps(overridden_existing_tasks, indent=2)}"
        )

    # Do the override!
    parameters["existing_tasks"].update(existing_tasks)

    # Log the new values for the `overridden_existing_tasks`
    new_values_for_overridden = {
        existing_task: parameters["existing_tasks"][existing_task]
        for existing_task in overridden_existing_tasks.keys()
    }

    if new_values_for_overridden:
        logger.info(
            f"New values for `overridden_existing_tasks`: {json.dumps(new_values_for_overridden, indent=2)}"
        )

    parameters["target_tasks_method"] = "train-target-tasks"
    parameters["optimize_target_tasks"] = True
    parameters["tasks_for"] = "action"
    parameters["training_config"] = input

    validate_model_continuation(parameters)

    parameters = Parameters(**parameters)
    taskgraph_decision({"root": graph_config.root_dir}, parameters=parameters)


translations_taskgraph/parameters.py

# This Source Code Form is subject to the terms of the Mozilla Public
# License, v. 2.0. If a copy of the MPL was not distributed with this
# file, You can obtain one at http://mozilla.org/MPL/2.0/.

import logging
from pathlib import Path
from taskgraph.parameters import extend_parameters_schema
from voluptuous import Extra, Optional, Required
import yaml

logger = logging.getLogger(__name__)


# By default, provide a very minimal config for CI that runs very quickly. This allows
# the pipeline to be validated in CI. The production training configs should override
# all of these values.
def get_ci_training_config(_=None) -> dict:
    vcs_path = (Path(__file__).parent / "../..").resolve()
    config_path = vcs_path / "taskcluster/configs/config.ci.yml"

    with config_path.open() as file:
        return {"training_config": yaml.safe_load(file)}


extend_parameters_schema(
    {
        Required("training_config"): {
            Required("target-stage"): str,
            Required("marian-args"): {
                Optional("training-backward"): {str: str},
                Optional("training-teacher"): {str: str},
                Optional("training-student"): {str: str},
                Optional("training-student-finetuned"): {str: str},
                Optional("decoding-backward"): {str: str},
                Optional("decoding-teacher"): {str: str},
            },
            Required("experiment"): {
                Required("name"): str,
                Required("src"): str,
                Required("trg"): str,
                Required("teacher-ensemble"): int,
                Required("teacher-mode"): str,
                Required("teacher-decoder"): str,
                Required("student-model"): str,
                Optional("corpus-max-sentences"): int,
                Required("mono-max-sentences-trg"): {
                    Required("total"): int,
                    Required("per-dataset"): int,
                },
                Required("mono-max-sentences-src"): {
                    Required("total"): int,
                    Required("per-dataset"): int,
                },
                Required("spm-sample-size"): int,
                Optional("spm-vocab-size"): int,
                Required("best-model"): str,
                Required("use-opuscleaner"): str,
                Optional("opuscleaner-mode"): str,
                Required("bicleaner"): {
                    Required("default-threshold"): float,
                    Optional("dataset-thresholds"): {
                        str: float,
                    },
                },
                Required("monocleaner"): {
                    Required("mono-src"): {
                        Required("default-threshold"): float,
                        Optional("dataset-thresholds"): {
                            str: float,
                        },
                    },
                    Required("mono-trg"): {
                        Required("default-threshold"): float,
                        Optional("dataset-thresholds"): {
                            str: float,
                        },
                    },
                },
                Required("hplt-min-doc-score"): {
                    Required("mono-src"): float,
                    Required("mono-trg"): float,
                },
            },
            Optional("datasets"): {
                str: [str],
            },
            Optional("continuation"): {
                Optional("vocab"): {
                    Required("src"): str,
                    Required("trg"): str,
                },
                Optional("models"): {
                    Optional("teacher"): {
                        Required("urls"): [str],
                        Required("mode"): str,
                        Required("type"): str,
                    },
                    Optional("backwards"): {
                        Required("url"): str,
                        Required("mode"): str,
                        Required("type"): str,
                    },
                },
                Optional("corpora"): {
                    Optional("backtranslations"): {
                        Required("src"): str,
                        Required("trg"): str,
                        Optional("aln"): str,
                    },
                    Optional("original-parallel"): {
                        Required("src"): str,
                        Required("trg"): str,
                        Optional("aln"): str,
                    },
                    Optional("student-distillation"): {
                        Required("src"): str,
                        Required("trg"): str,
                        Optional("aln"): str,
                    },
                },
            },
            Optional("taskcluster"): {
                Optional("split-chunks"): int,
                Required("worker-classes"): {
                    Required("default"): str,
                    Extra: str,
                },
            },
            Optional("wandb-publication"): bool,
        },
    },
    defaults_fn=get_ci_training_config,
)


def deep_setdefault(dict_, defaults):
    for k, v in defaults.items():
        if isinstance(dict_.get(k), dict):
            deep_setdefault(dict_[k], defaults[k])
        else:
            dict_[k] = v


def get_decision_parameters(graph_config, parameters):
    parameters.setdefault("training_config", {})
    deep_setdefault(parameters, get_ci_training_config())
    # We run the pipeline on a cron schedule to enable integration testing when
    # worker images change (see https://bugzilla.mozilla.org/show_bug.cgi?id=1937882).
    # These runs should _never_ be sent to W&B to avoid cluttering it up
    # with data of no value.
    if (
        parameters["tasks_for"] == "cron"
        and parameters["target_tasks_method"] == "train-target-tasks"
    ):
        logger.info("Overriding wandb-publication to be False for cron pipeline run")
        parameters["training_config"]["wandb-publication"] = False


translations_taskgraph/target_tasks.py

from taskgraph.target_tasks import register_target_task


@register_target_task("train-target-tasks")
def train_target_tasks(full_task_graph, parameters, graph_config):
    training_config = parameters["training_config"]
    stage = training_config["target-stage"]
    src = training_config["experiment"]["src"]
    trg = training_config["experiment"]["trg"]
    datasets = parameters["training_config"]["datasets"]

    def filter(task):
        # These attributes will be present on tasks from all stages
        if task.attributes.get("stage") != stage:
            return False

        if task.attributes.get("src_locale") != src:
            return False

        if task.attributes.get("trg_locale") != trg:
            return False

        # Datasets are only applicable to dataset-specific tasks. If these
        # attribute isn't present on the task it can be assumed to be included
        # if the above attributes matched, as it will be a task that is either
        # agnostic of datasets, or folds in datasets from earlier tasks.
        # (Pulling in the appropriate datasets for these task must be handled at
        # the task generation level, usually by the `find_upstreams` transform.)
        if "dataset" in task.attributes:
            dataset_category = task.attributes["dataset-category"]
            for ds in datasets.get(dataset_category, []):
                provider, dataset = ds.split("_", 1)
                # If the task is for any of the datasets in the specified category,
                # it's a match, and should be included in the target tasks.
                if (
                    task.attributes["provider"] == provider
                    and task.attributes["dataset"] == dataset
                ):
                    break

        return True

    return [label for label, task in full_task_graph.tasks.items() if filter(task)]


translations_taskgraph/transforms/cached_tasks.py

# This Source Code Form is subject to the terms of the Mozilla Public
# License, v. 2.0. If a copy of the MPL was not distributed with this
# file, You can obtain one at http://mozilla.org/MPL/2.0/.
#
# This transform is largely of the upstream `cached_task` transform in Taskgraph.
# It exists because there are two features that we need that are missing upstream:
# - The ability to influence the cache digest from parameters.
#   (https://github.com/taskcluster/taskgraph/issues/391)


import itertools

import taskgraph
from taskgraph.transforms.base import TransformSequence
from taskgraph.transforms.cached_tasks import order_tasks, format_task_digest
from taskgraph.util.cached_tasks import add_optimization
from taskgraph.util.hash import hash_path
from taskgraph.util.schema import Schema, optionally_keyed_by, resolve_keyed_by
from voluptuous import ALLOW_EXTRA, Any, Required, Optional

from translations_taskgraph.util.dict_helpers import deep_get

transforms = TransformSequence()


SCHEMA = Schema(
    {
        Required("attributes"): {
            Required("cache"): {
                Required("type"): str,
                Optional("resources"): optionally_keyed_by("provider", [str]),
                Optional("version"): int,
                Optional("from-parameters"): {
                    str: Any([str], str),
                },
            },
        },
    },
    extra=ALLOW_EXTRA,
)

transforms = TransformSequence()
transforms.add_validate(SCHEMA)


@transforms.add
def resolved_keyed_by_fields(config, jobs):
    for job in jobs:
        provider = job["attributes"].get("provider", None)
        resolve_keyed_by(
            job["attributes"]["cache"],
            "resources",
            item_name=job["description"],
            **{"provider": provider},
        )

        yield job


@transforms.add
def add_cache(config, jobs):
    for job in jobs:
        cache = job["attributes"]["cache"]
        cache_type = cache["type"]
        cache_resources = cache.get("resources")
        cache_parameters = cache.get("from-parameters")
        cache_version = cache.get("version")
        digest_data = []
        digest_data.extend(list(itertools.chain.from_iterable(job["worker"]["command"])))

        if cache_resources:
            for r in cache_resources:
                digest_data.append(hash_path(r))

        if cache_parameters:
            for param, path in cache_parameters.items():
                if isinstance(path, str):
                    value = deep_get(config.params, path)
                    digest_data.append(f"{param}:{value}")
                else:
                    for choice in path:
                        value = deep_get(config.params, choice)
                        if value is not None:
                            digest_data.append(f"{param}:{value}")
                            break

        if cache_version:
            digest_data.append(str(cache_version))

        job["cache"] = {
            "type": cache_type,
            # Upstream cached tasks use "/" as a separator for different parts
            # of the digest. If we don't remove them, caches are busted for
            # anything with a "/" in its label.
            "name": job["label"].replace("/", "_"),
            "digest-data": digest_data,
        }

        yield job


@transforms.add
def cache_task(config, tasks):
    if taskgraph.fast:
        for task in tasks:
            yield task
        return

    digests = {}
    for task in config.kind_dependencies_tasks.values():
        if "cached_task" in task.attributes:
            digests[task.label] = format_task_digest(task.attributes["cached_task"])

    for task in order_tasks(config, tasks):
        cache = task.pop("cache", None)
        if cache is None:
            yield task
            continue

        dependency_digests = []
        for p in task.get("dependencies", {}).values():
            if p in digests:
                dependency_digests.append(digests[p])
            else:
                raise Exception(
                    "Cached task {} has uncached parent task: {}".format(task["label"], p)
                )
        digest_data = cache["digest-data"] + sorted(dependency_digests)
        add_optimization(
            config,
            task,
            cache_type=cache["type"],
            cache_name=cache["name"],
            digest_data=digest_data,
        )
        digests[task["label"]] = format_task_digest(task["attributes"]["cached_task"])

        yield task


translations_taskgraph/transforms/cast_to.py

# This Source Code Form is subject to the terms of the Mozilla Public
# License, v. 2.0. If a copy of the MPL was not distributed with this
# file, You can obtain one at http://mozilla.org/MPL/2.0/.
#
# This transform has a very simple job: cast fields in a task definition from
# one type to another. The only reason it exists is because we have some fields
# that `task_context` fills in as a string, but that other transforms or code
# requires to be an int.

from taskgraph.transforms.base import TransformSequence
from taskgraph.util.schema import Schema
from voluptuous import ALLOW_EXTRA, Optional


SCHEMA = Schema(
    {
        Optional("cast-to"): {
            Optional("int"): [str],
        },
    },
    extra=ALLOW_EXTRA,
)

transforms = TransformSequence()
transforms.add_validate(SCHEMA)


@transforms.add
def cast(config, jobs):
    for job in jobs:
        casts = job.pop("cast-to", {})
        for field in casts.get("int", []):
            container, subfield = job, field
            while "." in subfield:
                f, subfield = subfield.split(".", 1)
                container = container[f]

            container[subfield] = int(container[subfield])

        yield job


translations_taskgraph/transforms/continuation.py

# This Source Code Form is subject to the terms of the Mozilla Public
# License, v. 2.0. If a copy of the MPL was not distributed with this
# file, You can obtain one at http://mozilla.org/MPL/2.0/.

"""
This transform is responsible for implementing corpus continuation, where previous
corpora from training runs can be re-used. This rewrites the dependencies for the
tasks in order to trim out tasks that are not needed. For instance providing all
the data to train a student, (vocab, corpora, backwards model) the teacher step
will be omitted from the training graph, and a vastly simplified graph will be
produced.
"""

from typing import Any, Iterable, Literal, Optional, TypedDict
from taskgraph.transforms.base import TransformSequence, TransformConfig

transforms = TransformSequence()


# This is minimally typed to just provide some hints for this transform.
Job = TypedDict(
    "Job",
    {
        "name": str,
        "fetches": Optional[dict[str, list[dict[str, Any]]]],
        "dependencies": dict[str, str],
        "from-deps": dict[str, Any],
        "task-context": dict[str, Any],
        "attributes": dict[str, Any],
    },
)


Vocab = TypedDict(
    "Vocab",
    {
        "src": str,
        "trg": str,
    },
)

BackwardsModel = TypedDict(
    "BackwardsModel",
    {
        "url": str,
        "mode": Literal["continue"] | Literal["init"] | Literal["use"],
        "type": Literal["default"] | Literal["opusmt"],
    },
)

Models = TypedDict(
    "Models",
    {
        "backwards": Optional[BackwardsModel],
    },
)

Corpus = TypedDict(
    "Corpus",
    {
        "src": str,
        "trg": str,
        "alignments": Optional[str],
        "tok-src": Optional[str],
        "tok-trg": Optional[str],
    },
)

Continuation = TypedDict(
    "Continuation",
    {
        "models": Optional[Models],
        "corpora": Optional[dict[str, Corpus]],
        "vocab": Optional[Vocab],
    },
)

transforms = TransformSequence()


def rewrite_dependencies(job: Job, old_task: str, new_task: str):
    # Rewrite the dependences
    # For example rewrite:
    #   dependencies:
    #       merge-corpus: merge-corpus-{src_locale}-{trg_locale}
    # To:
    #   dependencies:
    #       corpus-original-parallel: corpus-original-parallel-{src_locale}-{trg_locale}
    dependencies = job.get("dependencies", {})
    task_dependency = dependencies.pop(old_task, None)

    if task_dependency:
        dependencies[new_task] = new_task + "-{src_locale}-{trg_locale}"

    # Rewrite the fetches name to the new task.
    # For example here:
    #   fetches.merge-corpus -> fetches.corpus-original-parallel
    #
    # fetches:
    #     toolchain:
    #         - marian
    #     merge-corpus:
    #         - artifact: corpus.{src_locale}.zst
    #           extract: false
    #         - artifact: corpus.{trg_locale}.zst
    #           extract: false
    fetches = job.get("fetches") or {}
    artifacts = fetches.pop(old_task, None)
    if artifacts:
        fetches[new_task] = artifacts

    # Rewrite the fetches for "from-deps", which is the mechanism used for chunking.
    fetches = job.get("from-deps", {}).get("fetches", {})
    artifacts = fetches.pop(old_task, None)
    if artifacts:
        fetches[new_task] = artifacts

    # Replace any substitution fields that mention the old task.
    substitution_fields: list[str] = job.get("task-context", {}).get("substitution-fields", [])
    for key, value in enumerate(substitution_fields):
        substitution_fields[key] = value.replace(old_task, new_task)


@transforms.add
def apply_continuation(config: TransformConfig, jobs: Iterable[Job]):
    """
    When an existing corpus is available, rewriting the task graph to omit the steps
    needed to generate that corpus.

    Rewrites dependencies:
        merge-corpus -> corpus-original-parallel
        merge-mono-trg -> corpus-backtranslations
        cefilter -> corpus-distillation
    """
    training_config: dict = config.params["training_config"]
    continuation: Continuation = training_config.get("continuation", {})

    corpora = continuation.get("corpora")
    corpus_backtranslations = validate_corpora_config(corpora, "backtranslations")
    corpus_original_parallel = validate_corpora_config(corpora, "original-parallel")
    corpus_student_distillation = validate_corpora_config(corpora, "student-distillation")

    vocab: Optional[Vocab] = continuation.get("vocab")
    models: Optional[Models] = continuation.get("models")
    # If the models are in the "use" mode and are the "default" type, they can be used
    # for changing dependencies of tasks.
    model_backwards: Optional[BackwardsModel] = None

    if models:
        model_backwards = models.get("backwards")

        if (
            not model_backwards
            or model_backwards["mode"] != "use"
            or model_backwards["type"] != "default"
        ):
            model_backwards = None

    for job in jobs:
        # The stage is often the identifier you want rather than job name, for instance,
        # "alignments-student" is the stage, while "{src_locale}-{trg_locale}" is the
        # actual job name at this point in time.
        stage = job.get("attributes", {}).get("stage")
        name = job["name"]

        # Ensure continuation tasks don't get produced unless they are explicitly requested.
        if stage == "continuation":
            if (
                not corpus_backtranslations
                and name == "backtranslations-{src_locale}-{trg_locale}"
            ):
                continue
            if (
                not corpus_original_parallel
                and name == "original-parallel-{src_locale}-{trg_locale}"
            ):
                continue
            if (
                not corpus_student_distillation
                and name == "student-distillation-{src_locale}-{trg_locale}"
            ):
                continue
            if not vocab and name == "vocab-{src_locale}-{trg_locale}":
                continue
            if not model_backwards and name == "backwards-{src_locale}-{trg_locale}":
                continue

        if corpus_original_parallel:
            if stage == "merge-corpus":
                # Skip any jobs that should never be produced:
                continue

            rewrite_dependencies(
                job, old_task="merge-corpus", new_task="continuation-corpus-original-parallel"
            )
            if corpus_original_parallel.get("alignments"):
                rewrite_dependencies(
                    job,
                    old_task="alignments-original",
                    new_task="continuation-corpus-original-parallel",
                )

        if corpus_backtranslations:
            if stage == "merge-mono" and name == "trg":
                # Skip any jobs that should never be produced:
                continue

            rewrite_dependencies(
                job, old_task="merge-mono-trg", new_task="continuation-corpus-backtranslations"
            )
            if corpus_backtranslations.get("alignments"):
                if stage == "alignments-backtranslated":
                    continue
                rewrite_dependencies(
                    job,
                    old_task="alignments-backtranslated",
                    new_task="continuation-corpus-backtranslations",
                )

        if corpus_student_distillation:
            if stage in {
                "cefilter",
                "train-teacher",
                "evaluate-teacher",
                "evaluate-teacher-ensemble",
            }:
                # Skip any jobs that should never be produced:
                continue

            rewrite_dependencies(
                job, old_task="cefilter", new_task="continuation-corpus-student-distillation"
            )
            if corpus_student_distillation.get("alignments"):
                if stage == "alignments-student":
                    continue
                rewrite_dependencies(
                    job,
                    old_task="alignments-student",
                    new_task="continuation-corpus-student-distillation",
                )

        if vocab:
            if stage == "train-vocab":
                # Skip any jobs that should never be produced:
                continue

            rewrite_dependencies(job, old_task="train-vocab", new_task="continuation-vocab")

        if model_backwards:
            if stage in {"train-backwards", "evaluate-backwards"}:
                # Skip any jobs that should never be produced:
                continue
            rewrite_dependencies(
                job, old_task="train-backwards", new_task="continuation-model-backwards"
            )

        # If alignments need to be re-generated, don't attempt to re-use alignment priors.
        if (corpus_student_distillation and not corpus_student_distillation.get("alignments")) or (
            corpus_backtranslations and not corpus_backtranslations.get("alignments")
        ):
            remove_alignment_priors_dependencies(job)

        yield job


def remove_alignment_priors_dependencies(job: Job):
    """
    Removes the following in case the corpus.priors are not available.

        dependencies:
            alignments-original: alignments-original-{src_locale}-{trg_locale}
        fetches:
            alignments-original:
                - artifact: corpus.priors
    """
    fetches = job.get("fetches")
    dependencies = job.get("dependencies")
    if not dependencies or not fetches:
        return
    alignments = fetches.get("alignments-original", [])
    if len(alignments) == 1 and alignments[0].get("artifact") == "corpus.priors":
        dependencies.pop("alignments-original")
        fetches.pop("alignments-original")


def validate_corpora_config(
    corpora: Optional[dict[str, Corpus]], corpus_key: str
) -> Optional[Corpus]:
    """
    Ensure that all of the files are defined if using an existing corpus.
    """
    if not corpora:
        return None

    corpus_files = corpora.get(corpus_key)

    if not corpus_files:
        return None

    def raise_error(file_key: str):
        raise ValueError(f'The "{file_key}" key was not found in the "corpora.{corpus_key}"')

    if "src" not in corpus_files:
        raise_error("src")
    if "trg" not in corpus_files:
        raise_error("trg")

    if "tok-src" in corpus_files or "tok-trg" in corpus_files or "alignments" in corpus_files:
        if "tok-src" not in corpus_files:
            raise_error("tok-src")
        if "tok-trg" not in corpus_files:
            raise_error("tok-src")
        if "alignments" not in corpus_files:
            raise_error("alignments")

    return corpus_files  # type: ignore[reportReturnType]


translations_taskgraph/transforms/dependency_dummies.py

# This Source Code Form is subject to the terms of the Mozilla Public
# License, v. 2.0. If a copy of the MPL was not distributed with this
# file, You can obtain one at http://mozilla.org/MPL/2.0/.
#
# This transform sequence is used to workaround the fact that Taskcluster only
# allows a task to have ~100 upstream dependencies. This transform will clone
# each job given to and split the dependencies across these clones. Eg:
# - A job with 50 dependencies will result in 1 job yielded
# - A job with 150 dependencies will result in 2 jobs yielded
# - A job with 1550 depedencies will result in 16 jobs yielded
#
# The jobs yielded are identical to their original, aside from
# a `-N` being appended to their name, where N is a distinct number.

import copy

from taskgraph import MAX_DEPENDENCIES
from taskgraph.transforms.base import TransformSequence

transforms = TransformSequence()


def yield_job(orig_job, deps, count):
    job = copy.deepcopy(orig_job)
    job["dependencies"] = deps
    job["name"] = "{}-{}".format(orig_job["name"], count)

    return job


@transforms.add
def add_dependencies(config, jobs):
    for job in jobs:
        count = 1
        deps = {}

        for dep_label in sorted(job["dependencies"].keys()):
            deps[dep_label] = dep_label
            if len(deps) == MAX_DEPENDENCIES:
                yield yield_job(job, deps, count)
                deps = {}
                count += 1

        if deps:
            yield yield_job(job, deps, count)
            count += 1


translations_taskgraph/transforms/find_upstreams.py

# This Source Code Form is subject to the terms of the Mozilla Public
# License, v. 2.0. If a copy of the MPL was not distributed with this
# file, You can obtain one at http://mozilla.org/MPL/2.0/.
#
# This transform sequence sets `dependencies` and `fetches` based on
# the information provided in the `upstreams-config` data in each job
# and the given parameters.

# It will through all tasks generated from `kind-dependencies` and
# set any tasks that match the following conditions as dependencies:
# - src and trg locale given match the {src,trg}_locale attributes on the upstream task
# - `upstream-task-attributes` given match their equivalents on the upstream task
# - `dataset` attribute on the upstream task is one of the datasets provided in `parameters`
#   for the `dataset-category` given in the job.
#
# Additionally, fetches will be added for those tasks for each entry in `upstream-artifacts`.
#
# (It is not ideal that this transform hardcodes dataset handling, but because kinds are
# completely unaware of parameters, there's no other real way to do this.)

import copy

from taskgraph.transforms.base import TransformSequence
from taskgraph.util.schema import Schema, optionally_keyed_by, resolve_keyed_by
from voluptuous import ALLOW_EXTRA, Required, Optional

from translations_taskgraph.util.substitution import substitute
from translations_taskgraph.util.dataset_helpers import sanitize_dataset_name

SCHEMA = Schema(
    {
        Required("upstreams-config"): {
            Required("upstream-task-attributes"): {
                str: optionally_keyed_by("cleaning-type", str),
            },
            Required("upstream-artifacts"): [str],
        },
    },
    extra=ALLOW_EXTRA,
)

by_locales = TransformSequence()
by_locales.add_validate(SCHEMA)

MONO = Schema(
    {
        Required("upstreams-config"): {
            Required("upstream-task-attributes"): {
                str: optionally_keyed_by("cleaning-type", str),
            },
            Required("upstream-artifacts"): [str],
            Optional("substitution-fields"): [str],
        },
    },
    extra=ALLOW_EXTRA,
)

mono = TransformSequence()
mono.add_validate(MONO)


def get_cleaning_type(upstreams):
    candidates = set()

    for upstream in upstreams:
        if upstream.kind not in ("bicleaner", "clean-corpus"):
            continue

        candidates.add(upstream.attributes["cleaning-type"])

    for type_ in ("bicleaner-ai", "clean-corpus"):
        if type_ in candidates:
            return type_

    # Default to bicleaner-ai if no cleaning steps were found.
    return "bicleaner-ai"


@by_locales.add
def resolve_keyed_by_fields(config, jobs):
    for job in jobs:
        upstreams_config = job["upstreams-config"]
        if upstreams_config.get("upstream-task-attributes", {}).get("cleaning-type"):
            cleaning_type = get_cleaning_type(config.kind_dependencies_tasks.values())

            resolve_keyed_by(
                upstreams_config,
                "upstream-task-attributes.cleaning-type",
                item_name=job["description"],
                **{"cleaning-type": cleaning_type},
            )

        yield job


@by_locales.add
def upstreams_for_locales(config, jobs):
    datasets = config.params.get("training_config", {}).get("datasets")
    if not datasets:
        # There are no dataset jobs to yield.
        return
    for job in jobs:
        dataset_category = job["attributes"]["dataset-category"]
        target_datasets = datasets.get(dataset_category, [])
        upstreams_config = job.pop("upstreams-config")
        artifacts = upstreams_config["upstream-artifacts"]
        upstream_task_attributes = upstreams_config["upstream-task-attributes"]

        subjob = copy.deepcopy(job)
        subjob.setdefault("dependencies", {})
        subjob.setdefault("fetches", {})

        # Now that we've resolved which type of upstream task we want, we need to
        # find all instances of that task for our locale pair, add them to our
        # dependencies, and the necessary artifacts to our fetches.
        for task in sorted(config.kind_dependencies_tasks.values(), key=lambda t: t.label):
            # Filter out any tasks that don't match the desired attributes.
            if any(task.attributes.get(k) != v for k, v in upstream_task_attributes.items()):
                continue

            provider = task.attributes["provider"]
            dataset = task.attributes["dataset"]
            task_dataset = f"{provider}_{dataset}"

            # Filter out any tasks that don't match a desired dataset
            if task_dataset not in target_datasets:
                continue

            subs = {
                "src_locale": task.attributes["src_locale"],
                "trg_locale": task.attributes["trg_locale"],
                "dataset_sanitized": sanitize_dataset_name(dataset),
            }

            subjob["dependencies"][task.label] = task.label
            subjob["fetches"].setdefault(task.label, [])
            for artifact in sorted(artifacts):
                subjob["fetches"][task.label].append(
                    {
                        "artifact": artifact.format(**subs),
                        "extract": False,
                    }
                )

        yield subjob


@mono.add
def upstreams_for_mono(config, jobs):
    training_config = config.params.get("training_config", {})
    datasets = training_config.get("datasets")
    src = training_config["experiment"]["src"]
    trg = training_config["experiment"]["trg"]
    if not datasets:
        # There are no dataset jobs to yield.
        return

    for job in jobs:
        dataset_category = job["attributes"]["dataset-category"]
        target_datasets = datasets.get(dataset_category, [])
        job.setdefault("dependencies", {})
        job.setdefault("fetches", {})
        upstreams_config = job.pop("upstreams-config")
        upstream_task_attributes = upstreams_config["upstream-task-attributes"]
        artifacts = upstreams_config["upstream-artifacts"]
        substitution_fields = upstreams_config.get("substitution-fields", [])

        for task in sorted(config.kind_dependencies_tasks.values(), key=lambda t: t.label):
            # Filter out any tasks that don't match the desired attributes.
            if any(task.attributes.get(k) != v for k, v in upstream_task_attributes.items()):
                continue

            provider = task.attributes["provider"]
            dataset = task.attributes["dataset"]
            task_dataset = f"{provider}_{dataset}"

            # Filter out any tasks that don't match a desired dataset
            if task_dataset not in target_datasets:
                continue

            if dataset_category == "mono-src":
                locale = src
            elif dataset_category == "mono-trg":
                locale = trg
            else:
                raise Exception(
                    "Don't use `find_upstreams:mono` without the `mono-src` or `mono-trg` category!"
                )

            job["dependencies"][task.label] = task.label
            job["fetches"].setdefault(task.label, [])

            subs = {
                "provider": provider,
                "dataset": dataset,
                "dataset_sanitized": sanitize_dataset_name(dataset),
                "locale": locale,
                "src_locale": src,
                "trg_locale": trg,
            }

            for field in substitution_fields:
                container, subfield = job, field
                while "." in subfield:
                    f, subfield = subfield.split(".", 1)
                    container = container[f]

                container[subfield] = substitute(container[subfield], **subs)

            for artifact in sorted(artifacts):
                job["fetches"][task.label].append(
                    {
                        "artifact": artifact.format(**subs),
                        "extract": False,
                    }
                )

            job["attributes"]["src_locale"] = src
            job["attributes"]["trg_locale"] = trg

        yield job


translations_taskgraph/transforms/from_datasets.py

# This Source Code Form is subject to the terms of the Mozilla Public
# License, v. 2.0. If a copy of the MPL was not distributed with this
# file, You can obtain one at http://mozilla.org/MPL/2.0/.
#
# The transform sequences in this file are responsible for "fanning out" a job
# that operates on individual datasets into N jobs based on the parameters
# given. By default, it will fan out into one job for each dataset in the
# training config for from _all_ categories. This can be restricted by one or
# more of:
# - `category` to limit to the datasets in a particular category (eg: `train`)
# - `provider` to limit to datasets from particular provider (eg: `flores`)
# - `exclude-locales` to avoid generating jobs for given language pairs, eg:
#   {"src": "en", "trg": "ru"}. (This is primarily useful for tasks like
#   `bicleaner-ai` which only work if a bicleaner pack is available for a
#   locale pair.
#
# These transform sequences will also perform string formatting in the given
# `substitution-fields`. (Normally this would be done with `task-context`, but
# this transform is much more aware of things like `provider` and `dataset`,
# so it's simply easier to do it here for fields that need these things.) Both
# transform sequences make the following variables available:
# - `provider` is the dataset provider. Eg: the `opus` part of `opus_Books/v1`.
# - `dataset` is the dataset name. Eg: the `Books/v1` part of `opus_Books/v1`.
# - `dataset_sanitized` is the dataset name with `/` and `.` characters replaced
#   with an `_` to make them more suitable in filenames and URLs.
#   Eg: `Books_v1` from `Books/V1`.
# - `src_locale` is the `src` from the training config.
# - `trg_locale` is the `trg` from the tarining config.
#
# Note that there are two available transform sequences here: `per_dataset`
# and `mono`. `mono` does everything that `per_dataset` does, but also:
# - Requires a `category` of either `mono-src` or `mono-trg`. (It doesn't make
#   sense to use this sequence without a category, or with other ones.)
# - Makes `locale` available as a substitution parameter - which will either
#   be set to the `src` or `trg` locale, depending on which category was used.

import copy

from taskgraph.transforms.base import TransformSequence
from taskgraph.util.schema import Schema
from voluptuous import ALLOW_EXTRA, Optional

from translations_taskgraph.util.substitution import substitute
from translations_taskgraph.util.dataset_helpers import sanitize_dataset_name

SCHEMA = Schema(
    {
        Optional("dataset-config"): {
            # Fields in each `job` that need to be substituted with data
            # provided by this transform.
            Optional("substitution-fields"): [str],
            Optional("category"): str,
            Optional("provider"): str,
            Optional("exclude-locales"): [
                {
                    "src": str,
                    "trg": str,
                },
            ],
        },
    },
    extra=ALLOW_EXTRA,
)

per_dataset = TransformSequence()
per_dataset.add_validate(SCHEMA)

mono = TransformSequence()
mono.add_validate(SCHEMA)


@per_dataset.add
def jobs_from_datasets(config, jobs):
    for job in jobs:
        dataset_config = job.pop("dataset-config", {})
        category = dataset_config.get("category", "")
        provider = dataset_config.get("provider", "")
        substitution_fields = dataset_config.get("substitution-fields", [])
        exclude_locales = dataset_config.get("exclude-locales", [])
        datasets = config.params["training_config"]["datasets"]
        src = config.params["training_config"]["experiment"]["src"]
        trg = config.params["training_config"]["experiment"]["trg"]

        included_datasets = set()
        if category:
            included_datasets.update(datasets.get(category, []))
        else:
            for sets in datasets.values():
                included_datasets.update(sets)

        if {"src": src, "trg": trg} in exclude_locales:
            continue

        for full_dataset in included_datasets:
            dataset_provider, dataset = full_dataset.split("_", 1)
            if provider and provider != dataset_provider:
                continue

            subjob = copy.deepcopy(job)

            subs = {
                "provider": dataset_provider,
                "dataset": full_dataset,
                "dataset_sanitized": sanitize_dataset_name(dataset),
                "src_locale": src,
                "trg_locale": trg,
            }
            for field in substitution_fields:
                container, subfield = subjob, field
                while "." in subfield:
                    f, subfield = subfield.split(".", 1)
                    container = container[f]

                container[subfield] = substitute(container[subfield], **subs)

            subjob.setdefault("attributes", {})
            subjob["attributes"]["provider"] = dataset_provider
            subjob["attributes"]["dataset"] = dataset
            subjob["attributes"]["src_locale"] = src
            subjob["attributes"]["trg_locale"] = trg

            yield subjob


@mono.add
def jobs_for_mono_datasets(config, jobs):
    for job in jobs:
        dataset_config = job.pop("dataset-config", {})
        category = dataset_config.get("category")
        provider = dataset_config.get("provider", "")
        substitution_fields = dataset_config.get("substitution-fields", [])
        exclude_locales = dataset_config.get("exclude-locales", [])
        datasets = config.params["training_config"]["datasets"]
        src = config.params["training_config"]["experiment"]["src"]
        trg = config.params["training_config"]["experiment"]["trg"]

        if {"src": src, "trg": trg} in exclude_locales:
            continue

        if category not in ("mono-src", "mono-trg"):
            raise Exception(
                "from_datasets:mono can only be used with mono-src and mono-trg categories"
            )

        included_datasets = set()
        if category:
            included_datasets.update(datasets.get(category, []))
        else:
            for sets in datasets.values():
                included_datasets.update(sets)

        for full_dataset in included_datasets:
            dataset_provider, dataset = full_dataset.split("_", 1)
            if provider and provider != dataset_provider:
                continue

            subjob = copy.deepcopy(job)

            if category == "mono-src":
                locale = src
            elif category == "mono-trg":
                locale = trg
            else:
                raise Exception(
                    "from_datasets:mono can only be used with mono-src and mono-trg categories"
                )

            subs = {
                "provider": dataset_provider,
                "dataset": full_dataset,
                "dataset_sanitized": sanitize_dataset_name(dataset),
                "locale": locale,
                "src_locale": src,
                "trg_locale": trg,
            }
            for field in substitution_fields:
                container, subfield = subjob, field
                while "." in subfield:
                    f, subfield = subfield.split(".", 1)
                    container = container[f]

                container[subfield] = substitute(container[subfield], **subs)

            subjob.setdefault("attributes", {})
            subjob["attributes"]["provider"] = dataset_provider
            subjob["attributes"]["dataset"] = dataset
            subjob["attributes"]["locale"] = locale
            subjob["attributes"]["src_locale"] = src
            subjob["attributes"]["trg_locale"] = trg

            yield subjob


translations_taskgraph/transforms/marian_args.py

# This Source Code Form is subject to the terms of the Mozilla Public
# License, v. 2.0. If a copy of the MPL was not distributed with this
# file, You can obtain one at http://mozilla.org/MPL/2.0/.
#
# This is a simple transform sequence that takes the `marian_args` referenced
# in a job, turns its key/value pairs into standard unix command line
# options, and makes them available to `task-context` substitutions as
# `marian_args`. For example:
# If the `marian_args` input resolves to: `{"beam-size": "12", "mini-batch-words": "2000"}`
# Then `marian_args` in `task-context` will be: `--beam-size 12 --mini-batch-words 2000`

from taskgraph.transforms.base import TransformSequence
from taskgraph.util.schema import Schema
from voluptuous import ALLOW_EXTRA, Required

from translations_taskgraph.util.dict_helpers import deep_get

SCHEMA = Schema(
    {
        Required("marian-args"): {
            Required("from-parameters"): str,
        },
    },
    extra=ALLOW_EXTRA,
)

transforms = TransformSequence()
transforms.add_validate(SCHEMA)


@transforms.add
def render_command(config, jobs):
    for job in jobs:
        marian_args = ""
        for name, value in deep_get(
            config.params, job.pop("marian-args")["from-parameters"]
        ).items():
            marian_args = marian_args + f" --{name} {value}"

        if "from-object" not in job["task-context"]:
            job["task-context"]["from-object"] = {}
        job["task-context"]["from-object"]["marian_args"] = marian_args

        yield job


translations_taskgraph/transforms/skip_unless_inference_changed.py

# This Source Code Form is subject to the terms of the Mozilla Public
# License, v. 2.0. If a copy of the MPL was not distributed with this
# file, You can obtain one at http://mozilla.org/MPL/2.0/.
#
# This transform sequence will remove all jobs unless at least one inference
# impacting thing (an inference script or relevant Taskcluster code) has changed
# (This is done with the `files_changed` helper, which uses data in the
# parameters to determine files changed between the `base` and `head` revisions.)

# When upstream taskgraph supports better selection (https://github.com/taskcluster/taskgraph/issues/369)
# this can be replaced with it.

import os
from pathlib import Path

from taskgraph.transforms.base import TransformSequence

KIND_DIR = Path(__file__).parent.parent.parent / "kinds"

# Kinds are slightly special - there are some kinds that don't affect inference,
# and changing them shouldn't force inference to run.
INCLUDE_KINDS = ["inference"]
# Touching any file in any of these directories is considered an inference change
INFERENCE_DIRS = [
    "inference/**",
    "taskcluster/docker/inference/**",
]
INFERENCE_DIRS.extend(
    f"taskcluster/kinds/{kind}" for kind in os.listdir(KIND_DIR) if kind in INCLUDE_KINDS
)

transforms = TransformSequence()


@transforms.add
def skip_unless_inference_changed(config, jobs):
    for job in jobs:
        job.setdefault("optimization", {})
        job["optimization"]["skip-unless-changed"] = INFERENCE_DIRS

        yield job


translations_taskgraph/transforms/skip_unless_pipeline_changed.py

# This Source Code Form is subject to the terms of the Mozilla Public
# License, v. 2.0. If a copy of the MPL was not distributed with this
# file, You can obtain one at http://mozilla.org/MPL/2.0/.
#
# This transform sequence will remove all jobs unless at least one pipeline
# impacting thing (a pipeline script or relevant Taskcluster code) has changed
# (This is done with the `files_changed` helper, which uses data in the
# parameters to determine files changed between the `base` and `head` revisions.)

# When upstream taskgraph supports better selection (https://github.com/taskcluster/taskgraph/issues/369)
# this can be replaced with it.

import os
from pathlib import Path

from taskgraph.transforms.base import TransformSequence

KIND_DIR = Path(__file__).parent.parent.parent / "kinds"

# Kinds are slightly special - there are some kinds that don't affect the pipeline,
# and changing them shouldn't force the pipeline to run.
EXCLUDE_KINDS = ["test"]
# Touching any file in any of these directories is considered a pipeline change
PIPELINE_DIRS = [
    "pipeline/**",
    "taskcluster/docker/**",
    "taskcluster/requirements.txt",
    "taskcluster/scripts/**",
    "taskcluster/translations_taskgraph/**",
]
PIPELINE_DIRS.extend(
    f"taskcluster/kinds/{kind}" for kind in os.listdir(KIND_DIR) if kind not in EXCLUDE_KINDS
)

transforms = TransformSequence()


@transforms.add
def skip_unless_pipeline_changed(config, jobs):
    for job in jobs:
        job.setdefault("optimization", {})
        job["optimization"]["skip-unless-changed"] = PIPELINE_DIRS

        yield job


translations_taskgraph/transforms/training_continuation.py

from taskgraph.transforms.base import TransformSequence
from urllib.parse import urljoin
import os

CONTINUE_TRAINING_ARTIFACTS = (
    "devset.out",
    "model.npz",
    "model.npz.best-bleu-detok.npz",
    "model.npz.best-bleu-detok.npz.decoder.yml",
    "model.npz.best-ce-mean-words.npz",
    "model.npz.best-ce-mean-words.npz.decoder.yml",
    "final.model.npz.best-chrf.npz",
    "model.npz.best-chrf.npz",
    "final.model.npz.best-chrf.npz.decoder.yml",
    "model.npz.best-chrf.npz.decoder.yml",
    "model.npz.decoder.yml",
    "model.npz.optimizer.npz",
    "model.npz.progress.yml",
    "model.npz.yml",
    "train.log",
    "valid.log",
    "vocab.spm",
)

INITIALIZE_MODEL_ARTIFACTS = (
    "model.npz.best-bleu-detok.npz",
    "model.npz.best-ce-mean-words.npz",
    "final.model.npz.best-chrf.npz",
    "model.npz.best-chrf.npz",
)


def get_artifact_mount(url, directory, artifact_name):
    normalized_url = f"{url}/" if not url.endswith("/") else url
    artifact_url = urljoin(normalized_url, artifact_name)
    return {
        "content": {
            "url": artifact_url,
        },
        "file": os.path.join(directory, artifact_name),
    }


def get_artifact_mounts(urls, directory, artifact_names):
    for url in urls:
        artifact_mounts = []
        for artifact_name in artifact_names:
            artifact_mounts.append(get_artifact_mount(url, directory, artifact_name))
        yield artifact_mounts


transforms = TransformSequence()


@transforms.add
def add_pretrained_model_mounts(config, jobs):
    pretrained_models = config.params["training_config"].get("continuation", {}).get("models", {})
    for job in jobs:
        pretrained_models_training_artifact_mounts = {
            pretrained_model: get_artifact_mounts(
                (
                    # Only teachers can have multiple urls.
                    pretrained_models[pretrained_model]["urls"]
                    if pretrained_models[pretrained_model].get("urls")
                    else [pretrained_models[pretrained_model]["url"]]
                ),
                "./artifacts",
                INITIALIZE_MODEL_ARTIFACTS
                if pretrained_models[pretrained_model]["mode"] == "init"
                else CONTINUE_TRAINING_ARTIFACTS,
            )
            for pretrained_model in pretrained_models
        }
        pretrained_model_training_artifact_mounts = next(
            pretrained_models_training_artifact_mounts.get(config.kind, iter((None,)))
        )
        if pretrained_model_training_artifact_mounts:
            mounts = job["worker"].get("mounts", [])
            mounts.extend(pretrained_model_training_artifact_mounts)
            job["worker"]["mounts"] = mounts
            job["dependencies"].pop("train-vocab")
            job["fetches"].pop("train-vocab")

            if pretrained_models[config.kind]["mode"] == "use":
                # In use mode, no upstream dependencies of the training job are needed - the
                # task simply republishes the pretrained artifacts.
                job["dependencies"] = {}
                job["fetches"] = {}

                # We also need to adjust the caching parameters. The only thing that should influence
                # the cache digest are the pretrained model parameters.
                job["attributes"]["cache"]["resources"] = []
                job["attributes"]["cache"]["from-parameters"] = {
                    p: v
                    for p, v in job["attributes"]["cache"]["from-parameters"].items()
                    if p.startswith("pretrained")
                }

        yield job


evaluate_stage = TransformSequence()


@evaluate_stage.add
def skip_for_pretrained_models(config, jobs):
    # Find the types of pretrained models that are being used. This makes
    # it easier to filter them out in the loop below.
    pretrained_models = [
        pretrained.split("-")[-1].replace("backwards", "backward")
        for pretrained in config.params["training_config"]
        .get("continuation", {})
        .get("models", {})
        .keys()
    ]

    for job in jobs:
        if any([pretrained in job["attributes"]["stage"] for pretrained in pretrained_models]):
            continue

        yield job


translations_taskgraph/transforms/worker_selection.py

# This Source Code Form is subject to the terms of the Mozilla Public
# License, v. 2.0. If a copy of the MPL was not distributed with this
# file, You can obtain one at http://mozilla.org/MPL/2.0/.
#
# This transform sequence injects worker-specific environment variables
# (such as those that dependent on the number and type of GPUs a worker has)
# into task definitions. This avoids the need to discover this information at
# runtime, or adjust in kinds when changing worker types.

from taskgraph.transforms.base import TransformSequence
from taskgraph.util.schema import evaluate_keyed_by

transforms = TransformSequence()


@transforms.add
def set_worker_type(config, jobs):
    """Determines the general type of worker each task wants, which sometimes
    depends on `tasks-for`. Tasks typically will end up specifying one of the
    worker `aliases` from config.yml after this is evaluated, eg: b-cpu,
    b-largegpu-largedisk."""

    training_config = config.params.get("training_config")
    worker_classes = training_config["taskcluster"]["worker-classes"]
    worker_class = worker_classes.get(config.kind, worker_classes["default"])
    for job in jobs:
        # First, evaluate the `keyed-by` in the initial task specification from
        # the kind, if present. This should give us one of the keys from
        # `worker-configuration` in config.yml.
        task_worker_type = evaluate_keyed_by(
            job["worker-type"],
            job["description"],
            {"tasks-for": config.params["tasks_for"]},
        )

        # Now that we have one of the aliases, we need to resolve it to a
        # specific worker type, as some of those aliases have their own
        # `keyed-by` blocks, which may give different worker types depending
        # on what's in the training config.
        worker_alias_block = config.graph_config["local-worker-aliases"][task_worker_type].copy()
        job["worker-type"] = evaluate_keyed_by(
            worker_alias_block,
            task_worker_type,
            {"worker-class": worker_class},
        )

        yield job


@transforms.add
def inject_worker_env(config, jobs):
    for job in jobs:
        # This is called worker-type in jobs, but in reality it's an alias resolved in the graph config...
        worker_type = job["worker-type"]
        worker_config = config.graph_config["worker-configuration"].get(worker_type, {})

        worker_env = worker_config.get("env", {})
        if "GPUS" not in worker_env or "WORKSPACE" not in worker_env:
            # GPU tasks will not function correctly without these set; make this an error
            # before they even run.
            if "gpu" in worker_type:
                raise Exception(
                    "GPUS and/or WORKSPACE values missing from worker env, this is probably misconfiguration."
                )
            else:
                yield job
                continue

        job["worker"]["env"].update(worker_env)

        yield job


translations_taskgraph/util/__init__.py



translations_taskgraph/util/dataset_helpers.py

from pathlib import Path
from urllib.parse import urlparse
import hashlib


# We keep this relatively short because these datasets end up in task labels,
# which end up in task cache routes, which need to be <= 256 characters.
DATASET_NAME_MAX_LENGTH = 50


# Important! Keep in sync with `Dataset._escape` in pipeline/common/datasets.py.
def sanitize_dataset_name(dataset: str) -> str:
    # URLs can be too large when used as Taskcluster labels. Create a nice identifier for them.
    # See https://github.com/mozilla/translations/issues/527
    if dataset.startswith("https://") or dataset.startswith("http://"):
        url = urlparse(dataset)

        hostname = url.hostname
        if hostname == "storage.googleapis.com":
            hostname = "gcp"

        # Get the name of the file from theh path without the extension.
        file = Path(url.path).stem
        file = file.replace(".[LANG]", "").replace("[LANG]", "")

        # Compute a hash to avoid any name collisions.
        md5 = hashlib.md5()
        md5.update(dataset.encode("utf-8"))
        hash = md5.hexdigest()[:6]

        dataset = f"{hostname}_{file}_{hash}"
    # Even non-URL datasets can be too long, for example:
    # mtdata_ELRC-convention_against_torture_other_cruel_inhuman_or_degrading_treatment_or_punishment_united_nations-1-ell-eng
    # We need to truncate and hash any that are over a certain length
    elif len(dataset) > DATASET_NAME_MAX_LENGTH:
        md5 = hashlib.md5()
        md5.update(dataset.encode("utf-8"))
        hash = md5.hexdigest()[:6]

        truncated = dataset[:DATASET_NAME_MAX_LENGTH]
        dataset = f"{truncated}_{hash}"

    return (
        dataset.replace("://", "_")
        .replace("/", "_")
        .replace(".", "_")
        .replace(":", "_")
        .replace("[", "_")
        .replace("]", "_")
    )


translations_taskgraph/util/dict_helpers.py

def deep_get(dict_, field):
    container, subfield = dict_, field
    while "." in subfield:
        f, subfield = subfield.split(".", 1)
        if f not in container:
            return None

        container = container[f]

    return container.get(subfield)


translations_taskgraph/util/substitution.py

from typing import Any


class PartialSubstitutionDict(dict):
    """A dictionary that will return any missing keys as their formatable
    version. Useful when a string needs to be formatted multiple times
    in different places to get to its final form."""

    def __missing__(self, key):
        return "{" + key + "}"


def substitute(item: Any, **subs):
    if isinstance(item, list):
        for i in range(len(item)):
            item[i] = substitute(item[i], **subs)
    elif isinstance(item, dict):
        new_dict = {}
        for k, v in item.items():
            k = k.format_map(PartialSubstitutionDict(subs))
            new_dict[k] = substitute(v, **subs)
        item = new_dict
    elif isinstance(item, str):
        item = item.format_map(PartialSubstitutionDict(subs))
    else:
        item = item

    return item
