pipeline
.
├── alignments
│   ├── __pycache__
│   │   ├── align.cpython-310.pyc
│   │   └── tokenizer.cpython-310.pyc
│   ├── align.py
│   ├── generate-shortlist.sh
│   ├── prune_shortlist.py
│   ├── requirements
│   │   ├── alignments.in
│   │   └── alignments.txt
│   └── tokenizer.py
├── bicleaner
│   ├── __pycache__
│   │   └── download_pack.cpython-310.pyc
│   ├── bicleaner.sh
│   ├── download_pack.py
│   └── requirements
│       ├── bicleaner-ai.in
│       └── bicleaner-ai.txt
├── cefilter
│   ├── ce-filter.sh
│   └── score.sh
├── clean
│   ├── clean-corpus.sh
│   ├── clean-mono.sh
│   ├── fixes
│   │   ├── detok.sh
│   │   ├── mtdata_JW300.mt.sh
│   │   ├── mtdata_JW300.sh
│   │   ├── mtdata_OPUS_DOGC_v2.ca.sh
│   │   ├── mtdata_OPUS_DOGC_v2.es.sh
│   │   ├── mtdata_OPUS_DOGC_v2.sh
│   │   ├── mtdata_OPUS_ECB_v1.sh
│   │   ├── mtdata_OPUS_SETIMES_v2.sh
│   │   ├── mtdata_OPUS_UNPC_v1_0.en.sh
│   │   ├── mtdata_OPUS_UNPC_v1_0.fr.sh
│   │   ├── mtdata_neulab_tedtalksv1_train.ro.sh
│   │   └── mtdata_neulab_tedtalksv1_train.sh
│   ├── merge-corpus.py
│   ├── merge-mono.py
│   ├── opuscleaner
│   │   ├── __pycache__
│   │   │   └── generate_filters.cpython-310.pyc
│   │   ├── clean-corpus.sh
│   │   ├── configs
│   │   │   ├── default.filters.json
│   │   │   ├── en-ja
│   │   │   │   └── default.filters.json
│   │   │   ├── en-ko
│   │   │   │   └── default.filters.json
│   │   │   ├── en-zh
│   │   │   │   ├── WikiMatrix-v1.filters.json
│   │   │   │   └── default.filters.json
│   │   │   ├── ja-en
│   │   │   │   └── default.filters.json
│   │   │   ├── ko-en
│   │   │   │   └── default.filters.json
│   │   │   ├── opus_LinguaTools-WikiTitles-v2014.filters.json
│   │   │   ├── opus_NLLB-v1.filters.json
│   │   │   ├── opus_OpenSubtitles-v2018.filters.json
│   │   │   ├── opus_UNPC-v1.0.filters.json
│   │   │   ├── opus_XLEnt-v1.2.filters.json
│   │   │   ├── remove_frequent_patterns.txt
│   │   │   ├── ru-en
│   │   │   │   └── opus_ELRC-3075-wikipedia_health-v1.filters.json
│   │   │   └── zh-en
│   │   │       ├── WikiMatrix-v1.filters.json
│   │   │       └── default.filters.json
│   │   └── generate_filters.py
│   ├── requirements
│   │   ├── clean.in
│   │   ├── clean.txt
│   │   ├── merge.in
│   │   └── merge.txt
│   └── tools
│       ├── clean_mono.py
│       ├── clean_parallel.py
│       ├── deescape-special-chars.perl
│       ├── langid_fasttext.py
│       ├── normalize-punctuation.perl
│       └── remove-non-printing-char.perl
├── common
│   ├── __init__.py
│   ├── __pycache__
│   │   ├── __init__.cpython-310.pyc
│   │   ├── __init__.cpython-312.pyc
│   │   ├── arg_utils.cpython-310.pyc
│   │   ├── command_runner.cpython-310.pyc
│   │   ├── datasets.cpython-310.pyc
│   │   ├── datasets.cpython-312.pyc
│   │   ├── downloads.cpython-310.pyc
│   │   ├── downloads.cpython-312.pyc
│   │   ├── logging.cpython-310.pyc
│   │   ├── logging.cpython-312.pyc
│   │   ├── marian.cpython-310.pyc
│   │   ├── memory.cpython-310.pyc
│   │   └── memory.cpython-312.pyc
│   ├── arg_utils.py
│   ├── command_runner.py
│   ├── datasets.py
│   ├── downloads.py
│   ├── logging.py
│   ├── marian.py
│   └── memory.py
├── continuation
│   ├── corpus.py
│   ├── model.py
│   ├── requirements
│   │   ├── continuation.in
│   │   └── continuation.txt
│   └── vocab.py
├── data
│   ├── __pycache__
│   │   ├── cjk.cpython-310.pyc
│   │   ├── cjk.cpython-312.pyc
│   │   ├── dataset_importer.cpython-310.pyc
│   │   └── dataset_importer.cpython-312.pyc
│   ├── analyze.py
│   ├── cjk.py
│   ├── dataset_importer.py
│   ├── download-corpus.sh
│   ├── download-mono.py
│   ├── importers
│   │   ├── corpus
│   │   │   ├── flores.sh
│   │   │   ├── mtdata.sh
│   │   │   ├── opus.sh
│   │   │   ├── sacrebleu.sh
│   │   │   └── url.py
│   │   └── mono
│   │       ├── __pycache__
│   │       │   ├── hplt.cpython-310.pyc
│   │       │   └── hplt.cpython-312.pyc
│   │       └── hplt.py
│   └── requirements
│       ├── analyze.in
│       ├── analyze.txt
│       ├── data.in
│       └── data.txt
├── eval
│   ├── eval.py
│   └── requirements
│       ├── eval.in
│       └── eval.txt
├── quantize
│   ├── decoder.yml
│   ├── export.sh
│   ├── quantize.sh
│   └── requirements
│       ├── quantize.in
│       └── quantize.txt
├── setup
│   ├── compile-extract-lex.sh
│   ├── compile-fast-align.sh
│   ├── compile-marian.sh
│   ├── compile-preprocess.sh
│   ├── install-deps.sh
│   └── install-kenlm.sh
├── train
│   ├── configs
│   │   ├── model
│   │   │   ├── backward.yml
│   │   │   ├── student.base.yml
│   │   │   ├── student.tiny.yml
│   │   │   └── teacher.yml
│   │   ├── opustrainer
│   │   │   ├── backward.cjk.yml
│   │   │   ├── backward.yml
│   │   │   ├── student.cjk.yml
│   │   │   ├── student.yml
│   │   │   ├── teacher.one-stage.cjk.yml
│   │   │   ├── teacher.one-stage.yml
│   │   │   ├── teacher.two-stage.cjk.yml
│   │   │   └── teacher.two-stage.yml
│   │   └── training
│   │       ├── backward.train.yml
│   │       ├── student.finetune.yml
│   │       ├── student.train.yml
│   │       └── teacher.train.yml
│   ├── requirements
│   │   ├── train.in
│   │   └── train.txt
│   ├── spm-vocab.sh
│   └── train.py
└── translate
    ├── __pycache__
    │   ├── splitter.cpython-310.pyc
    │   └── translate_ctranslate2.cpython-310.pyc
    ├── collect.sh
    ├── decoder.yml
    ├── extract_best.py
    ├── merge-corpus.sh
    ├── requirements
    │   ├── extract_best.in
    │   ├── extract_best.txt
    │   ├── splitter.in
    │   ├── splitter.txt
    │   ├── translate-ctranslate2.in
    │   └── translate-ctranslate2.txt
    ├── splitter.py
    ├── translate.py
    └── translate_ctranslate2.py

47 directories, 156 files

alignments/align.py

#!/usr/bin/env python3
"""
Calculates alignments for a parallel corpus.

Some efficiency measures were implemented as it needs to process 500M sentences long corpus for the student model:
1. Tokenization with Moses with remapping the alignments back to whitespace based tokenization to reduce vocabulary size and improve accuracy
2. Using fast C++ Moses tokenizer
2. Parallelization with multiprocessing (tokenization and remapping)
3. Buffering on writing the output files to improve throughput


Example:
    BIN=bin SRC=ru TRG=en python pipeline/alignments/align.py \
        --corpus_src=fetches/corpus.ru.zst
        --corpus_trg=fetches/corpus.en.zst
        --output_path=artifacts/corpus.aln.zst
        --priors_input_path=fetches/corpus.priors
        --priors_output_path=artifacts/corpus.priors
"""

import argparse
import multiprocessing
import os
from pathlib import Path
import shutil
import subprocess
import sys
from contextlib import ExitStack
from enum import Enum
from glob import glob
from typing import Dict, Optional

import zstandard
from tqdm import tqdm

from pipeline.alignments.tokenizer import tokenize, TokenizerType
from pipeline.common.datasets import decompress
from pipeline.common.logging import get_logger

logger = get_logger("alignments")


class Tokenization(Enum):
    spaces = "spaces"
    moses = "moses"
    icu = "icu"


def run(
    corpus_src: str,
    corpus_trg: str,
    output_path: str,
    tokenization: Tokenization,
    chunk_lines: int,
    output_tokenized: bool,
    priors_input_path: Optional[str],
    priors_output_path: Optional[str],
):
    bin = os.environ["BIN"]
    src = os.environ["SRC"]
    trg = os.environ["TRG"]

    tmp_dir = os.path.join(os.path.dirname(output_path), "tmp")
    os.makedirs(tmp_dir, exist_ok=True)

    corpus_src = maybe_decompress(corpus_src)
    corpus_trg = maybe_decompress(corpus_trg)

    if tokenization == Tokenization.spaces:
        tokenized_src, tokenized_trg = corpus_src, corpus_trg
        output_aln = output_path
    else:
        ext = f".tok-{tokenization.value}"
        tokenized_src = (
            corpus_src[: corpus_src.rfind(".")] + ext + corpus_src[corpus_src.rfind(".") :]
        )
        tokenized_trg = (
            corpus_trg[: corpus_trg.rfind(".")] + ext + corpus_trg[corpus_trg.rfind(".") :]
        )
        output_aln = os.path.join(tmp_dir, "aln")

        if tokenization == Tokenization.moses:
            tokenizer = TokenizerType.fast_moses
        elif tokenization == Tokenization.icu:
            tokenizer = TokenizerType.icu
        else:
            raise ValueError(f"Unrecognized tokenization type {tokenization}")
        # C++ tokenizer can process 100k sentences per second on a single core,
        # so the chunks to parallelize things should be large enough to increase throughput
        tokenize(corpus_src, tokenized_src, src, sentences_per_chunk=500000, tokenizer=tokenizer)
        tokenize(corpus_trg, tokenized_trg, trg, sentences_per_chunk=500000, tokenizer=tokenizer)

    fwd_path, rev_path = align(
        corpus_src=tokenized_src,
        corpus_trg=tokenized_trg,
        priors_input_path=priors_input_path,
        tmp_dir=tmp_dir,
        chunk_lines=chunk_lines,
    )
    symmetrize(bin=bin, fwd_path=fwd_path, rev_path=rev_path, output_path=output_aln)

    if priors_output_path:
        write_priors(
            corpus_src=tokenized_src,
            corpus_trg=tokenized_trg,
            fwd_path=fwd_path,
            rev_path=rev_path,
            priors_output_path=priors_output_path,
        )

    if tokenization != Tokenization.spaces:
        if output_tokenized:
            logger.info("Saving tokenized corpus")
            # Copy tokenized corpus to output directory
            for file in tokenized_src, tokenized_trg:
                output_corpus = shutil.move(file, os.path.dirname(output_path))
                subprocess.check_call(["zstdmt", "-f", "--rm", output_corpus])
        else:
            # Remap alignments to whitespace based tokenization
            remapped_aln = os.path.join(tmp_dir, "aln.remapped")
            remap(corpus_src, corpus_trg, tokenized_src, tokenized_trg, output_aln, remapped_aln)
            output_aln = remapped_aln

    if output_path.endswith(".zst"):
        logger.info("Compressing final alignments")
        subprocess.check_call(["zstdmt", "--rm", output_aln])
        output_aln += ".zst"
    shutil.move(output_aln, output_path)
    shutil.rmtree(tmp_dir)


def maybe_decompress(file_path: str):
    if file_path.endswith(".zst"):
        return str(decompress(file_path, remove=True, logger=logger))
    return file_path


def align(
    corpus_src: str,
    corpus_trg: str,
    tmp_dir: str,
    chunk_lines: int,
    priors_input_path: Optional[str],
):
    import eflomal

    logger.info("Splitting corpus into parts")
    # align in chunks to prevent OOM
    # produces chunks of files, like "corpus.en.aa", "corpus.en.ab", "corpus.en.ac" etc.
    subprocess.check_call(["split", "--lines", str(chunk_lines), corpus_src, corpus_src + "."])
    subprocess.check_call(["split", "--lines", str(chunk_lines), corpus_trg, corpus_trg + "."])

    fwd_path = os.path.join(tmp_dir, "aln.fwd")
    rev_path = os.path.join(tmp_dir, "aln.rev")

    for src_part in sorted(glob(f"{corpus_src}.*")):
        suffix = src_part.split(".")[-1]
        logger.info(f"Processing part {suffix}")

        with ExitStack() as stack:
            if priors_input_path:
                logger.info(f"Using provided priors: {priors_input_path}")
                priors_input = stack.enter_context(open(priors_input_path, "r", encoding="utf-8"))
            else:
                priors_input = None

            src_input = stack.enter_context(open(f"{corpus_src}.{suffix}", "r", encoding="utf-8"))
            trg_input = stack.enter_context(open(f"{corpus_trg}.{suffix}", "r", encoding="utf-8"))

            logger.info("Calculating alignments...")
            # We use eflomal aligner.
            # It is less memory intensive than fast_align.
            # fast_align failed with OOM in a large white-space tokenized corpus
            aligner = eflomal.Aligner()
            aligner.align(
                src_input,
                trg_input,
                links_filename_fwd=f"{fwd_path}.{suffix}",
                links_filename_rev=f"{rev_path}.{suffix}",
                priors_input=priors_input,
                quiet=False,
                use_gdb=False,
            )

    # Merge alignments parts into one file
    with open(fwd_path, "w") as fwd_out:
        fwd_parts = sorted(glob(f"{fwd_path}.*"))
        logger.info(f"Merging alignments: {fwd_parts}")
        subprocess.check_call(["cat"] + fwd_parts, stdout=fwd_out)
    with open(rev_path, "w") as rev_out:
        rev_parts = sorted(glob(f"{rev_path}.*"))
        logger.info(f"Merging alignments: {rev_parts}")
        subprocess.check_call(["cat"] + rev_parts, stdout=rev_out)

    return fwd_path, rev_path


def symmetrize(bin: str, fwd_path: str, rev_path: str, output_path: str):
    """
    Symmetrize the forward and reverse alignments of the corpus.

    Alignments are generated in two directions, source to target, and target to source.
    This function symmetrizes them so that both directions share the same alignment information.
    It uses `atools` binary from `fast_align`
    """
    logger.info("Symmetrizing alignments...")
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    # Wrap the file with a compressor stream if it needs to be compressed
    with ExitStack() as stack:
        with (
            zstandard.ZstdCompressor().stream_writer(stack.enter_context(open(output_path, "wb")))
            if output_path.endswith(".zst")
            else stack.enter_context(open(output_path, "w", encoding="utf-8"))
        ) as stream:
            with subprocess.Popen(
                [
                    os.path.join(bin, "atools"),
                    "-i",
                    fwd_path,
                    "-j",
                    rev_path,
                    "-c",
                    "grow-diag-final-and",
                ],
                stdout=subprocess.PIPE,
                text=True,
                bufsize=1,
                encoding="utf-8",
            ) as proc:
                for line in proc.stdout:
                    stream.write(line.encode("utf-8") if output_path.endswith(".zst") else line)

                proc.wait()
                # Check for any errors in the subprocess execution
                if proc.returncode != 0:
                    logger.error(f"atools exit code: {proc.returncode}")
                    raise subprocess.CalledProcessError(proc.returncode, proc.args)


def write_priors(
    corpus_src: str,
    corpus_trg: str,
    fwd_path: str,
    rev_path: str,
    priors_output_path: str,
):
    import eflomal

    logger.info("Calculating priors...")
    with ExitStack() as stack:
        src_input = stack.enter_context(open(corpus_src, "r", encoding="utf-8"))
        trg_input = stack.enter_context(open(corpus_trg, "r", encoding="utf-8"))
        fwd_f = stack.enter_context(open(fwd_path, "r", encoding="utf-8"))
        rev_f = stack.enter_context(open(rev_path, "r", encoding="utf-8"))
        priors_tuple = eflomal.calculate_priors(src_input, trg_input, fwd_f, rev_f)
        logger.info(f"Writing priors to {priors_output_path}...")
        priors_output = stack.enter_context(open(priors_output_path, "w", encoding="utf-8"))
        eflomal.write_priors(priors_output, *priors_tuple)


def remap(
    src_path: str,
    trg_path: str,
    tok_src_path: str,
    tok_trg_path: str,
    aln_path: str,
    output_aln_path: str,
) -> None:
    """
    Remaps alignments that were calculated for tokenized corpus to whitespace-tokenized ones.
    :param src_path: path to whitespace-tokenized sentences in source language
    :param trg_path: path to whitespace-tokenized sentences in target language
    :param tok_src_path: path to tokenized sentences in source language
    :param tok_trg_path: path to tokenized sentences in target language
    :param aln_path: path to the alignments calculated for tokenized corpus
    :param output_aln_path: path to output alignments file remapped to whitespace-tokenized corpus
    """
    logger.info("Remapping alignments to whitespace tokenization")

    with ExitStack() as stack:
        pool = stack.enter_context(multiprocessing.Pool(processes=multiprocessing.cpu_count()))
        # Buffering helps to minimize IO operations which speeds thing up significantly
        output = stack.enter_context(open(output_aln_path, "w", buffering=500000))

        lines = zip(
            stack.enter_context(open(src_path)),
            stack.enter_context(open(trg_path)),
            stack.enter_context(open(tok_src_path)),
            stack.enter_context(open(tok_trg_path)),
            stack.enter_context(open(aln_path)),
        )

        # send lines to worker processes in chunks
        for aln in tqdm(pool.imap(remap_line, lines, chunksize=10000), mininterval=10):
            output.write(aln)


def remap_line(params):
    """
    Remaps alignments for a single line in a corpus
    """
    src, trg, tok_src, tok_trg, aln = params
    src_map = map_indices(tok_src, src)
    trg_map = map_indices(tok_trg, trg)

    remapped_aln = []
    for pair in aln.split():
        src_idx, trg_idx = map(int, pair.split("-"))
        new_pair = (src_map[src_idx], trg_map[trg_idx])
        if new_pair not in remapped_aln:
            remapped_aln.append(new_pair)

    return " ".join([f"{idx1}-{idx2}" for idx1, idx2 in remapped_aln]) + "\n"


def map_indices(tok_sentence: str, orig_sentence: str) -> Dict[int, int]:
    """
    Map token indices from tokenized sentence to original sentence.
    :param tok_sentence: tokenized sentence
    :param orig_sentence: original sentence
    :return: Dictionary of indices that maps tokenized words to original words
    """
    tok_words = tok_sentence.split()
    orig_words = orig_sentence.split()
    tok_to_orig_indices = {}
    orig_idx = 0
    tok_idx = 0

    # For 'Hello, world!'
    # Map ['Hello', ',', 'world', '!'] [0, 1, 2, 3]
    # To ['Hello,', 'world!'] [0, 1]
    # tok -> orig: {0: 0, 1: 0, 2: 1, 3: 1}

    while orig_idx < len(orig_words):
        orig_word = orig_words[orig_idx]
        word = ""
        while tok_idx < len(tok_words) and word != orig_word:
            word += tok_words[tok_idx]
            tok_to_orig_indices[tok_idx] = orig_idx
            tok_idx += 1

        orig_idx += 1

    return tok_to_orig_indices


def main() -> None:
    logger.info(f"Running with arguments: {sys.argv}")
    parser = argparse.ArgumentParser(
        description=__doc__,
        # Preserves whitespace in the help text.
        formatter_class=argparse.RawTextHelpFormatter,
    )

    parser.add_argument("--type", metavar="TYPE", type=str, help="Dataset type: mono or corpus")
    parser.add_argument(
        "--corpus_src",
        metavar="CORPUS_SRC",
        type=str,
        help="Full path to the source sentences in a parallel dataset. Supports decompression using zstd. "
        "For example `fetches/corpus.ru` or `fetches/corpus.ru.zst`",
    )
    parser.add_argument(
        "--corpus_trg",
        metavar="CORPUS_TRG",
        type=str,
        help="Full path to the target sentences in a parallel dataset. Supports decompression using zstd. "
        "For example `fetches/corpus.en` or `fetches/corpus.en.zst`",
    )
    parser.add_argument(
        "--output_path",
        metavar="OUTPUT_PATH",
        type=str,
        help="A full path to the output alignments file. It will be compressed if the path ends with .zst. "
        "For example artifacts/corpus.aln or artifacts/corpus.aln.zst",
    )
    parser.add_argument(
        "--priors_input_path",
        metavar="PRIORS_INPUT_PATH",
        type=str,
        default=None,
        help="A full path to the model priors calculated in advance. This can speed up generation.",
    )
    parser.add_argument(
        "--priors_output_path",
        metavar="PRIORS_OUTPUT_PATH",
        type=str,
        default=None,
        help="Calculate and save the model priors to the specified file path. "
        "The file will be compressed if it ends with .zst",
    )
    parser.add_argument(
        "--tokenization",
        metavar="TOKENIZATION",
        type=Tokenization,
        choices=list(Tokenization),
        default=Tokenization.spaces,
        help="Use the specified tokenization method. Default is `spaces` which means no tokenization will be applied. "
        "It remaps the alignments back to whitespace tokenized ones if another tokenization method is used.",
    )
    parser.add_argument(
        "--output_tokenized",
        metavar="OUTPUT_TOKENIZED",
        type=bool,
        default=False,
        action=argparse.BooleanOptionalAction,
        help="Output tokenized corpus and do not remap alignments to whitespace based tokenization",
    )
    parser.add_argument(
        "--chunk_lines",
        metavar="CHUNK_LINES",
        type=int,
        # use env to override from tests
        default=int(os.getenv("ALN_CHUNK_LINES", "50000000")),
        help="Split corpus to chunks of N lines to calculate alignments on them separately. "
        "This helps with reducing the memory footprint. 100M by default.",
    )
    args = parser.parse_args()
    logger.info("Starting generating alignments.")

    priors_input_path = args.priors_input_path
    if priors_input_path and not Path(priors_input_path).exists():
        # This can happen on training continuation.
        print("The priors were not found, they will be regenerated.")
        priors_input_path = None

    run(
        corpus_src=args.corpus_src,
        corpus_trg=args.corpus_trg,
        output_path=args.output_path,
        tokenization=args.tokenization,
        chunk_lines=args.chunk_lines,
        output_tokenized=args.output_tokenized,
        priors_input_path=priors_input_path,
        priors_output_path=args.priors_output_path,
    )
    logger.info("Finished generating alignments.")


if __name__ == "__main__":
    main()


alignments/generate-shortlist.sh

#!/bin/bash
##
# Generates a lexical shortlist for a corpus.
#
#
# It also generate SentencePiece tokenized alignments that are required for extract_lex
#

set -x
set -euo pipefail

echo "###### Generating alignments and shortlist"
[[ -z "${MARIAN}" ]] && echo "MARIAN is empty"
[[ -z "${BIN}" ]] && echo "BIN is empty"
[[ -z "${SRC}" ]] && echo "SRC is empty"
[[ -z "${TRG}" ]] && echo "TRG is empty"

corpus_prefix=$1
vocab_path=$2
output_dir=$3
threads=$4

if [ "$threads" = "auto" ]; then
  threads=$(nproc)
fi

cd "$(dirname "${0}")"

mkdir -p "${output_dir}"
dir="${output_dir}/tmp_shortlist"
mkdir -p "${dir}"

corpus_src="${corpus_prefix}.${SRC}.zst"
corpus_trg="${corpus_prefix}.${TRG}.zst"


echo "### Subword segmentation with SentencePiece"
zstdmt -dc "${corpus_src}" |
  parallel --no-notice --pipe -k -j "${threads}" --block 50M "${MARIAN}/spm_encode" --model "${vocab_path}" \
   >"${dir}/corpus.spm.${SRC}"

zstdmt -dc "${corpus_trg}" |
  parallel --no-notice --pipe -k -j "${threads}" --block 50M "${MARIAN}/spm_encode" --model "${vocab_path}" \
   >"${dir}/corpus.spm.${TRG}"

python3 align.py \
  --corpus_src="${dir}/corpus.spm.${SRC}" \
  --corpus_trg="${dir}/corpus.spm.${TRG}" \
  --output_path="${output_dir}/corpus.aln"

echo "### Creating shortlist"
"${BIN}/extract_lex" \
  "${dir}/corpus.spm.${TRG}" \
  "${dir}/corpus.spm.${SRC}" \
  "${output_dir}/corpus.aln" \
  "${dir}/lex.s2t" \
  "${dir}/lex.t2s"

if [ -f "${dir}/lex.s2t" ]; then
  zstdmt "${dir}/lex.s2t"
fi

rm "${dir}/corpus.spm.${TRG}"
rm "${dir}/corpus.spm.${SRC}"
rm "${output_dir}/corpus.aln"

echo "### Shortlist pruning"
"${MARIAN}/spm_export_vocab" --model="${vocab_path}" --output="${dir}/vocab.txt"
zstdmt -dc "${dir}/lex.s2t.zst" |
  grep -v NULL |
  python3 "prune_shortlist.py" 100 "${dir}/vocab.txt" |
  zstdmt >"${output_dir}/lex.s2t.pruned.zst"

echo "### Deleting tmp dir"
rm -rf "${dir}"

echo "###### Done: Generating alignments and shortlist"


alignments/prune_shortlist.py

#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import sys

MAX = int(sys.argv[1])
TOP = sys.argv[2]

tops = []

with open(TOP, "r") as f:
    for line in f:
        tops.append(line.strip().split()[0])

tops = tops[: MAX + 2]

vocabTrg = []
vocabSrc = []
pairs = {}

for line in sys.stdin:
    try:
        trg, src, prob = line.strip().split()
    # some lines include empty items for zh-en, 63 from ~400k
    except ValueError:
        continue

    if trg == "NULL" or src == "NULL":
        continue

    # sys.stderr.write("{} {} {} \n".format(trg, src, prob))

    vocabTrg.append(trg)
    vocabSrc.append(src)

    prob = float(prob)
    if src in pairs:
        pairs[src][trg] = prob
    else:
        pairs[src] = {trg: prob}

vocabTrg = set(vocabTrg)
vocabSrc = set(vocabSrc)

for src in vocabSrc:
    d = pairs[src]
    topSrc = list(sorted(d, key=d.get, reverse=True)[:MAX])
    for trg in topSrc + tops:
        if trg in d:
            prob = d[trg]
            print("{} {} {:.8f}".format(trg, src, prob))


alignments/requirements/alignments.in

eflomal==1.0.0b1
opus-fast-mosestokenizer==0.0.8.5
tqdm
requests==2.31.0
zstandard
PyICU==2.8.1


alignments/tokenizer.py

#!/usr/bin/env python3
"""
Tokenizes a text file with line separated sentences using Moses tokenizer.

Example:
  python pipeline/alignments/tokenizer.py --input_path=data/datasets/news.2023.en.shuffled.deduped \
    --output_path=data/datasets/news.2023.en.shuffled.deduped.tok-icu --lang=en --chunk_size=500000 --tokenizer=icu

Using C++ opus-fast-mosestokenizer sometimes requires specifying LD_LIBRARY_PATH before starting the Python process
see https://github.com/Helsinki-NLP/opus-fast-mosestokenizer/issues/6
export LD_LIBRARY_PATH=.../<you-python-env>/lib/python3.10/site-packages/mosestokenizer/lib

Using ICU tokenizer requires installing it with `apt-get install python3-icu`,
see more installation instructions here: https://pypi.org/project/PyICU/

Whitespaces are ignored by Moses based tokenizers and preserved and replaced with a special token "▁" by ICU tokenizer
which allows lossless reconstruction of the original text on detokenization.

"""
import argparse
import multiprocessing
from abc import ABC, abstractmethod
from enum import Enum
from typing import List

from tqdm import tqdm

from pipeline.common.logging import get_logger

logger = get_logger("tokenizer")


class TokenizerType(Enum):
    fast_moses = "fast_moses"
    sacre_moses = "sacre_moses"
    icu = "icu"


class Tokenizer(ABC):
    def __init__(self, lang: str):
        self.lang = lang

    @abstractmethod
    def tokenize(self, text: str) -> List[str]:
        pass

    @abstractmethod
    def detokenize(self, tokens: List[str]) -> str:
        pass


class FastMosesTokenizer(Tokenizer):
    """
    Uses Moses tokenizer https://github.com/Helsinki-NLP/opus-fast-mosestokenizer
    """

    def __init__(self, lang):
        super().__init__(lang)
        from mosestokenizer import MosesTokenizer

        try:
            self.tokenizer = MosesTokenizer(lang)
        except RuntimeError as err:
            msg = str(err)
            if "No known abbreviations for language" in msg:
                # Fall-back to English if the language is not found
                self.tokenizer = MosesTokenizer("en")
            else:
                raise err

    def tokenize(self, text: str) -> List[str]:
        return self.tokenizer.tokenize(text)

    def detokenize(self, tokens: List[str]) -> str:
        return self.tokenizer.detokenize(tokens)


class SacreMosesTokenizer(Tokenizer):
    """
    Uses Moses tokenizer https://github.com/hplt-project/sacremoses
    """

    def __init__(self, lang):
        super().__init__(lang)
        import sacremoses

        self.tokenizer = sacremoses.MosesTokenizer(lang)
        self.detokenizer = sacremoses.MosesDetokenizer(lang)

    def tokenize(self, text: str) -> List[str]:
        return self.tokenizer.tokenize(text)

    def detokenize(self, tokens: List[str]) -> str:
        return self.detokenizer.detokenize(tokens)


class IcuTokenizer(Tokenizer):
    """
    Uses ICU based word segmenter https://pypi.org/project/PyICU/
    Preserves whitespaces as tokens by replacing them with a special character "▁".
    Allows lossless reconstruction of the original text on detokenization.
    """

    # Same character is used by SentencePiece
    SPACE_TOKEN = "▁"

    def tokenize(self, text: str) -> List[str]:
        from icu import BreakIterator, Locale

        bi = BreakIterator.createWordInstance(Locale(self.lang))
        bi.setText(text)

        tokens = []
        start = bi.first()
        for end in bi:
            token = text[start:end]
            if (
                token and token != "\n"
            ):  # exclude empty tokens, but leave whitespaces and replace them with a special token
                tokens.append(token.replace(" ", self.SPACE_TOKEN))
            start = end
        return tokens

    def detokenize(self, tokens: List[str]) -> str:
        return "".join(tokens).replace(self.SPACE_TOKEN, " ")


def _read_file_in_chunks(file_path, chunk_size):
    with open(file_path, "r", encoding="utf-8") as file:
        while True:
            lines = file.readlines(chunk_size)
            if not lines:
                break
            yield [line.rstrip() for line in lines]


def _tokenize_lines(params) -> List[str]:
    lines, lang, tok_type = params

    if tok_type == TokenizerType.fast_moses:
        tokenizer = FastMosesTokenizer(lang)
    elif tok_type == TokenizerType.sacre_moses:
        tokenizer = SacreMosesTokenizer(lang)
    elif tok_type == TokenizerType.icu:
        tokenizer = IcuTokenizer(lang)
    else:
        raise ValueError(f"Unknown tokenizer type: {tok_type}")

    tokenized = []
    for line in lines:
        tokens = tokenizer.tokenize(line)
        tokenized.append(" ".join(tokens))
    return tokenized


def tokenize(
    input_path: str,
    output_path: str,
    lang: str,
    tokenizer: TokenizerType,
    sentences_per_chunk: int = 100000,
) -> None:
    logger.info(f"Tokenizing {input_path} with Moses tokenizer")

    with multiprocessing.Pool(processes=multiprocessing.cpu_count()) as pool:
        with open(output_path, "w") as output_file:
            chunks = _read_file_in_chunks(input_path, chunk_size=sentences_per_chunk)

            pbar = tqdm(mininterval=10)
            # ~100K sentences per second on a single core
            for tokenized_chunk in pool.imap(
                _tokenize_lines,
                ((ch, lang, tokenizer) for ch in chunks),
            ):
                output_file.write("\n".join(tokenized_chunk) + "\n")
                pbar.update(len(tokenized_chunk))


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description=__doc__,
        # Preserves whitespace in the help text.
        formatter_class=argparse.RawTextHelpFormatter,
    )
    parser.add_argument(
        "--output_path",
        metavar="OUTPUT_PATH",
        type=str,
        help="Output file",
    )
    parser.add_argument(
        "--input_path",
        metavar="INPUT_PATH",
        type=str,
        default=None,
        help="Input file",
    )
    parser.add_argument(
        "--lang",
        metavar="LANG",
        type=str,
        default=None,
        help="Language",
    )
    parser.add_argument(
        "--chunk_size",
        metavar="CHUNK_SIZE",
        type=int,
        default=None,
        help="Number of lines to process per chunk",
    )
    parser.add_argument(
        "--tokenizer",
        metavar="TOKENIZER",
        type=TokenizerType,
        choices=TokenizerType,
        default=TokenizerType.icu,
        help="Tokenization method",
    )
    args = parser.parse_args()
    tokenize(
        input_path=args.input_path,
        output_path=args.output_path,
        lang=args.lang,
        sentences_per_chunk=args.chunk_size,
        tokenizer=args.tokenizer,
    )


bicleaner/bicleaner.sh

#!/bin/bash
##
# Cleans corpus using bicleaner-ai
#
# See:
#   docs/bicleaner.md

set -x
set -euo pipefail

echo "###### Bicleaner filtering"

test -v SRC
test -v TRG
test -v CUDA_DIR
test -v CUDNN_DIR

# cuda and cudnn libs
export LD_LIBRARY_PATH=${CUDA_DIR}/lib64:${CUDNN_DIR}:${LD_LIBRARY_PATH:+LD_LIBRARY_PATH:}

corpus_prefix=$1
output_prefix=$2
bicleaner_threshold=$3
threads=$4
pack_dir=$5

if [ "$threads" = "auto" ]; then
  threads=$(nproc)
fi

output_dir=$(dirname "${output_prefix}")
mkdir -p "${output_dir}"

if [ "${bicleaner_threshold}" == "0" ] || [ "${bicleaner_threshold}" == "0.0" ]; then
  echo "Threshold is 0, skipping filtering"
  cp "${corpus_prefix}.${SRC}.zst" "${output_prefix}.${SRC}.zst"
  cp "${corpus_prefix}.${TRG}.zst" "${output_prefix}.${TRG}.zst"
else

  export scol=1
  export tcol=2
  # Get model src-trg from metadata.yaml
  model_source_lang=$(grep "source_lang" ${pack_dir}/*.yaml | awk '{print $2}')
  model_target_lang=$(grep "target_lang" ${pack_dir}/*.yaml | awk '{print $2}')
  # for example if SRC-TRG = en-ru
  # the model can be: en-ru, ru-en, en-xx
  if [ ${model_source_lang} == ${TRG} ] || [ ${model_target_lang} == ${SRC} ]; then
    # swap columns
    export scol=2
    export tcol=1
  fi

  #Export cuda visible devices if empty or not set
  if [ -z "${CUDA_VISIBLE_DEVICES:-}" ]; then
    export CUDA_VISIBLE_DEVICES=$(nvidia-smi --query-gpu=index --format=csv,noheader);
  fi

  echo "### Classifying"
  if [ ${#CUDA_VISIBLE_DEVICES} -gt 1 ]; then # Use gnu-parallel'd bicleaner-ai if we have more than 1 GPU
       #Convert CUDA_VISIBLE_DEVICES to an array
       export CUDA_VISIBLE_ARRAY=(${CUDA_VISIBLE_DEVICES//,/ })
       #Turn on tensorflow logging in bicleaner-ai
       export TF_CPP_MIN_LOG_LEVEL=0
       #This function expects a bicleaner yaml and a 1-based index into the CUDA_VISIBLE_ARRAY
       #Example: /mnt/nanna0/nbogoych/data/data/fr-en/fr-en-prod/biclean/pack/metadata.yaml index_in_CUDA_VISIBLE_ARRAY+1
       biclean() {
               export CUDA_VISIBLE_ARRAY=(${CUDA_VISIBLE_DEVICES//,/ })
               export CUDA_VISIBLE_DEVICES=${CUDA_VISIBLE_ARRAY[$(($2-1))]}
               # The GPU devices have failed to be found, and bicleaner AI falls back
               # to operate on the CPU very slowly. To guard against this wasting expensive
               # GPU time, always check that it can find GPUs.
               python3 -c "import tensorflow; exit(0) if tensorflow.config.list_physical_devices('GPU') else exit(9001)"
               bicleaner-ai-classify --disable_hardrules --scol ${scol} --tcol ${tcol} - - $1
       }
       export -f biclean
       # {%} is a 1-indexed job slot number from GNU parallel.  We use that as the 1-indexed offset in CUDA_VISIBLE_ARRAY
       paste <(zstdmt -dc "${corpus_prefix}.${SRC}.zst") <(zstdmt -dc "${corpus_prefix}.${TRG}.zst") |
       parallel -j ${#CUDA_VISIBLE_ARRAY[@]} --pipe -k --block 10M biclean "${pack_dir}"/*.yaml {%} |
       zstdmt >"${output_prefix}.scored.zst"
  else
   export BICLEANER_AI_THREADS=${threads}
   paste <(zstdmt -dc "${corpus_prefix}.${SRC}.zst") <(zstdmt -dc "${corpus_prefix}.${TRG}.zst") |
     bicleaner-ai-classify --disable_hardrules --scol ${scol} --tcol ${tcol} "${threads}"  - - "${pack_dir}"/*.yaml |
     zstdmt >"${output_prefix}.scored.zst"
  fi

  echo "### Filtering"
  zstdmt -dc "${output_prefix}.scored.zst" |
    awk -v threshold=${bicleaner_threshold} -F"\t" '{if ($3>threshold) {print $0}}' |
    zstdmt >"${output_prefix}.best.zst"

  zstdmt -dc "${output_prefix}.scored.zst" |
    awk -v threshold=${bicleaner_threshold} -F"\t" '{if ($3<=threshold) {print $0}}' |
    zstdmt >"${output_prefix}.filtered.zst"

  echo "Lines before filtering: $(zstdmt -dc "${output_prefix}.scored.zst" | wc -l)"
  echo "Lines after filtering: $(zstdmt -dc "${output_prefix}.best.zst" | wc -l)"

  echo "### Writing output corpus"
  zstdmt -dc "${output_prefix}.best.zst" |
    tee >(cut -f1 | zstdmt >"${output_prefix}.${SRC}.zst") |
    cut -f2 | zstdmt >"${output_prefix}.${TRG}.zst"

  # do not delete intermediate files to inspect them and tune the threshold
fi

echo "###### Done: Bicleaner filtering"


bicleaner/download_pack.py

#!/usr/bin/env python3
"""
Downloads bicleaner-ai model for a lanuage pair.
Fallbacks to the multilingual model if the lanuage pair is not supported.

Example:
    python download_pack.py \
        --src=en \
        --trg=ru \
        artifacts/bicleaner-model-en-ru.zst
"""

import argparse
import os
import shutil
import subprocess
import tarfile
import tempfile
from typing import Optional

from pipeline.common.downloads import compress_file
from pipeline.common.logging import get_logger

logger = get_logger(__file__)


# bicleaner-ai-download downloads the latest models from Hugging Face / Github
# If a new model is released and you want to invalidate Taskcluster caches,
# change this file since it is a part of the cache digest
# The last model was added to https://huggingface.co/bitextor on Mar 11, 2024
def _run_download(src: str, trg: str, dir: str) -> subprocess.CompletedProcess:
    # use large multilingual models
    model_type = "full-large" if trg == "xx" else "full"
    return subprocess.run(
        ["bicleaner-ai-download", src, trg, model_type, dir], capture_output=True, check=False
    )


def _compress_dir(dir_path: str) -> str:
    logger.info(f"Compressing {dir_path}")

    tarball_path = f"{dir_path}.tar"
    with tarfile.open(tarball_path, "w") as tar:
        tar.add(dir_path, arcname=os.path.basename(dir_path))

    compressed_path = str(compress_file(tarball_path))

    return compressed_path


def check_result(result: subprocess.CompletedProcess):
    """Checks the return code, and outputs the stdout and stderr if it fails."""
    if result.returncode != 0:
        print(result.stdout)
        print(result.stderr)
        result.check_returncode()


def download(src: str, trg: str, output_path: str) -> None:
    tmp_dir = os.path.join(tempfile.gettempdir(), f"bicleaner-ai-{src}-{trg}")

    if os.path.exists(tmp_dir):
        # A previous download attempt failed, remove the temporary files.
        shutil.rmtree(tmp_dir)

    os.mkdir(tmp_dir)

    # Attempt to download a model.
    # 1: src-trg
    # 2: trg-src
    # 3: multilingual model
    logger.info(f"Attempt 1 of 3: Downloading a model for {src}-{trg}")
    result = _run_download(src, trg, tmp_dir)

    meta_path = os.path.join(tmp_dir, "metadata.yaml")
    if os.path.exists(meta_path):
        check_result(result)
        logger.info(f"The model for {src}-{trg} is downloaded")
    else:
        src, trg = trg, src
        logger.info(f"Attempt 2 of 3. Downloading a model for {src}-{trg}")
        result = _run_download(src, trg, tmp_dir)

        if os.path.exists(meta_path):
            check_result(result)
            print(f"The model for {src}-{trg} is downloaded")
        else:
            logger.info("Attempt 3 of 3. Downloading the multilingual model en-xx")
            src = "en"
            trg = "xx"
            result = _run_download(src, trg, tmp_dir)

            if not os.path.exists(meta_path):
                check_result(result)
                raise Exception("Could not download the multilingual model")

            print(f"The model for {src}-{trg} is downloaded")

    pack_path = tmp_dir
    logger.info("Compress the downloaded pack.")
    pack_path = _compress_dir(pack_path)

    # Move to the expected path
    logger.info(f"Moving {pack_path} to {output_path}")
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    shutil.move(pack_path, output_path)
    logger.info("Done")


def main(args: Optional[list[str]] = None) -> None:
    parser = argparse.ArgumentParser(
        description=__doc__,
        formatter_class=argparse.RawTextHelpFormatter,  # Preserves whitespace in the help text.
    )
    parser.add_argument("--src", type=str, help="Source language code")
    parser.add_argument("--trg", type=str, help="Target language code")
    parser.add_argument(
        "output_path",
        type=str,
        help="Full output file or directory path for example artifacts/en-pt.zst",
    )

    parsed_args = parser.parse_args(args)

    download(
        src=parsed_args.src,
        trg=parsed_args.trg,
        output_path=parsed_args.output_path,
    )


if __name__ == "__main__":
    main()


bicleaner/requirements/bicleaner-ai.in

# Note: this package is installed manually before the rest of the requirements
# using the toolchain taskcluster/kinds/toolchain/kind.yml
cython_hunspell==2.0.3
bicleaner-ai==3.0.1


cefilter/ce-filter.sh

#!/bin/bash
##
# Filters student parallel corpus with scores produced by a reversed NMT model.
#

set -x
set -euo pipefail

echo "###### Cross entropy filtering"
test -v SRC
test -v TRG

corpus_prefix=$1
output_prefix=$2
scores=$3

cd "$(dirname "${0}")"

# Part of the data to be removed (0.05 is 5%)
remove=0.05
output_dir=$(dirname "${output_prefix}")
tmp="${output_dir}/tmp"
mkdir -p "${tmp}"

echo "### Sorting scores"
if [ ! -s "${tmp}/sorted.zst" ]; then
  buffer_size="$(echo "$(grep MemTotal /proc/meminfo | awk '{print $2}')"*0.9 | bc | cut -f1 -d.)"
  paste "${scores}" <(zstdmt -dc "${corpus_prefix}.${SRC}.zst") <(zstdmt -dc "${corpus_prefix}.${TRG}.zst") |
  LC_ALL=C sort -n -k1,1 -S "${buffer_size}K" -T "${tmp}" |
  zstdmt >"${tmp}/sorted.zst"
fi

echo "### Cutting the best scored corpus"
if [ ! -s "${tmp}/best.zst" ]; then
  lines=$(zstdmt -dc "${tmp}/sorted.zst" | wc -l)
  startline=$(echo ${lines}*${remove} | bc | cut -f1 -d.)
  zstdmt -dc "${tmp}/sorted.zst" | tail -n +${startline} | cut -f2,3 | zstdmt >"${tmp}/best.zst"
fi

echo "### Writing output corpus"
zstdmt -dc "${tmp}/best.zst" |
  tee >(cut -f1 | zstdmt >"${output_prefix}.${SRC}.zst") |
  cut -f2 | zstdmt >"${output_prefix}.${TRG}.zst"

echo "### Deleting tmp dir"
rm -rf "${tmp}"

echo "###### Done: Cross entropy filtering"


cefilter/score.sh

#!/bin/bash
##
# Scores a corpus with a reversed NMT model.
#


set -x
set -euo pipefail

echo "###### Scoring"
test -v MARIAN
test -v GPUS
test -v SRC
test -v TRG
test -v WORKSPACE

model=$1
vocab=$2
corpus_prefix=$3
output=$4

zstdmt --rm -d "${corpus_prefix}.${SRC}.zst"
zstdmt --rm -d "${corpus_prefix}.${TRG}.zst"

dir=$(dirname "${output}")
mkdir -p "${dir}"

"${MARIAN}/marian-scorer" \
  --model "${model}" \
  --vocabs "${vocab}" "${vocab}" \
  --train-sets "${corpus_prefix}.${TRG}" "${corpus_prefix}.${SRC}" \
  --mini-batch 32 \
  --mini-batch-words 1500 \
  --maxi-batch 1000 \
  --max-length 250 \
  --max-length-crop \
  --normalize \
  --devices ${GPUS} \
  --workspace "${WORKSPACE}" \
  --log "${dir}/scores.txt.log" \
  >"${output}"

echo "###### Done: Scoring"


clean/clean-corpus.sh

#!/bin/bash
##
# Basic cleaning of parallel corpora.
#

set -x
set -euo pipefail

echo "###### Cleaning corpus"


test -v SRC
test -v TRG

input_prefix=$1
output_prefix=$2
threads=$3
dataset=$4

if [ "$threads" = "auto" ]; then
  threads=$(nproc)
fi
cd "$(dirname "${0}")"
export PYTHONPATH="tools"

dir="$(dirname "${output_prefix}")"
mkdir -p "${dir}"

echo "### Cleaning ${input_prefix}"

######################################################################
echo "### Basic preprocessing"
for lng in "${SRC}" "${TRG}"; do
  test -s "${output_prefix}.${lng}.nrm.zst" ||
    zstdmt -dc "${input_prefix}.${lng}.zst" |
    parallel --no-notice --pipe -k -j "${threads}" --block 50M \
      "perl tools/deescape-special-chars.perl | perl tools/remove-non-printing-char.perl" |
    zstdmt >"${output_prefix}.${lng}.nrm.zst"
done

#####################################################################
echo "### Apply monolingual fixes"
for lng in $SRC $TRG; do
    if [[ ! -x fixes/${dataset}.${lng}.sh ]]; then
      test -s "${output_prefix}.${lng}.monofix.zst" ||
        cp "${output_prefix}.${lng}.nrm.zst" "${output_prefix}.${lng}.monofix.zst"
    else
        test -s "${output_prefix}.${lng}.monofix.zst" ||
          zstdmt -dc "${output_prefix}.${lng}.nrm.zst" \
              | fixes/"${dataset}"."${lng}".sh \
              | zstdmt >"${output_prefix}.${lng}.monofix.zst"
    fi
done

######################################################################
echo "### Apply bilingual fixes"
if [[ -x fixes/${dataset}.sh ]]; then
    FIX="fixes/${dataset}.sh ${SRC} ${TRG} ${threads}"
else
    FIX="cat"
fi
test -s "${output_prefix}.${SRC}${TRG}.fix.zst" ||
  paste <(zstdmt -dc "${output_prefix}.${SRC}.monofix.zst") <(zstdmt -dc "${output_prefix}.${TRG}.monofix.zst") \
      | $FIX \
      | zstdmt > "${output_prefix}.${SRC}${TRG}.fix.zst"

######################################################################
echo "### Rule-based filtering"
test -s "${output_prefix}.${SRC}${TRG}.rule-based.zst" ||
  zstdmt -dc "${output_prefix}.${SRC}${TRG}.fix.zst" |
  parallel --no-notice --pipe -k -j "${threads}" --block 50M \
    "python3 tools/clean_parallel.py -l1 ${SRC} -l2 ${TRG} --debug" \
    2>"${output_prefix}.${SRC}${TRG}.clean.debug.txt" |
  zstdmt >"${output_prefix}.${SRC}${TRG}.rule-based.zst"

######################################################################
echo "### Language identification"
test -s "${output_prefix}.${SRC}${TRG}.langid.zst" ||
  # langid_fasttext.py will download this file if it is not already present. When it runs in
  # parallel, this will typically cause the file to be corrupt.
  test -s tools/lid.176.bin || wget -O tools/lid.176.bin https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin
  zstdmt -dc "${output_prefix}.${SRC}${TRG}.rule-based.zst" |
  # memory intensive
  parallel --no-notice --pipe -k -j "$(echo "${threads}"/4 | bc)" --block 50M \
    "python3 -Wi tools/langid_fasttext.py -f 1 | python3 -Wi tools/langid_fasttext.py -f 1" |
  grep -P "^${SRC}\t${TRG}\t" |
  cut -f3,4 |
  zstdmt >"${output_prefix}.${SRC}${TRG}.langid.zst"

######################################################################
echo "### Removing leading and repetitive white spaces"

zstdmt -dc "${output_prefix}.${SRC}${TRG}.langid.zst" |
cut -f1 |
sed -e 's/^[[:space:]]*//' |
tr -s " " |
zstdmt >"${output_prefix}.${SRC}.zst"

zstdmt -dc "${output_prefix}.${SRC}${TRG}.langid.zst" |
cut -f2 |
sed -e 's/^[[:space:]]*//' |
tr -s " " |
zstdmt >"${output_prefix}.${TRG}.zst"

test -s "${output_prefix}.${SRC}.zst" || exit 1
test -s "${output_prefix}.${TRG}.zst" || exit 1

echo "### Remove input_prefix from intermediate steps"
rm -rf "${output_prefix}".*.nrm.zst "${output_prefix}".*.langid.zst \
  "${output_prefix}".*.rule-based.zst "${output_prefix}".*.*fix.zst

echo "### Clean ${input_prefix} is written to  ${output_prefix}"

echo "###### Done: Cleaning corpus"


clean/clean-mono.sh

#!/bin/bash
##
# Basic cleaning of monolingual corpora.
#
# This script takes in a an archive file, e.g. /builds/worker/artifacts/news_2007.en.zst
# and rewrites in place using a variety of cleaning rules including:
#
#  - De-escape special characters.
#  - Remove non-printing characters.
#  - Specific dataset fixes provided by: pipeline/clean/fixes/*.sh
#  - Filter by language detection (via fastText)

set -x
set -euo pipefail

echo "###### Cleaning monolingual data"

#                   Example inputs:
lang=$1             # en
input_prefix=$2     # $MOZ_FETCHES_DIR/news_2007
output_prefix=$3    # /builds/worker/artifacts/news_2007
threads=$4          # auto
dataset=$5          # news-crawl_news.2007
fluency_threshold=$6 # 0.7

# Example output: /builds/worker/artifacts/news_2007.en.zst

echo "### Cleaning ${input_prefix}"

if [ "$threads" = "auto" ]; then
  threads=$(nproc)
fi
cd "$(dirname "${0}")"
export PYTHONPATH="tools"

dir="$(dirname "${output_prefix}")"
mkdir -p "${dir}"

######################################################################
echo "### Basic preprocessing from moses"
test -s "${output_prefix}.${lang}.nrm.zst" ||
  zstdmt -dc "${input_prefix}.${lang}.zst" |
  parallel --no-notice --pipe -k -j "${threads}" --block 50M \
    "perl tools/deescape-special-chars.perl | perl tools/remove-non-printing-char.perl" |
  zstdmt -c >"${output_prefix}.${lang}.nrm.zst"

#####################################################################
echo "### Apply dataset fixes from pipeline/clean/fixes"
if [[ ! -x fixes/${dataset}.${lang}.sh ]]; then
  test -s "${output_prefix}.${lang}.monofix.zst" ||
    cp "${output_prefix}.${lang}.nrm.zst" "${output_prefix}.${lang}.monofix.zst"
else
  test -s "${output_prefix}.${lang}.monofix.zst" ||
    zstdmt -dc "${output_prefix}.${lang}.nrm.zst" \
        | fixes/"${dataset}"."${lang}".sh \
        | zstdmt >"${output_prefix}.${lang}.monofix.zst"
fi

######################################################################
echo "### Filter by language identification"
test -s "${output_prefix}.${lang}.langid.zst" ||
  # langid_fasttext.py will download this file if it is not already present. When it runs in
  # parallel, this will typically cause the file to be corrupt.
  test -s tools/lid.176.bin || wget -O tools/lid.176.bin https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin
  zstdmt -dc "${output_prefix}.${lang}.monofix.zst" |
  # memory intensive
  parallel --no-notice --pipe -k -j "$(echo "${threads}"/4 | bc)" --block 50M "python3 tools/langid_fasttext.py" |
  grep -P "^${lang}\t" | cut -f2 |
  zstdmt >"${output_prefix}.${lang}.langid.zst"

######################################################################
echo "### Rule-based filtering"

zstdmt -dc "${output_prefix}.${lang}.langid.zst" |
parallel --no-notice --pipe -k -j "${threads}" --block 50M \
  "python3 tools/clean_mono.py -l ${lang} --debug" \
  2>"${output_prefix}.${lang}.clean.debug.txt" |
zstdmt >"${output_prefix}.${lang}.rule-based.zst"

test -s "${output_prefix}.${lang}.rule-based.zst" || exit 1

######################################################################
echo "### Filter by fluency score"

if [ "${fluency_threshold}" == "0" ] || [ "${fluency_threshold}" == "0.0" ]; then
  echo "Threshold is 0, skipping filtering"
  cp "${output_prefix}.${lang}.rule-based.zst" "${output_prefix}.${lang}.zst"
else
  # the model is 125MB, similar in size to the fastText one, so it's ok to download it here
  monocleaner-download $lang ${dir}/monocleaner
  test -s "${output_prefix}.${lang}.zst" ||
    zstd -dc "${output_prefix}.${lang}.rule-based.zst" |
    # memory intensive
    parallel --no-notice --pipe -k -j "$(echo "${threads}"/4 | bc)" --block 50M "monocleaner --disable_hardrules --disable_lang_ident ${dir}/monocleaner/${lang}" |
    awk -F'\t' '$2>'${fluency_threshold} | cut -f1 |
    zstdmt >"${output_prefix}.${lang}.zst"

  test -s "${output_prefix}.${lang}.zst" || exit 1
fi
echo "Lines before filtering: $(zstdmt -dc "${input_prefix}.${lang}.zst" | wc -l)"
echo "Lines after rule-based filtering: $(zstdmt -dc "${output_prefix}.${lang}.rule-based.zst" | wc -l)"
echo "Lines after fluency filtering: $(zstdmt -dc "${output_prefix}.${lang}.zst" | wc -l)"

######################################################################
echo "### Remove data from intermediate steps"
rm -rf "${output_prefix}".*.nrm.zst "${output_prefix}".*.langid.zst \
  "${output_prefix}".*.monofix.zst "${output_prefix}".*.rule-based.zst ${dir}/monocleaner

echo "### Rule-based cleaning log written to: ${output_prefix}.${lang}.clean.debug.txt"
echo "### Clean data is written to: ${output_prefix}.${lang}.zst"

echo "###### Done: Cleaning monolingual data"


clean/fixes/detok.sh

#!/bin/bash
set -e

# Detokenize

SRC=$1
TRG=$2
threads=$3

temp=$(mktemp -d)

tee >(cut -f1 | sacremoses -j $threads -l $SRC detokenize >$temp/$SRC.detok) \
    | cut -f2 | sacremoses -j $threads -l $TRG detokenize >$temp/$TRG.detok

paste $temp/$SRC.detok $temp/$TRG.detok

rm -r $temp


clean/fixes/mtdata_JW300.mt.sh

#!/bin/bash

# Fix Maltese tokenization in JW300 that detokenizer cannot fix
sed "s/ - $(echo -ne \u200b) /-/g" \
    | sed 's/ - /-/g'


clean/fixes/mtdata_JW300.sh

#!/bin/bash
set -e

# Detokenize JW300
fixes/detok.sh $1 $2 $3


clean/fixes/mtdata_OPUS_DOGC_v2.ca.sh

#!/bin/bash

# Detokenize Catalan apostrophe, dates and laws, and ending period
# detokenize middle dot
sed "s/\([lndsLNDS]\) ' \([a-zA-Z1]\)/\1'\2/g" \
    | sed "s#\([0-9]\) \?/ \?\([0-9]\)#\1/\2#g" \
    | sed "s/\([a-z]\) .\$/\1./g" \
    | sed "s/l · l/l·l/g"


clean/fixes/mtdata_OPUS_DOGC_v2.es.sh

#!/bin/bash

# Detokenize dates and laws, and ending period
sed "s#\([0-9]\) \?/ \?\([0-9]\)#\1/\2#g" \
    | sed "s/\([a-z]\) .\$/\1./g"


clean/fixes/mtdata_OPUS_DOGC_v2.sh

#!/bin/bash
set -e

# Detokenize DOGC
fixes/detok.sh $1 $2 $3


clean/fixes/mtdata_OPUS_ECB_v1.sh

#!/bin/bash
set -e

# Detokenize JW300
fixes/detok.sh $1 $2 $3


clean/fixes/mtdata_OPUS_SETIMES_v2.sh

#!/bin/bash
set -e

# Detokenize SETIMES

SRC=$1
TRG=$2
threads=$3

temp=$(mktemp -d)

tee >(cut -f1 | sacremoses -j $threads -l $SRC detokenize >$temp/$SRC.detok) \
    >(cut -f2 | sacremoses -j $threads -l $TRG detokenize >$temp/$TRG.detok)

paste $temp/$SRC.detok $temp/$TRG.detok

rm -r $temp


clean/fixes/mtdata_OPUS_UNPC_v1_0.en.sh

#!/bin/bash

# Detokenize English possessive
sed "s/\([a-z]\) ' \([s]\)/\1'\2/g"


clean/fixes/mtdata_OPUS_UNPC_v1_0.fr.sh

#!/bin/bash

# Detokenize French apostrophe
sed "s/\([lndsLNDS]\) ' \([a-zA-Z]\)/\1'\2/g"


clean/fixes/mtdata_neulab_tedtalksv1_train.ro.sh

#!/bin/bash

# Detokenize Romanian hyphens
sed -E "s/(\w) - (\w)/\1-\2/g"


clean/fixes/mtdata_neulab_tedtalksv1_train.sh

#!/bin/bash
set -e

# Detokenize neulabs
fixes/detok.sh $1 $2 $3


clean/merge-corpus.py

"""
Merges multiple corpora into a single "source" language file, and a single "target"
language file, each.

For instance:

  dataset1.en.zst dataset1.ru.zst
  dataset2.en.zst dataset2.ru.zst
  dataset3.en.zst dataset3.ru.zst

  Gets merged into:

  corpus.en.zst
  corpus.ru.zst
"""

import argparse
from contextlib import ExitStack
from glob import glob
from pathlib import Path
from typing import Generator, Optional
from pipeline.common.datasets import (
    FilteringStep,
    Statistics,
    WeakStringSet,
    shuffle_with_max_lines,
)
from pipeline.common.downloads import get_human_readable_file_size, read_lines, write_lines
from pipeline.common.logging import get_logger

logger = get_logger(__file__)


class FilteringStatistics(Statistics):
    """
    Gather statistics about the filtering process.
    """

    def __init__(self, dataset_path: Path) -> None:
        super().__init__(dataset_path)
        self.parallel_corpus = FilteringStep(
            "The parallel corpora are merged and deduplicated",
        )
        self.final_truncated = FilteringStep("The final result can be truncated by max_lines")
        self.datasets = []

    def add_parallel_dataset(self, location: str):
        # e.g. /path/to/ada83_v1.en.zst
        path = Path(location)
        # e.g. ada83_v1
        dataset_stem = Path(path.stem).stem
        step = FilteringStep(dataset_stem)
        self.datasets.append(step)
        return step


def log_dataset(location: str):
    logger.info(f"Reading dataset {location}")


class DeduplicateCorpus:
    def __init__(
        self,
        datasets_src: list[Path],
        datasets_trg: list[Path],
        src_outpath: Path,
        trg_outpath: Path,
        stats: FilteringStatistics,
    ) -> None:
        self.datasets_src: list[Path] = datasets_src
        self.datasets_trg: list[Path] = datasets_trg
        self.src_outpath: Path = src_outpath
        self.trg_outpath: Path = trg_outpath
        self.stats: FilteringStatistics = stats
        self.dataset_stats: FilteringStep = None

    def run(
        self,
        total_corpus_bytes: int,
        max_lines: Optional[int],
    ):
        stats = self.stats
        with ExitStack() as stack:
            src_outfile = stack.enter_context(write_lines(self.src_outpath))
            trg_outfile = stack.enter_context(write_lines(self.trg_outpath))

            if max_lines:
                for line in shuffle_with_max_lines(
                    line_stream=self.yield_lines_string(stack),
                    seed=38540735095,
                    max_lines=max_lines,
                    total_byte_size=total_corpus_bytes,
                ):
                    src_line, trg_line = line.split("\t")
                    src_outfile.write(src_line)
                    trg_outfile.write(trg_line)

                stats.final_truncated.visited = stats.parallel_corpus.kept
                stats.final_truncated.kept = min(max_lines, stats.parallel_corpus.kept)
            else:
                for src_line, trg_line in self.yield_lines_tuple(stack):
                    src_outfile.write(src_line)
                    trg_outfile.write(trg_line)

                stats.final_truncated.kept = stats.parallel_corpus.kept
                stats.final_truncated.visited = stats.parallel_corpus.kept

    def yield_lines_tuple(self, stack: ExitStack) -> Generator[tuple[str, str], None, None]:
        strings_seen = WeakStringSet()
        stats = self.stats
        src_lines: Generator[str, None, None] = stack.enter_context(
            read_lines(self.datasets_src, on_enter_location=self.on_enter_location)
        )
        trg_lines: Generator[str, None, None] = stack.enter_context(
            read_lines(self.datasets_trg, on_enter_location=log_dataset)
        )

        for src_line, trg_line in zip(src_lines, trg_lines):
            # No separator is needed as the newline is included.
            line = src_line + trg_line

            if line in strings_seen:
                stats.parallel_corpus.filtered += 1
                self.dataset_stats.filtered += 1
            else:
                stats.parallel_corpus.kept += 1
                self.dataset_stats.kept += 1

                strings_seen.add(line)

                yield src_line, trg_line

    def yield_lines_string(self, stack: ExitStack) -> Generator[str, None, None]:
        for src_line, trg_line in self.yield_lines_tuple(stack):
            if "\t" in src_line or "\t" in trg_line:
                logger.error("A line contained a tab character, skipping:")
                logger.error(f" src: {src_line}")
                logger.error(f" trg: {src_line}")
            else:
                yield f"{src_line}\t{trg_line}"

    def on_enter_location(self, location):
        log_dataset(location)
        self.dataset_stats = self.stats.add_parallel_dataset(location)


def sample_corpus(
    artifacts: Path, name: str, sample_size: int, src_outpath: Path, trg_outpath: Path
):
    """
    Generate a sample of the corpus data with the following format:

    e.g.
    > cat artifacts/corpus.sample.txt
    Sentence 1 in source language
    Sentence 1 in target language

    Sentence 2 in source language
    Sentence 2 in target language

    Sentence 3 in source language
    Sentence 3 in target language
    ...
    """
    total_byte_size = src_outpath.stat().st_size + trg_outpath.stat().st_size

    with ExitStack() as stack:
        sample_path = artifacts / f"{name}.sample.txt"

        src_lines = stack.enter_context(read_lines(src_outpath))
        trg_lines = stack.enter_context(read_lines(trg_outpath))
        sample_outfile = stack.enter_context(
            write_lines(
                sample_path,
                # The browser won't know the encoding when viewing this sample without including
                # a "byte order mark", which python can do via this encoding.
                encoding="utf-8-sig",
            )
        )

        def join_src_trg():
            for src_line, trg_line in zip(src_lines, trg_lines):
                # The src and trg line each have a newline at the end. This means that
                # each sentence pair will be separate by a blank line to make for easy
                # scanning of datasets.
                yield f"{src_line}{trg_line}\n"

        logger.info("Stream in:")
        logger.info(f" - {src_outpath}")
        logger.info(f" - {trg_outpath}")
        logger.info(f"Write a {sample_size:,} line sample of the merged corpus:")
        logger.info(f" - {sample_path}")

        for line in shuffle_with_max_lines(
            line_stream=join_src_trg(),
            seed=9834523434,
            max_lines=sample_size,
            total_byte_size=total_byte_size,
        ):
            sample_outfile.write(line)


def get_datasets(src: str, trg: str, datasets_glob: str):
    dataset_paths: list[str] = glob(datasets_glob)
    datasets_src: list[Path] = []
    datasets_trg: list[Path] = []
    dataset_paths.sort()

    total_corpus_bytes = 0

    for dataset in dataset_paths:
        path = Path(dataset)
        if dataset.endswith(f"{src}.zst"):
            datasets_src.append(path)
        elif dataset.endswith(f"{trg}.zst"):
            datasets_trg.append(path)
        else:
            raise Exception(f"Dataset does not match naming scheme: {dataset}")

        formatted_size, bytes = get_human_readable_file_size(path)
        logger.info(f" - {path} ({formatted_size})")
        total_corpus_bytes += bytes

    return datasets_src, datasets_trg, total_corpus_bytes


def main() -> None:
    parser = argparse.ArgumentParser(
        description=__doc__,
        # Preserves whitespace in the help text.
        formatter_class=argparse.RawTextHelpFormatter,
    )
    parser.add_argument(
        "--src",
        type=str,
        help="The source locale",
    )

    parser.add_argument(
        "--trg",
        type=str,
        help="The target locale",
    )

    parser.add_argument(
        "--datasets_glob",
        type=str,
        help="A glob-style path to the mono datasets, e.g. /path/to/*.zst",
    )

    parser.add_argument(
        "--max_lines",
        type=str,
        default="None",
        help="The (optionally) maximum number of sentences that will be merged.",
    )

    parser.add_argument(
        "--sample_size", type=int, default=10_000, help="Generate a random sample of sentences."
    )

    parser.add_argument(
        "--artifacts",
        type=Path,
        help="The path to the artifacts directory.",
    )

    parser.add_argument(
        "--name",
        type=str,
        help='The final corpus name, e.g. "corpus" will output a "corpus.en.zst" file.',
    )

    args = parser.parse_args()

    datasets_src, datasets_trg, total_corpus_bytes = get_datasets(
        args.src, args.trg, args.datasets_glob
    )

    logger.info("Parallel datasets:")

    src_outpath = args.artifacts / f"{args.name}.{args.src}.zst"
    trg_outpath = args.artifacts / f"{args.name}.{args.trg}.zst"

    stats = FilteringStatistics(args.artifacts / args.name)

    max_lines: Optional[int] = None
    if args.max_lines != "None":
        max_lines = int(args.max_lines)

    deduplicate_corpus = DeduplicateCorpus(
        datasets_src,
        datasets_trg,
        src_outpath,
        trg_outpath,
        stats,
    )

    deduplicate_corpus.run(total_corpus_bytes, max_lines)

    sample_corpus(
        artifacts=args.artifacts,
        name=args.name,
        sample_size=args.sample_size,
        src_outpath=src_outpath,
        trg_outpath=trg_outpath,
    )

    stats.save_json()


if __name__ == "__main__":
    main()


clean/merge-mono.py

import argparse
import glob
import os
from dataclasses import dataclass
from pathlib import Path
from typing import Generator

from pipeline.common.datasets import (
    CountingStep,
    FilteringStep,
    Statistics,
    WeakStringSet,
    shuffle_with_max_lines,
)
from pipeline.common.downloads import (
    format_bytes,
    get_human_readable_file_size,
    read_lines,
    write_lines,
)
from pipeline.common.logging import get_logger
from pipeline.common.memory import log_memory

logger = get_logger(__file__)


@dataclass
class FilteringStatistics(Statistics):
    """
    Gather statistics about the filtering process.
    """

    def __init__(self, dataset_path: Path) -> None:
        super().__init__(dataset_path)
        self.final_truncated_monolingual_lines = CountingStep(
            "After truncation via the config's `experiment.mono-max-sentences-src.total`, "
            "how many lines are left."
        )

        self.final_truncated_monolingual_codepoints = CountingStep(
            "The amount of codepoints in the final monolingual corpus."
        )

        self.parallel_corpus_lines = CountingStep(
            "The size of the merged parallel corpus before truncation."
        )

        self.duplicates_of_parallel_corpus = CountingStep(
            "How much of the monolingual data was duplicated in the merged parallel corpus."
        )

        self.duplicates_of_monolingual_corpus = CountingStep(
            "How much of the monolingual data was duplicated across the monolingual datasets."
        )

        self.deduplicated_size = FilteringStep(
            "What was the size of the monolingual data and how much was deduplicated. This "
            "doesn't count the truncation of datasets at the datasets gathering time."
        )

        self.deduplicated_monolingual_lines = CountingStep(
            "After deduplication, how much monolingual data is left."
        )


def filter_and_write_monolingual_data(
    mono_datasets: list[str],
    output_path: Path,
    parallel_hashes: WeakStringSet,
    max_lines: int,
    sample_size: int,
    stats: FilteringStatistics,
) -> None:
    """
    Filtering is done with a set[int]. Seeing if a line is in the set should be O(1)
    in terms of time complexity. A set[int] was chosen (storing the hash) rather than
    a set[str], as the latter would retain the string in memory.
    """

    mono_hashes = WeakStringSet()

    def deduplicate_lines(lines: Generator[str, None, None]) -> Generator[str, None, None]:
        """
        This is the generator that will perform the deduplication on a line stream. It's passed
        into the shuffler, so needs to be its own function.
        """
        parallel_discards = 0
        mono_discards = 0
        retained = 0
        for line in lines:
            # Don't add this sentence if it's in the original parallel corpus, or if it's
            # already present in the monolingual data, perhaps from another source.
            if line in parallel_hashes:
                parallel_discards += 1
            elif line in mono_hashes:
                mono_discards += 1
            else:
                retained += 1
                mono_hashes.add(line)  # Don't add this sentence again.

                # Report progress periodically.
                if retained % 1_000_000 == 0:
                    discards = parallel_discards + mono_discards
                    log_memory()
                    logger.info(f"{retained:,} kept, {discards:,} discarded")

                yield line

        stats.deduplicated_size.kept = retained
        stats.deduplicated_size.filtered = parallel_discards + mono_discards
        stats.deduplicated_monolingual_lines.value = retained

        stats.duplicates_of_parallel_corpus.value = parallel_discards
        stats.duplicates_of_monolingual_corpus.value = mono_discards
        stats.parallel_corpus_lines.value = len(parallel_hashes)

    # Estimate the byte size. The better the estimate, the better the data distribution will be.
    # When filtering mono NLLB data against parallel NLLB data, roughly 70% is kept.
    byte_size_estimate = 0
    for dataset in mono_datasets:
        byte_size_estimate += os.path.getsize(dataset)
    byte_size_estimate *= 0.7

    log_memory(gc_collect=True)
    logger.info("Deduplicated and shuffling lines in memory.")
    with read_lines(mono_datasets) as mono_dataset_lines:
        final_lines = shuffle_with_max_lines(
            line_stream=deduplicate_lines(
                mono_dataset_lines,
            ),
            seed=347489345,
            max_lines=max_lines,
            total_byte_size=byte_size_estimate,
        )

    log_memory(gc_collect=True)
    logger.info(f"Write the final file: {output_path}")
    with write_lines(output_path) as outfile:
        stats.final_truncated_monolingual_lines.value = len(final_lines)
        for i, line in enumerate(final_lines):
            stats.final_truncated_monolingual_codepoints.value += len(line)
            outfile.write(line)
            if i % 1_000_000 == 999_999:
                logger.info(f"Wrote line {i+1:,} to {output_path}")

    log_memory(gc_collect=True)
    sample_path = output_path.parent / f"{output_path.stem}.sample.txt"
    logger.info(f"Write a 10,000 line sample of the final: {sample_path}")
    with write_lines(
        sample_path,
        # The browser won't know the encoding when viewing this sample without including
        # a "byte order mark", which python can do via this encoding.
        encoding="utf-8-sig",
    ) as outfile:
        for line in shuffle_with_max_lines(
            line_stream=final_lines,
            seed=9834523434,
            max_lines=sample_size,
            total_byte_size=os.path.getsize(output_path),
        ):
            outfile.write(line)

    log_memory(gc_collect=True)
    stats_path = stats.save_json()
    logger.info(f"Saved the stats: {stats_path}")


def compute_line_hashes(path: Path) -> WeakStringSet:
    """
    In order to de-duplicate sentences we can compute a hash and store it in memory. This makes
    it so that we don't have to store the full sentence in memory. It's about 10 bytes per int
    stored in the set.
    """
    line_hashes = WeakStringSet()
    sentences_visited = 0

    with read_lines(path) as lines:
        for line in lines:
            sentences_visited += 1
            if sentences_visited % 1_000_000 == 0:
                logger.info(f"Hashing sentence {sentences_visited:,}")
            line_hashes.add(line)

    return line_hashes


def main() -> None:
    parser = argparse.ArgumentParser(description="Merge monolingual datasets.")
    parser.add_argument(
        "--parallel_corpus",
        type=Path,
        help="The path to the parallel corpus of this language, e.g. $MOZ_FETCHES_DIR/corpus.ca.zst",
    )
    parser.add_argument(
        "--output",
        type=Path,
        help="The path to the output compressed file, e.g. /builds/worker/artifacts/mono.ca.zst",
    )
    parser.add_argument(
        "--max_sentences", type=int, help="The maximum number of sentences that will be merged."
    )
    parser.add_argument(
        "--datasets_glob",
        type=str,
        help="A glob-style path to the mono datasets, e.g. /path/to/*.zst",
    )
    parser.add_argument(
        "--sample_size", type=int, default=10_000, help="Generate a random sample of sentences."
    )

    args = parser.parse_args()

    output_path: Path = args.output
    max_sentences: int = args.max_sentences
    parallel_corpus: str = args.parallel_corpus
    mono_dataset_paths: list[str] = glob.glob(args.datasets_glob)

    if not mono_dataset_paths:
        raise FileNotFoundError(f"No files found matching glob pattern: {args.datasets_glob}")

    logger.info("Monolingual datasets:")
    total_mono_bytes = 0
    for path in mono_dataset_paths:
        formatted_size, bytes = get_human_readable_file_size(path)
        logger.info(f" - {path} ({formatted_size})")
        total_mono_bytes += bytes

    logger.info(f" - {format_bytes(total_mono_bytes)} total")

    formatted_size = (get_human_readable_file_size(path))[0]
    logger.info("Parallel corpus:")
    logger.info(f" - {parallel_corpus} ({formatted_size})")

    # Ensure output directory exists
    output_dir = output_path.parent
    output_dir.mkdir(parents=True, exist_ok=True)

    # Compute the line hashes so that the monolingual data can be de-duplicated.
    # It's about 10 bytes per hash in a set, so for a 100 million sentence corpus,
    # it would be ~1G in memory.
    log_memory()
    logger.info(f"Compute hashes of the parallel data: {path}")
    line_hashes = compute_line_hashes(parallel_corpus)

    stats = FilteringStatistics(output_path)

    filter_and_write_monolingual_data(
        mono_datasets=mono_dataset_paths,
        output_path=output_path,
        parallel_hashes=line_hashes,
        max_lines=max_sentences,
        sample_size=args.sample_size,
        stats=stats,
    )

    logger.info("Done: Merging monolingual datasets")


if __name__ == "__main__":
    main()
    log_memory()


clean/opuscleaner/clean-corpus.sh

#!/bin/bash
##
# Cleaning parallel corpora with OpusCleaner
#

set -x
set -euo pipefail

echo "###### Cleaning corpus with OpusCleaner"

test -v SRC
test -v TRG

input_prefix=$1
output_prefix=$2
threads=$3
dataset=$4
mode=$5

if [ "$threads" = "auto" ]; then
  threads=$(nproc)
fi

cd "$(dirname "${0}")"
dir="$(dirname "${output_prefix}")"
mkdir -p "${dir}"

echo "Downloading FastText model."
# pre-download fast text model as it's causing constant issues
filters_dir="/builds/worker/.local/lib/python3.10/site-packages/opuscleaner/filters"
wget -O "${filters_dir}/large.bin" https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin

echo "### Generating cleaning config: ${dataset}.${SRC}-${TRG}.filters.json"
# save new filter to dataset output dir
filter_path="${output_prefix}.${SRC}-${TRG}.filters.json"
python3 generate_filters.py "${input_prefix}" "${SRC}" "${TRG}" "${dataset}" "${filter_path}" "${mode}"
test -s "${filter_path}" || exit 1

echo "### Cleaning ${input_prefix} with filter ${filter_path}"
paste <(zstdmt -dc "${input_prefix}.${SRC}.zst") \
      <(zstdmt -dc "${input_prefix}.${TRG}.zst") |
opuscleaner-clean \
  --parallel ${threads} \
  --batch-size=50000 \
  --input=- \
  "${filter_path}" "${SRC}" "${TRG}" |
  tee >(cut -f1 | zstdmt >"${output_prefix}.${SRC}.zst") |
        cut -f2 | zstdmt >"${output_prefix}.${TRG}.zst"

echo "### Checking length of the files"
test -s "${output_prefix}.${SRC}.zst" || exit 1
test -s "${output_prefix}.${TRG}.zst" || exit 1
new_len_src="$(zstdmt -dc "${output_prefix}.${SRC}.zst" | wc -l)"
new_len_trg="$(zstdmt -dc "${output_prefix}.${TRG}.zst" | wc -l)"
orig_len_src="$(zstdmt -dc "${input_prefix}.${SRC}.zst" | wc -l)"
[[ ${new_len_src} -ge 1 ]] || exit 1
[[ ${new_len_trg} -ge 1 ]] || exit 1
[[ "${new_len_src}" = "${new_len_trg}" ]] || exit 1
echo "### Filtered length: ${new_len_src} / ${orig_len_src}"

echo "### Clean ${input_prefix} is written to  ${output_prefix}"

echo "###### Done: Cleaning corpus with OpusCleaner"


clean/opuscleaner/generate_filters.py

"""
Generates filter config for a dataset based on defaults to use in OpusCleaner
"""

import argparse
import json
import os
from enum import Enum
from typing import Optional

CURRENT_FOLDER = os.path.dirname(os.path.abspath(__file__))


class Mode(Enum):
    custom = "custom"
    defaults = "defaults"


def find_custom_filter(src: str, trg: str, dataset: str) -> Optional[str]:
    # TODO: we'll likely need to move to a separate repo for those
    # TODO: to not include all filters for all languages in TC artifacts

    # workaround: we use "_" or "/" to separate the dataset version for OPUS datasets and OpusCleaner uses "-"
    idx = dataset.rfind("/") if "/" in dataset else dataset.rfind("_")
    dataset_opus = f"{dataset[:idx]}-{dataset[idx + 1:]}" if idx else ""

    # note: do not call the folder with default filters "filters" because it's a magic word for opuscleaner-clean
    # and it starts processing such folder

    # First look in the langauge specific directory, then in the common directory
    # as language specific dataset configs should override the generic ones.
    # The cleaning rules are symmetrical, so we can use one language specific config for both directions
    paths = [
        f"{CURRENT_FOLDER}/configs/{src}-{trg}/{dataset}.filters.json",
        f"{CURRENT_FOLDER}/configs/{src}-{trg}/{dataset_opus}.filters.json",
        f"{CURRENT_FOLDER}/configs/{src}-{trg}/default.filters.json",
        f"{CURRENT_FOLDER}/configs/{dataset}.filters.json",
        f"{CURRENT_FOLDER}/configs/{dataset_opus}.filters.json",
    ]

    for path in paths:
        if os.path.exists(path):
            return path
    return None


def build_config(config_path: str, src: str, trg: str) -> str:
    # TODO: ideally "other" for "deescape-special-chars" should be replaced to <trg> for supported languages
    with open(config_path) as f:
        config_str = f.read()
        config_str = config_str.replace("<src>", src).replace("<trg>", trg)
        # this replacement is required for the custom filters that were copied from OpusCleaner UI too
        abs_path_patterns = f"{CURRENT_FOLDER}/configs/remove_frequent_patterns.txt"
        config_str = config_str.replace("configs/remove_frequent_patterns.txt", abs_path_patterns)
        return json.loads(config_str)


def generate(dataset: str, output: str, src: str, trg: str, mode: Mode) -> None:
    # look whether there are custom filters produced by OpusCleaner UI first
    # if a custom filter is not found, use defaults
    filter_path = None
    if mode == Mode.custom:
        filter_path = find_custom_filter(src, trg, dataset)
    if filter_path is None:
        filter_path = f"{CURRENT_FOLDER}/configs/default.filters.json"

    print(f"Using filter {filter_path}")
    config = build_config(filter_path, src, trg)
    with open(output, "w") as f:
        json.dump(config, f, indent=2)


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "input_prefix", metavar="INPUT_PREFIX", type=str, help="Dataset file prefix"
    )
    parser.add_argument("src", metavar="SRC", type=str, help="Source language code")
    parser.add_argument("trg", metavar="TRG", type=str, help="Target language code")
    parser.add_argument("dataset", metavar="DATASET", type=str, help="Dataset name")
    parser.add_argument("output", metavar="OUTPUT_PATH", type=str, help="Write filter config here")
    parser.add_argument(
        "mode",
        metavar="MODE",
        type=Mode,
        choices=list(Mode),
        default=Mode.defaults,
        help="What filters to use: custom dataset specific ones or the defaults",
    )
    args = parser.parse_args()

    generate(args.dataset, args.output, args.src, args.trg, args.mode)


clean/requirements/clean.in

opuscleaner==0.4.3
fasttext==0.9.2
sacremoses==0.0.53
more_itertools==10.1.0
requests==2.31.0
opusfilter==3.2.0


clean/requirements/merge.in

requests==2.31.0
psutil==6.0.0


clean/tools/clean_mono.py

#!/usr/bin/env python
# -*- coding: utf-8 -*-

import argparse
import re
import sys

MIN_LENGTH = 2  # minimum number of words in a sentence
MAX_LENGTH = 150  # maximum number of words in a sentence

RATIO_ALPHA_WORDS = 0.4  # minimum fraction of "real" words in a sentence
RATIO_ALPHA_CHARS = 0.5  # minimum fraction of alpha characters in a sentence

from clean_parallel import CHARS


def main():
    args = parse_user_args()

    for i, line in enumerate(sys.stdin):
        src = line.strip()
        if not src:
            continue

        skip = clean_mono(src, args.lang)
        if skip:
            if args.debug:
                sys.stderr.write("{}\t{}\n".format(skip, src))
            continue
        sys.stdout.write("{}\n".format(src))


def clean_mono(src, lang):
    # TODO: move mono cleaning to OpusCleaner
    #  when it support this https://github.com/hplt-project/OpusCleaner/issues/141

    # treat individual characters as tokens for CJK
    src_toks = src.split() if lang not in {"zh", "ja", "ko"} else src
    src_len = len(src_toks)

    if not src_len:
        return "EMPTY"

    if src_len < MIN_LENGTH:
        return "TOO_SHORT"

    if src_len > MAX_LENGTH:
        return "TOO_LONG"

    if lang in CHARS:
        num_alpha = sum([1 if re.match(CHARS[lang], t, re.IGNORECASE) else 0 for t in src_toks])
        if num_alpha / float(src_len) < RATIO_ALPHA_WORDS:
            return "RATIO_ALPHA"

        char_alpha = len(re.findall(CHARS[lang], src, re.IGNORECASE))
        if char_alpha / float(len(src.replace(" ", ""))) < RATIO_ALPHA_CHARS:
            return "RATIO_CHARS"

    return None


def parse_user_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("-l", "--lang", default="en")
    parser.add_argument("--debug", action="store_true")
    return parser.parse_args()


if __name__ == "__main__":
    main()


clean/tools/clean_parallel.py

#!/usr/bin/env python
# -*- coding: utf-8 -*-

import argparse
import re
import sys

# The variables below need to be adjusted for a language pair and dataset.
# To add a new language, define the list of alpha characters in the dict below.

MIN_LENGTH = 1  # minimum number of words in a sentence, should be > 0
MAX_LENGTH = 150  # maximum number of words in a sentence
RATIO_LENGTH = 0.5  # minimum length ratio of source/target and target/source

RATIO_ALPHA_WORDS = 0.4  # minimum fraction of "real" words in a source sentence
RATIO_ALPHA_CHARS = 0.5  # minimum fraction of alpha characters in a source sentence

CHARS = {
    "bg": r"[АаБбВвГгДддЕеЖжЗзИиЙйКкkasЛлМмНнОоПпРрСсТтУуФфХхЦцЧчШшЩщЪъЬьЮюЯя]",
    "cs": r"[a-zÁáČčĎďÉéěÍíŇňÓóŘřŠšŤťÚúůÝýŽž]",
    "ca": r"[a-zÀàÈèÉéÍíÒòÓóÚúÇç]",
    "da": r"[a-zÆæØøÅå]",
    "de": r"[a-zÄäÖöÜüß]",
    "en": r"[a-z]",
    "el": r"[a-zΑαΒβΓγΔδΕεΖζΗηΘθΙιΚκΛλΜμΝνΞξΟοΠπΡρΣσςΤτΥυΦφΧχΨψΩω]",
    "es": r"[a-zÁáÉéÍíÓóÚúñÑ]",
    "et": r"[a-zÕõÄäÖöÜü]",
    "eu": r"[a-zñÑ]",
    "fi": r"[a-zÅåÄäÖö]",
    "fr": r"[a-zÂâÁáÀàâÇçÉéÈèÊêÓóÒòÔôŒœÜüÛûŸÿ]",
    "ga": r"[abcdefghilmnoprstuáéíóúÁÉÍÓÚ]",
    "gl": r"[a-zÁáÉéÍíÓóÚúÑñ]",
    "hr": r"[abcčČćĆdđĐefghijklmnoprsšŠtuvzžŽ]",
    "hu": r"[a-zÁáÉéÍíÓóÖöŐőŰű]",
    "is": r"[abdefghijklmnoprstuvxyÁáðÐÉéÍíÓóÚúÝýÞþÆæÖö]",
    "it": r"[a-zàÀèÈéÉìÌíÍîÎòÒóÓùÙúÚ]",
    "lt": r"[aąbcČčdeĘęĖėfghiĮįyjklmnoprsŠštuŲųŪūvzŽž]",
    "lv": r"[aĀābcČčdeĒēfgĢģhiĪījkĶķlĻļmnŅņoprsŠštuŪūvzŽž]",
    "mt": r"[abĊċdefĠġghĦħiiejklmnopqrstuvwxŻżz]",
    "nb": r"[a-zÂâÁáÀàâÉéÈèÊêÓóÒòÔôÜüÆæØøÅå]",
    "nl": r"[a-zÂâÁáÀàâÉéÈèÊêÓóÒòÔôÚú]",
    "no": r"[a-zÂâÁáÀàâÉéÈèÊêÓóÒòÔôÜüÆæØøÅå]",
    "nn": r"[a-zÂâÁáÀàâÉéÈèÊêÓóÒòÔôÜüÆæØøÅå]",
    "pl": r"[a-zĄąĆćĘęŁłŃńÓóŚśŹźŻż]",
    "pt": r"[a-zÂâÁáÀàÃãÇçÉéÈèÊêÍíÌìÓóÒòÔôÕõÚúÙù]",
    "ro": r"[a-zĂăÂâÎîȘșȚț]",
    "ru": r"[а-я]",
    "sk": r"[a-záäÁÄčČďĎžéÉíÍĺĹľĽňŇóÓôÔŕŔšŠťŤúÚýÝžŽ]",
    "sl": r"[abcčČdđĐefghijklmnoprsšŠtuvzžŽ]",
    "sv": r"[a-zÅåÄäÖö]",
}


def main():
    args = parse_user_args()

    for i, line in enumerate(sys.stdin):
        fields = line.strip().split("\t")
        if len(fields) < 2:
            continue

        src = fields[-2].strip()
        trg = fields[-1].strip()

        skip = clean_parallel(src, trg, args.src_lang, args.trg_lang)
        if skip:
            if args.debug:
                sys.stderr.write("{}\t{}".format(skip, line))
            continue
        sys.stdout.write(line)


def clean_parallel(src, trg, src_lang, trg_lang):
    if src.lower() == trg.lower():
        return "IDENTICAL"

    src_toks = src.split()
    trg_toks = trg.split()
    src_len = len(src_toks)
    trg_len = len(trg_toks)

    if not src_len or not trg_len:
        return "EMPTY"

    # https://stackoverflow.com/questions/23680976/python-removing-non-latin-characters
    # if re.search(u'[^\x00-\x7F\x80-\xFF\u0100-\u017F\u0180-\u024F\u1E00-\u1EFF]', src):
    #    return "SRC_NON_LATIN"

    # if re.search(u'[^\x00-\x7F\x80-\xFF\u0100-\u017F\u0180-\u024F\u1E00-\u1EFF]', trg):
    #    return "TRG_NON_LATIN"

    ratio_len = src_len / float(trg_len)
    if ratio_len < RATIO_LENGTH or ratio_len > (1.0 / RATIO_LENGTH):
        return "RATIO_LENGTH"

    if src_len < MIN_LENGTH or trg_len < MIN_LENGTH:
        return "TOO_SHORT"

    if src_len > MAX_LENGTH or trg_len > MAX_LENGTH:
        return "TOO_LONG"

    if src_lang in CHARS:
        num_alpha = sum(
            [1 if re.match(CHARS[src_lang], t, re.IGNORECASE) else 0 for t in src_toks]
        )
        if num_alpha / float(src_len) < RATIO_ALPHA_WORDS:
            return "RATIO_ALPHA_SRC"

        char_alpha = len(re.findall(CHARS[src_lang], src, re.IGNORECASE))
        if char_alpha / float(len(src.replace(" ", ""))) < RATIO_ALPHA_CHARS:
            return "RATIO_CHARS_SRC"

    if trg_lang in CHARS:
        num_alpha = sum(
            [1 if re.match(CHARS[trg_lang], t, re.IGNORECASE) else 0 for t in trg_toks]
        )
        if num_alpha / float(trg_len) < RATIO_ALPHA_WORDS:
            return "RATIO_ALPHA_TRG"

        char_alpha = len(re.findall(CHARS[trg_lang], trg, re.IGNORECASE))
        if char_alpha / float(len(trg.replace(" ", ""))) < RATIO_ALPHA_CHARS:
            return "RATIO_CHARS_TRG"

    return None


def parse_user_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("-l1", "--src-lang", default="es")
    parser.add_argument("-l2", "--trg-lang", default="en")
    parser.add_argument("--debug", action="store_true")
    return parser.parse_args()


if __name__ == "__main__":
    main()


clean/tools/langid_fasttext.py

#!/usr/bin/env python3
# -*- coding: utf-8 -*-

# Usage:
#   ./langid-fasttext.py < sents.txt > code-tab-sents.txt
#
# Installation:
#   pip3 install pybind11 fasttext --user
#
# Parallelize:
#   cat sents.txt | parallel --pipe -k -j16 --block 20M ./langid-fasttext.py > code-tab-sents.txt

import argparse
import os
import sys

import fasttext

BIN = "lid.176.bin"
URL = "https://dl.fbaipublicfiles.com/fasttext/supervised-models/{}".format(BIN)


def main():
    args = parse_user_args()

    mpath = os.path.join(os.path.dirname(os.path.realpath(__file__)), BIN)
    if not os.path.exists(mpath):
        sys.stderr.write("Downloading model {} ...\n".format(URL))
        import urllib.request

        urllib.request.urlretrieve(URL, mpath)

    model = fasttext.load_model(mpath)

    for line in sys.stdin:
        fields = line.strip().split("\t")
        lid = model.predict(fields[args.field])
        sys.stdout.write("{}\t{}".format(lid[0][0][-2:], line))


def parse_user_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("-f", "--field", default=0, type=int, help="text field, default: 0")
    return parser.parse_args()


if __name__ == "__main__":
    main()


common/__init__.py

import math


# Avoid circular dependencies by placing this in this file.
def format_bytes(bytes: int) -> str:
    """Convert bytes into a human readable string."""

    if bytes == 0:
        return "0 B"

    size_name = ["B", "KB", "MB", "GB", "TB", "PB", "EB", "ZB", "YB"]
    size_index = int(math.floor(math.log(abs(bytes), 1000)))
    rounded_size = round(bytes / math.pow(1000, size_index), 1)
    return f"{rounded_size} {size_name[size_index]}"


common/arg_utils.py

"""
Utilities for arg parsing in the pipeline.
"""

from typing import Optional


def ensure_string(name: str, value: str) -> str:
    """Ensures that a value exists, handling Taskcluster's "None"."""
    if handle_none_value(value):
        return value
    raise ValueError(f"A value for {name} was not provided.")


def handle_none_value(value: str) -> Optional[str]:
    """When Taskcluster can't find a value, it uses the string "None"."""
    if value == "None":
        return None
    return value


common/command_runner.py

import os
import re
from shlex import join
import shlex
import subprocess


def _get_indented_command_string(command_parts: list[str]) -> str:
    """
    Print out a command with the flags indented, so that it's easy to read.
    """
    command = join(command_parts)
    parts = re.split(r"( --\w)", command)

    formatted_command = [parts[0].strip()]

    for i in range(1, len(parts), 2):
        option = parts[i].strip() + parts[i + 1].strip()
        formatted_command.append(f"  {option}")

    return "\n".join(formatted_command)


def apply_command_args(dict: dict[str, any]):
    """
    Takes in a dictionary, and applies the keys as command line flags.

    input: { "key": "value" }
    output: "--key value"

    input: { "inputs": ["valueA", "valueB"] }
    output: "--inputs valueA valueB"
    """

    for key, value in dict.items():
        yield f"--{key}"
        if value is None:
            continue

        if isinstance(value, list):
            for v in value:
                yield str(v)
            continue

        yield str(value)


def run_command_pipeline(
    commands: list[list[str]], pipe_stderr=False, capture=False, logger=None
) -> str | None:
    """
    Executes a series of shell commands in a pipeline, where the output of one command
    is piped to the next. Optionally captures the final output or logs the pipeline
    process. It raises `CalledProcessError` if any command in the pipeline fails.

    Args:
      commands: A list of command arguments where each command is
        represented as a list of strings.
      pipe_stderr: If True, pipes `stderr` of each command into `stdout`.
      capture: If True, captures and returns the output of the final command in the
        pipeline. If False, output is printed to stdout. Defaults to False.
      logger: A logger instance used for logging the command execution. If provided,
        it will log the constructed pipeline commands. Defaults to None.

    Example:
      python_scripts = run_command_pipeline(
        [
            ["ls", "-l"],
            ["grep", ".py"],
            ["sort"]
        ],
        capture=True
      )
    """
    if pipe_stderr:
        joiner = "2>&1 |"
    else:
        joiner = "|"

    if logger:
        # Log out a nice representation of this command.
        final_command = _get_indented_command_string(commands[0])
        for command_parts in commands[1:]:
            final_command = (
                f"{final_command}\n{joiner} {_get_indented_command_string(command_parts)}"
            )

        logger.info("Running:")
        for line in final_command.split("\n"):
            logger.info(line)

    command_string = f" {joiner} ".join([shlex.join(command) for command in commands])

    if capture:
        return subprocess.check_output(command_string, shell=True).decode("utf-8")

    subprocess.check_call(command_string, shell=True)


def run_command(
    command: list[str], capture=False, shell=False, logger=None, env=None
) -> str | None:
    """
    Runs a command and outputs a nice representation of the command to a logger, if supplied.

    Args:
      command: The command arguments provided to subprocess.check_call
      capture: If True, captures and returns the output of the final command in the
        pipeline. If False, output is printed to stdout.
      logger: A logger instance used for logging the command execution. If provided,
        it will log the pipeline commands.
      env: The environment object.

    Example:
      directory_listing = run_command(
        ["ls", "-l"],
        capture=True
      )
    """
    # Expand any environment variables.
    command = [os.path.expandvars(part) for part in command]

    if logger:
        # Log out a nice representation of this command.
        logger.info("Running:")
        for line in _get_indented_command_string(command).split("\n"):
            logger.info(line)

    if capture:
        return subprocess.check_output(command).decode("utf-8")

    subprocess.check_call(command, env=env)


common/datasets.py

from collections.abc import Iterable
import hashlib
import json
from logging import Logger
import os
import subprocess
import tempfile
from dataclasses import dataclass
from io import TextIOWrapper
from pathlib import Path
from random import Random
from typing import Callable, Iterator, Literal, Optional, Set, Union
from urllib.parse import urlparse
import unicodedata

# We keep this relatively short because these datasets end up in task labels,
# which end up in task cache routes, which need to be <= 256 characters.
DATASET_NAME_MAX_LENGTH = 50


class Dataset:
    """
    Convert a dataset key into a structured format.

    e.g.

    dataset.key               "opus_CCAligned/v1"
    dataset.importer:         "opus"
    dataset.name:             "CCAligned/v1"
    dataset.file_safe_key():  "opus_CCAligned_v1"
    dataset.file_safe_name(): "CCAligned_v1"
    """

    def __init__(self, dataset_key: str) -> None:
        key_parts = dataset_key.split("_")

        self.key = dataset_key
        self.importer = key_parts[0]
        self.name = "_".join(key_parts[1:])

        if not self.importer:
            raise Exception(f"Could not find the importer in the dataset key {dataset_key}")

        if not self.name:
            raise Exception(f"Could not find the name in the dataset key {dataset_key}")

    # Important! Keep in sync with dataset_helpers.py.
    def _escape(dataset: str) -> str:
        # URLs can be too large when used as Taskcluster labels. Create a nice identifier for them.
        # See https://github.com/mozilla/translations/issues/527
        if dataset.startswith("https://") or dataset.startswith("http://"):
            url = urlparse(dataset)

            hostname = url.hostname
            if hostname == "storage.googleapis.com":
                hostname = "gcp"

            # Get the name of the file from theh path without the extension.
            file = Path(url.path).stem
            file = file.replace(".[LANG]", "").replace("[LANG]", "")

            # Compute a hash to avoid any name collisions.
            md5 = hashlib.md5()
            md5.update(dataset.encode("utf-8"))
            hash = md5.hexdigest()[:6]

            dataset = f"{hostname}_{file}_{hash}"
        # Even non-URL datasets can be too long, for example:
        # mtdata_ELRC-convention_against_torture_other_cruel_inhuman_or_degrading_treatment_or_punishment_united_nations-1-ell-eng
        # We need to truncate and hash any that are over a certain length
        elif len(dataset) > DATASET_NAME_MAX_LENGTH:
            md5 = hashlib.md5()
            md5.update(dataset.encode("utf-8"))
            hash = md5.hexdigest()[:6]

            truncated = dataset[:DATASET_NAME_MAX_LENGTH]
            dataset = f"{truncated}_{hash}"

        return (
            dataset.replace("://", "_")
            .replace("/", "_")
            .replace(".", "_")
            .replace(":", "_")
            .replace("[", "_")
            .replace("]", "_")
        )

    def file_safe_key(self) -> str:
        return Dataset._escape(self.key)

    def file_safe_name(self) -> str:
        return Dataset._escape(self.name)


def shuffle_with_max_lines(
    line_stream: Iterator[str],
    seed: str,
    max_lines: int,
    total_byte_size: Optional[int] = None,
    estimate_total_byte_size: Optional[Callable[[float], int]] = None,
) -> list[str]:
    """
    Shuffle a line stream, but only retain up to a maximum number of lines in memory.
    Note that the final ordering is determined by the seed and the contents of the file. So
    running this multiple times on the same dataset will return the same result, but running
    it with the same seed and different content will create a different ordering.

    Only run for monolingual data or where the parallel sentences are in the same line and
    separated by a delimiter.

    The distribution should be even unless the initial content is not representative of the
    general size of the sentences, in this case the distribution will be slightly biased. See
    the test cases for more in-depth examples.

    These options are mutually exclusive, and one must be provided:
    - total_byte_size - The byte size of the lines.
    - estimate_total_byte_size - An estimate of the size of the corpus after max_lines have been
                                 filled. The average bytes per line is provided
    """
    lines: list[str] = []

    random = Random(seed)  # Make this deterministic based on dataset key.

    total_bytes = 0

    if total_byte_size is None:
        assert (
            estimate_total_byte_size
        ), "Either total_byte_size or estimate_total_byte_size must be provided"

    # Fill up the lines up until the max, and measure the total bytes.
    for line in line_stream:
        # Encoding returns the underlying byte representation which is then measured.
        total_bytes = total_bytes + len(line.encode("utf-8"))

        lines.append(line)

        if len(lines) == max_lines:
            break

    if total_byte_size is None:
        total_byte_size = estimate_total_byte_size(float(total_bytes) / float(max_lines))

    line_index = len(lines)
    random.shuffle(lines)

    # Consume the rest of the line stream, but sample based on the probability that adding
    # something to the collection will be representative.

    for i, line in enumerate(line_stream):
        # Continuously adjust this estimation in case the first sampled data is not representative.
        total_bytes = total_bytes + len(line.encode("utf-8"))
        average_bytes_per_line = total_bytes / (max_lines + i + 1)
        estimated_lines = total_byte_size / average_bytes_per_line
        line_sampling_probability = max_lines / estimated_lines

        if random.random() < line_sampling_probability:
            if len(lines) == max_lines:
                # Treat the `lines` list as a ring buffer since we've reached the max lines. As new
                # lines are randomly sampled, old randomly sampled lines roll out of the buffer.
                lines[line_index % max_lines] = line
                line_index += 1
            else:
                # Python throws "IndexError: list assignment index out of range" if you attempt
                # to assign outside the existing range, so use an append here.
                lines.append(line)

    # Do a final shuffle to ensure that the newly sampled lines are shuffled with the original
    # set of shuffled lines.
    random.shuffle(lines)

    return lines


def shuffle_in_temp_files(
    line_stream: Iterator[str],
    output: TextIOWrapper,
    seed: str,
    chunk_bytes: int,
    bucket_bytes: int,
    chunk_dir: Optional[str] = tempfile.gettempdir(),
    keep_chunks=False,
):
    """
    Shuffle large datasets by storing chunks to the file system. The ordering is guaranteed to be
    stable across two datasets as long as they are the same length. For instance it could be used
    to shuffle `dataset.en.zst` and `dataset.ca.zst` the same if the two are parallel sentences.

    Take in a stream of lines (from a download, or stdin) and split it out to chunks.

    tmpdir
    ├── chunk.1
    ├── chunk.2
    ├── chunk.3
    ├── chunk.4
    ├── ...
    └── chunk.100

    After the entire dataset is written to chunks, pick random chunks and put them into a
    bucket. Only one bucket is fully loaded into memory at a time, and the contents
    of the bucket is shuffled in memory.

    Bucket:
    ┌───────────┐
    │ chunk.85  │
    │ chunk.3   │
    │ chunk.52  │
    │ chunk.30  │
    │ chunk.12  │
    │ chunk.18  │
    └───────────┘

    • shuffle bucket lines
    • write to output

    At most 1 bucket will be held in memory. At most the dataset + 1 bucket of file space will be
    needed when running this algorithm.
    """
    random = Random(seed)

    chunk_index = 0
    chunk_file = open(os.path.join(chunk_dir, f"chunk.{chunk_index}"), "wt")

    # Write out the chunks to disk.
    bytes_written_to_chunk = 0
    for line in line_stream:
        line_bytes = len(line.encode("utf-8")) + 1

        if bytes_written_to_chunk + line_bytes > chunk_bytes:
            # Start a new chunk.
            chunk_file.close()
            chunk_index += 1
            chunk_file = open(os.path.join(chunk_dir, f"chunk.{chunk_index}"), "wt")
            bytes_written_to_chunk = 0

        chunk_file.write(line + "\n")
        bytes_written_to_chunk += line_bytes

    chunk_file.close()

    # Shuffle the chunk indexes
    chunk_count = chunk_index + 1

    shuffled_chunk_indexes = [*range(chunk_count)]
    random.shuffle(shuffled_chunk_indexes)

    # Load a single bucket into memory, discarding the chunks.
    bucket_count = 0
    bytes_in_bucket = 0
    bucket = []

    for chunk_index in shuffled_chunk_indexes:
        chunk_name = os.path.join(chunk_dir, f"chunk.{chunk_index}")

        # Read in the chunk line by line.
        with open(chunk_name, "r") as file:
            for line in file.readlines():
                bucket.append(line)
                bytes_in_bucket += len(line.encode("utf-8"))

                # If the bucket overflows, shuffle and write it out.
                if bytes_in_bucket > bucket_bytes:
                    random.shuffle(bucket)
                    for shuffled_line in bucket:
                        output.write(shuffled_line)

                    # Create the new bucket.
                    bucket = []
                    bytes_in_bucket = 0
                    bucket_count += 1

        if not keep_chunks:
            os.remove(chunk_name)

    if len(bucket) > 0:
        random.shuffle(bucket)
        for shuffled_line in bucket:
            output.write(shuffled_line)

    print(f"Shuffled with {bucket_count} buckets.")


class Statistics:
    """
    Base class for handling statistical data and JSON serialization in the pipeline. All
    public data attributes in the implementing class will be saved as JSON. This class
    standardizes how the JSON is generated, and where it is saved.

    You can derive data at JSON generation time by providing an update_derived_data method.

    For instance stats.save_json() for Statistics("nllb.en.zst") would produce "nllb.en.stats.json".
    """

    def __init__(self, dataset_path: Optional[Union[Path, str]] = None) -> None:
        self._dataset_path = Path(dataset_path) if dataset_path else None

    def save_json(self) -> Path:
        """
        Standardizes how the JSON is saved, based on the dataset.
        """
        if not self._dataset_path:
            raise Exception("A dataset_path is required when saving to JSON.")

        path = self._dataset_path.parent / f"{self._dataset_path.stem}.stats.json"
        obj = self.as_json()
        with open(path, "w", encoding="utf-8") as json_file:
            json.dump(obj, json_file, indent=2)
            json_file.write("\n")
        return path

    def _is_subclass(value: any):
        """
        Determine if a child object is a subclass or not.
        """
        try:
            return issubclass(value.__class__, Statistics)
        except AttributeError:
            return False

    def as_json(root: Union[int, str, float, list, "Statistics"]) -> Union[int, str, float, list]:
        """
        Recursively walk the data attributes of the statistics.
        """
        if Statistics._is_subclass(root):
            stats: Statistics = root
            stats.update_derived_data()
            obj = {}
            for key, value in stats.__dict__.items():
                if key.startswith("_"):
                    continue
                obj[key] = Statistics.as_json(value)

            return obj

        if isinstance(root, list):
            return [Statistics.as_json(item) for item in root]

        if isinstance(root, dict):
            root_dict: dict = root
            return {key: Statistics.as_json(value) for key, value in root_dict.items()}

        if isinstance(root, (float, int, str)):
            return root

        return str(root)

    def update_derived_data(self):
        """
        Update any derived data in the sub values. Override this method if anything
        needs to be derived.
        """
        pass


class FilteringStep(Statistics):
    """
    For each step for filtering, store how many were kept or filtered.
    """

    def __init__(
        self, description: str, filtered=0, kept=0, dataset_path: Optional[Path] = None
    ) -> None:
        super().__init__(dataset_path)
        self.description = description
        self.filtered = filtered
        self.kept = kept
        self.visited = 0

    def update_derived_data(self):
        super().update_derived_data()
        # Only two of the values need to be kept up to date, the last can be computed.
        if not self.visited:
            self.visited = self.filtered + self.kept
        elif self.filtered and not self.kept:
            self.kept = self.visited - self.filtered
            return
        elif self.kept and not self.filtered:
            self.filtered = self.visited - self.kept


@dataclass
class CountingStep(Statistics):
    """
    This is just a single value that is being counted.
    """

    value: int
    description: str

    def __init__(
        self,
        description: str,
        value=0,
        dataset_path: Optional[Path] = None,
    ) -> None:
        super().__init__(dataset_path)
        self.description = description
        self.value = value


class WeakStringSet(Set):
    """
    A Set that weakly holds on to strings by storing a hashed `int`. Using this class
    makes it easy to see if a string is duplicated across large datasets without holding
    the entire set of strings in memory.

    Usage:
        unique_strings = WeakStringSet()
        unique_strings.add("string a")
        unique_strings.add("string b")

        assert "string a" in unique_strings
        assert "string b" in unique_strings
        assert "string c" not in unique_strings
    """

    def __init__(self, iter: Optional[Iterable[str]] = None) -> None:
        if iter:
            super().__init__((WeakStringSet._hash_string(string) for string in iter))
        else:
            super().__init__()

    def __contains__(self, string: str) -> bool:
        return super().__contains__(WeakStringSet._hash_string(string))

    def add(self, string: str) -> None:
        """
        Add a string to the weak set. The strings are stored uniquely based on their
        contents with the whitespace surrounding them stripped.
        """
        super().add(WeakStringSet._hash_string(string))

    def update(self, iter: Iterable[str]):
        super().update((WeakStringSet._hash_string(string) for string in iter))

    def remove(self, string: str):
        super().remove(WeakStringSet._hash_string(string))

    def discard(self, string: str):
        super().discard(WeakStringSet._hash_string(string))

    def _hash_string(string: str) -> int:
        """
        Return a hash of a line. The line has its whitespace stripped and text representation
        normalized to ensure a consistent representation.
        """
        cleaned_line = unicodedata.normalize("NFC", string.strip())
        return hash(cleaned_line)


def decompress(
    source: Union[str, Path],
    destination: Optional[Union[Path, str]] = None,
    remove: bool = False,
    logger: Optional[Logger] = None,
) -> Path:
    """
    Decompresses a file using the appropriate command based on its file extension.

    Args:
    file_path: The path to the file to be decompressed
    remove: If set to `True`, the original compressed file will be removed after decompression.
    destination: Be default the file will be decompressed next to the original. This arguments
                 allows for overriding the destination.
    logger: Log information about the decompression
    """
    if isinstance(source, str):
        source = Path(source)
    if not destination:
        destination = source.parent / source.stem

    if logger:
        logger.info(f"[decompress] From: {source}")
        logger.info(f"[decompress] To: {destination}")

    if source.suffix == ".zst":
        command = ["zstdmt", "--decompress", "--force", "-o", destination, source]
        if remove:
            command.append("--rm")

        subprocess.check_call(command)
    elif source.suffix == ".gz":
        command = ["gzip", "-c", "-d", source]
        with open(destination, "wb") as out_file:
            subprocess.check_call(command, stdout=out_file)
        if remove:
            source.unlink()
    else:
        raise Exception(f"Unknown file type to decompress: {source}")

    if remove:
        logger.info(f"[decompress] Removed: {source}")

    return destination


def compress(
    source: Union[str, Path],
    destination: Optional[Union[Path, str]] = None,
    remove: bool = False,
    compression_type: Union[Literal["zst"], Literal["gz"]] = None,
    logger: Optional[Logger] = None,
) -> Path:
    """
    Compress a file using the appropriate command based on its file extension.

    Args:
    source:     The path to the file to be compressed
    destination: Be default the file will be compressed next to the original. This arguments
                 allows for overriding the destination.
    remove:      If set to `True`, the original decompressed file will be removed.
    type:        The type defaults to "zst", and is implied by the destination, however it can
                 be explicitly set.
    logger:      Log information about the compression
    """
    if isinstance(source, str):
        source = Path(source)
    if isinstance(destination, str):
        destination = Path(destination)

    # Ensure the compression type is valid and present
    if compression_type and destination:
        assert f".{type}" == destination.suffix, "The compression type and destination must match."

    if not compression_type:
        if destination:
            compression_type = destination.suffix[1:]
        else:
            compression_type = "zst"

    # Set default destination if not provided
    if not destination:
        destination = source.with_suffix(f"{source.suffix}.{compression_type}")

    if logger:
        logger.info(f"Compressing: {source}")
        logger.info(f"Destination: {destination}")

    if compression_type == "zst":
        command = ["zstdmt", "--compress", "--force", "--quiet", source, "-o", destination]
        if remove:
            command.append("--rm")
        subprocess.check_call(command)
    elif compression_type == "gz":
        with open(destination, "wb") as out_file:
            subprocess.check_call(["gzip", "-c", "--force", source], stdout=out_file)
        if remove:
            source.unlink()
    else:
        raise ValueError(f"Unsupported compression type: {compression_type}")

    if remove:
        logger.info(f"Removed {source}")

    return destination


common/downloads.py

import gzip
import io
import json
import os
import shutil
import time
from contextlib import ExitStack, contextmanager
from io import BufferedReader
from pathlib import Path
from typing import Any, Callable, Generator, Literal, Optional, Union
from zipfile import ZipFile

import requests
from zstandard import ZstdCompressor, ZstdDecompressor

from pipeline.common import format_bytes
from pipeline.common.logging import get_logger

logger = get_logger(__file__)


def stream_download_to_file(url: str, destination: Union[str, Path]) -> None:
    """
    Streams a download to a file, and retries several times if there are any failures. The
    destination file must not already exist.
    """
    if os.path.exists(destination):
        raise Exception(f"That file already exists: {destination}")

    logger.info(f"Destination: {destination}")

    # If this is mocked for a test, use the locally mocked path.
    mocked_location = get_mocked_downloads_file_path(url)
    if mocked_location:
        shutil.copy(mocked_location, destination)
        return

    with open(destination, "wb") as file, DownloadChunkStreamer(url) as chunk_streamer:
        for chunk in chunk_streamer.download_chunks():
            file.write(chunk)


def get_mocked_downloads_file_path(url: str) -> Optional[str]:
    """If there is a mocked download, get the path to the file, otherwise return None"""
    mocked_downloads_str = os.environ.get("MOCKED_DOWNLOADS")
    if not mocked_downloads_str:
        return None

    mocked_downloads = json.loads(mocked_downloads_str)

    if not isinstance(mocked_downloads, dict):
        raise Exception(
            "Expected the mocked downloads to be a json object mapping the URL to file path"
        )

    source_file = mocked_downloads.get(url)
    if not source_file:
        print("MOCKED_DOWNLOADS:", mocked_downloads)
        raise Exception(f"Received a URL that was not in MOCKED_DOWNLOADS {url}")

    if not os.path.exists(source_file):
        raise Exception(f"The source file specified did not exist {source_file}")

    logger.info("Mocking a download.")
    logger.info(f"   url: {url}")
    logger.info(f"  file: {source_file}")

    return source_file


def location_exists(location: str):
    """
    Checks if a location (url or file path) exists.
    """
    if get_mocked_downloads_file_path(location):
        return True

    if location.startswith("http://") or location.startswith("https://"):
        response = requests.head(location, allow_redirects=True)
        return response.ok
    return os.path.exists(location)


def attempt_mocked_request(url: str) -> Optional[BufferedReader]:
    """
    If there are mocked download, use that.
    """
    file_path = get_mocked_downloads_file_path(url)
    if file_path:
        return open(file_path, "rb")
    return None


def get_download_size(url: str) -> int:
    """Get the total bytes of a file to download."""
    mocked_file_path = get_mocked_downloads_file_path(url)
    if mocked_file_path:
        return os.path.getsize(mocked_file_path)

    response = requests.head(url, allow_redirects=True)
    size = response.headers.get("content-length", 0)
    return int(size)


class RemoteDecodingLineStreamer:
    """
    Base class to stream lines directly from a remote file.
    """

    def __init__(self, url: str) -> None:
        self.url = url

        self.decoding_stream = None
        self.byte_chunk_stream = None
        self.line_stream = None

    def __enter__(self):
        mocked_request = attempt_mocked_request(self.url)
        if mocked_request:
            # We are in a test.
            logger.info(f"Using a mocked download: {self.url}")
            self.byte_chunk_stream = mocked_request
            self.decoding_stream = self.decode(self.byte_chunk_stream)
        else:
            self.byte_chunk_stream = DownloadChunkStreamer(self.url).__enter__()
            self.decoding_stream = self.decode(self.byte_chunk_stream)

        self.line_stream = io.TextIOWrapper(self.decoding_stream, encoding="utf-8")

        return self.line_stream

    def __exit__(self, _exc_type, _exc_val, _exc_tb):
        if self.line_stream:
            self.line_stream.close()
        if self.decoding_stream:
            self.decoding_stream.close()
        if self.byte_chunk_stream:
            self.byte_chunk_stream.close()

    def decode(self, byte_stream: Any) -> Any:
        # This byte stream requires no decoding, so just pass it on through.
        return byte_stream


class RemoteGzipLineStreamer(RemoteDecodingLineStreamer):
    """
    Stream lines directly from a remote gzip file. The line includes the newlines separator.

    Usage:

        with RemoteGzipLineStreamer(url) as lines:
            for line in lines:
                print(line)
    """

    def decode(self, byte_stream):
        return gzip.GzipFile(fileobj=byte_stream)


class RemoteZstdLineStreamer(RemoteDecodingLineStreamer):
    """
    Stream lines directly from a remote zstd file. The line includes the newlines separator.

    Usage:

        with RemoteZstdLineStreamer(url) as lines:
            for line in lines:
                print(line)
    """

    def decode(self, byte_stream):
        return ZstdDecompressor().stream_reader(byte_stream)


class DownloadChunkStreamer(io.IOBase):
    """
    Streams a download as chunks, and retries several times if there are any failures. This
    clas implements io.IOBase so it can be used as a file reader.

    Iterator over chunks directly:

        with DownloadChunkStreamer(url) as chunk_streamer:
            for chunk in chunk_streamer.download_chunks():
                f.write(chunk)

    Or pass it as a file handle:

        with DownloadChunkStreamer(url) as f:
             gzip.GzipFile(fileobj=f)
    """

    def __init__(self, url: str, total_retries=3, timeout_sec=10.0, wait_before_retry_sec=60.0):
        self.url = url
        self.response = None

        # How many retry attempts should there be, and how long to wait between retries.
        self.total_retries = total_retries
        self.wait_before_retry_sec = wait_before_retry_sec

        # How long to wait for a response to timeout? This is the time that no new data is received.
        self.timeout_sec = timeout_sec

        self.report_every = 0.05  # What percentage of the download to report updates?
        self.next_report_percent = self.report_every  # The next report percentage.

        self.downloaded_bytes = 0
        self.chunk_bytes = 8 * 1024

        # The buffered `read` data.
        self.buffer = b""

        # The Generator result of _download_chunks.
        self.chunk_iter: Optional[Generator[bytes, None, None]] = None

    def __enter__(self):
        """
        On enter, kick off the download, and store the chunk iterator. This iterator
        handles the restarts for Requests.
        """
        self.chunk_iter = self.download_chunks()

        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.close()

    def close(self):
        """
        Close out the response, and cancel any iterators.
        """
        if self.response:
            self.response.close()

        self.response = None
        self.chunk_iter = None

    def read(self, size=-1) -> bytes:
        """
        This method implements the io.IOBase read method. It buffers the chunks until the `size`
        requirement is fulfilled. It is backed by the chunks_iter created by the download_chunks
        method.
        """
        if not self.chunk_iter:
            # The chunk iterator was consumed. Return an empty byte object to indicate the download
            # is complete.
            return b""

        if size < 0:
            # Load everything into the buffer, and return it.
            for chunk in self.chunk_iter:
                self.buffer += chunk
            result = self.buffer
            self.buffer = b""
            return result

        # Load the buffer with requested amount of data to read. -1 indicates load everything.
        while len(self.buffer) < size:
            chunk = next(self.chunk_iter, None)
            if chunk:
                self.buffer += chunk
            else:
                # The stream ended.
                break

        # Return the requested read amount, and divide up the remaining buffer.
        result = self.buffer[:size]
        self.buffer = self.buffer[size:]

        return result

    def readable(self):
        return True

    def download_chunks(self) -> Generator[bytes, None, None]:
        """
        This method is the generator that is responsible for running the request, and retrying
        when there is a failure. It yields the fixed size byte chunks, and exposes a generator
        to be consumed. This generator can be used directly in a for loop, or the entire class
        can be passed in as a file handle.
        """
        next_report_percent = self.report_every
        total_bytes = 0

        for retry in range(self.total_retries):
            if retry > 0:
                logger.error(f"Remaining retries: {self.total_retries - retry}")

            try:
                headers = {}
                if self.downloaded_bytes > 0:
                    # Pick up the download from where it was before.
                    headers = {"Range": f"bytes={self.downloaded_bytes}-"}

                self.response = requests.get(
                    self.url, headers=headers, stream=True, timeout=self.timeout_sec
                )
                self.response.raise_for_status()

                # Report the download size.
                if not total_bytes and "content-length" in self.response.headers:
                    total_bytes = int(self.response.headers["content-length"])
                    logger.info(f"Download size: {total_bytes:,} bytes")

                for chunk in self.response.iter_content(chunk_size=self.chunk_bytes):
                    if not chunk:
                        continue

                    self.downloaded_bytes += len(chunk)

                    # Report the percentage downloaded every `report_every` percentage.
                    if total_bytes and self.downloaded_bytes >= next_report_percent * total_bytes:
                        logger.info(
                            f"{self.downloaded_bytes / total_bytes * 100.0:.0f}% downloaded "
                            f"({self.downloaded_bytes}/{total_bytes} bytes)"
                        )
                        next_report_percent += self.report_every

                    yield chunk

                # The download is complete.
                self.close()
                logger.info("100% downloaded - Download finished.")
                return

            except requests.exceptions.Timeout as error:
                logger.error(f"The connection timed out: {error}.")

            except requests.exceptions.RequestException as error:
                # The RequestException is the generic error that catches all classes of "requests"
                # errors. Don't attempt to be be smart about this, just attempt again until
                # the retries are done.
                logger.error(f"A download error occurred: {error}")

            # Close out the response on an error. It will be recreated when retrying.
            if self.response:
                self.response.close()
                self.response = None

            logger.info(f"Retrying in {self.wait_before_retry_sec} sec")
            time.sleep(self.wait_before_retry_sec)

        self.close()
        raise Exception("The download failed.")

    def decode(self, byte_stream) -> Generator[bytes, None, None]:
        """Pass through the byte stream. This method can be specialized by child classes."""
        return byte_stream


@contextmanager
def _read_lines_multiple_files(
    files: list[Union[str, Path]],
    encoding: str,
    path_in_archive: Optional[str],
    on_enter_location: Optional[Callable[[str], None]] = None,
) -> Generator[Generator[str, None, None], None, None]:
    """
    Iterates through each line in multiple files, combining it into a single stream.
    """

    stack = None

    def iter(stack: ExitStack):
        for file_path in files:
            logger.info(f"Reading lines from: {file_path}")
            lines = stack.enter_context(
                read_lines(file_path, path_in_archive, on_enter_location, encoding=encoding)
            )
            yield from lines
            stack.close()

    try:
        stack = ExitStack()
        yield iter(stack)
    finally:
        if stack:
            stack.close()


@contextmanager
def _read_lines_single_file(
    location: Path | str,
    encoding: str,
    path_in_archive: Optional[str] = None,
    on_enter_location: Optional[Callable[[str], None]] = None,
) -> Generator[Generator[str, None, None], None, None]:
    """
    A smart function to efficiently stream lines from a local or remote file.
    The location can either be a URL or a local file system path.
    It handles gzip, zst, and plain text files.

    Args:
        location - URL or file path
        path_in_archive  - The path to a file in a zip archive
        on_enter_location - A lambda for when a new location is entered
    """
    location = str(location)
    if on_enter_location:
        on_enter_location(location)

    if location.startswith("http://") or location.startswith("https://"):
        # If this is mocked for a test, use the locally mocked path.
        mocked_location = get_mocked_downloads_file_path(location)
        if mocked_location:
            location = mocked_location

    stack = ExitStack()

    try:
        if location.startswith("http://") or location.startswith("https://"):
            # This is a remote file.

            response = requests.head(location, allow_redirects=True)
            content_type = response.headers.get("Content-Type")
            if content_type == "application/gzip":
                yield stack.enter_context(RemoteGzipLineStreamer(location))  # type: ignore[reportReturnType]

            elif content_type == "application/zstd":
                yield stack.enter_context(RemoteZstdLineStreamer(location))  # type: ignore[reportReturnType]

            elif content_type == "application/zip":
                raise Exception("Streaming a zip from a remote location is supported.")

            elif content_type == "text/plain":
                yield stack.enter_context(RemoteDecodingLineStreamer(location))  # type: ignore[reportReturnType]

            elif location.endswith(".gz") or location.endswith(".gzip"):
                yield stack.enter_context(RemoteGzipLineStreamer(location))  # type: ignore[reportReturnType]

            elif location.endswith(".zst"):
                yield stack.enter_context(RemoteZstdLineStreamer(location))  # type: ignore[reportReturnType]
            else:
                # Treat as plain text.
                yield stack.enter_context(RemoteDecodingLineStreamer(location))  # type: ignore[reportReturnType]

        else:  # noqa: PLR5501
            # This is a local file.
            if location.endswith(".gz") or location.endswith(".gzip"):
                yield stack.enter_context(gzip.open(location, "rt", encoding=encoding))  # type: ignore[reportReturnType]

            elif location.endswith(".zst"):
                input_file = stack.enter_context(open(location, "rb"))
                zst_reader = stack.enter_context(ZstdDecompressor().stream_reader(input_file))
                yield stack.enter_context(io.TextIOWrapper(zst_reader, encoding=encoding))  # type: ignore[reportReturnType]

            elif location.endswith(".zip"):
                if not path_in_archive:
                    raise Exception("Expected a path into the zip file.")
                zip = stack.enter_context(ZipFile(location, "r"))
                if path_in_archive not in zip.namelist():
                    raise Exception(f"Path did not exist in the zip file: {path_in_archive}")
                file = stack.enter_context(zip.open(path_in_archive, "r"))
                yield stack.enter_context(io.TextIOWrapper(file, encoding=encoding))  # type: ignore[reportReturnType]
            else:
                # Treat as plain text.
                yield stack.enter_context(open(location, "rt", encoding=encoding))  # type: ignore[reportReturnType]
    finally:
        stack.close()


def read_lines(
    location_or_locations: Union[Path, str, list[Union[str, Path]]],
    path_in_archive: Optional[str] = None,
    on_enter_location: Optional[Callable[[str], None]] = None,
    encoding="utf-8",
):
    """
    A smart function to efficiently stream lines from a local or remote file.
    The location can either be a URL or a local file system path.
    It handles gzip, zst, and plain text files.
    It can also handle a list of files.

    Args:
        location_or_locations - A single URL or file path, or a list
        path_in_archive  - The path to a file in a zip archive

    Usage:
        with read_lines("output.txt.gz") as lines:
            for line in lines:
                print(line)

        paths = [
            "http://example.com/file.txt.gz",
            "path/to/file.txt.zst",
        ]
        with read_lines(paths) as lines:
            for line in lines:
                print(line)
    """

    if isinstance(location_or_locations, list):
        return _read_lines_multiple_files(
            location_or_locations, encoding, path_in_archive, on_enter_location
        )

    return _read_lines_single_file(
        location_or_locations, encoding, path_in_archive, on_enter_location
    )


@contextmanager
def write_lines(path: Path | str, encoding="utf-8"):
    """
    A smart function to create a context to write lines to a file. It works on .zst, .gz, and
    raw text files. It reads the extension to determine the file type. If writing out a raw
    text file, for instance a sample of a dataset that is just used for viewing, include a
    "byte order mark" so that the browser can properly detect the encoding.

    with write_lines("output.txt.gz") as output:
        output.write("writing a line\n")
        output.write("writing a second lines\n")
    """

    stack = None
    try:
        path = str(path)
        stack = ExitStack()

        if path.endswith(".zst"):
            file = stack.enter_context(open(path, "wb"))
            compressor = stack.enter_context(ZstdCompressor().stream_writer(file))
            yield stack.enter_context(io.TextIOWrapper(compressor, encoding=encoding))
        elif path.endswith(".gz"):
            yield stack.enter_context(gzip.open(path, "wt", encoding=encoding))
        else:
            yield stack.enter_context(open(path, "wt", encoding=encoding))

    finally:
        if stack:
            stack.close()


def count_lines(path: Path | str) -> int:
    """
    Similar to wc -l, this counts the lines in a file. However, this command does so regardless
    of the compression strategy used on the file.
    """
    with read_lines(path) as lines:
        return sum(1 for _ in lines)


def is_file_empty(path: Path | str) -> bool:
    """
    Attempts to read a line to determine if a file is empty or not. Works on local or remote files
    as well as compressed or uncompressed files.
    """
    with read_lines(path) as lines:
        try:
            next(lines)
            return False
        except StopIteration:
            return True


def get_file_size(location: Path | str) -> int:
    """Get the size of a file, whether it is remote or local."""
    if isinstance(location, str) and (
        location.startswith("http://") or location.startswith("https://")
    ):
        return get_download_size(location)
    return os.path.getsize(location)


def get_human_readable_file_size(location: Path | str) -> tuple[str, int]:
    """Get the size of a file in a human-readable string, and the numeric bytes."""
    bytes = get_file_size(location)
    return format_bytes(bytes), bytes


def compress_file(
    path: Union[str, Path], keep_original: bool = True, compression: Literal["zst", "gz"] = "zst"
) -> Path:
    """
    Compresses a file to .zst or .gz format. It returns the path of the compressed file.
    "zst" is the preferred compression scheme.
    """
    path = Path(path)

    if compression == "zst":
        compressed_path = Path(str(path) + ".zst")
        cctx = ZstdCompressor()
        with open(path, "rb") as infile:
            with open(compressed_path, "wb") as outfile:
                outfile.write(cctx.compress(infile.read()))

    elif compression == "gz":
        compressed_path = Path(str(path) + ".gz")
        with open(path, "rb") as infile:
            with gzip.open(compressed_path, "wb") as outfile:
                outfile.write(infile.read())

    else:
        raise ValueError(f"Unsupported compression format: {compression}")

    if not keep_original:
        # Delete the original file
        path.unlink()

    return compressed_path


def decompress_file(
    path: Union[str, Path],
    keep_original: bool = True,
    decompressed_path: Optional[Union[str, Path]] = None,
) -> Path:
    """
    Decompresses a .gz or .zst file. It returns the path of the decompressed file.
    """
    path = Path(path)

    if decompressed_path:
        decompressed_path = Path(decompressed_path)
    else:
        # Remove the original suffix
        decompressed_path = path.with_suffix("")

    with ExitStack() as stack:
        decompressed_file = stack.enter_context(decompressed_path.open("wb"))

        if path.suffix == ".gz":
            compressed_file = stack.enter_context(gzip.open(str(path), "rb"))
            decompressed_file.write(compressed_file.read())
            while True:
                # Write the data out in chunks so that all of the it doesn't need to be
                # into memory.
                chunk = compressed_file.read(10_240)
                if not chunk:
                    break
                decompressed_file.write(chunk)

        elif path.suffix == ".zst":
            compressed_file = stack.enter_context(open(path, "rb"))
            for chunk in ZstdDecompressor().read_to_iter(compressed_file):
                # Write the data out in chunks so that all of the it doesn't need to be
                # into memory.
                decompressed_file.write(chunk)
        else:
            raise ValueError(f"Unsupported file extension: {path.suffix}")

    if not keep_original:
        # Delete the original file
        path.unlink()

    return decompressed_path


common/logging.py

import logging
from pathlib import Path
import subprocess
import threading
import time

logging.basicConfig(level=logging.INFO, format="[%(name)s] %(message)s")

STOP_BYTE_COUNT_LOGGER = False
STOP_GPU_LOGGER = False


def get_logger(name: str):
    """
    Get a logger using the __file__ name.

    For example in pipeline/bicleaner/download_pack.py

        logger = get_logger(__file__)
        logger.info("This is a log.")

    Will log:

        > [download_pack] This is a log.
    """

    logger = logging.getLogger(Path(name).stem)
    logger.setLevel(logging.INFO)
    return logger


def _log_gpu_stats(logger: logging.Logger, interval_seconds: int):
    # Only load gpustat when it's needed.
    import gpustat

    global STOP_GPU_LOGGER
    while True:
        time.sleep(interval_seconds)
        if STOP_GPU_LOGGER:
            STOP_GPU_LOGGER = False
            return
        try:
            logger.info("[gpu] Current GPU stats:")
            gpustat.print_gpustat()
        except subprocess.CalledProcessError as e:
            logger.error(f"Failed to retrieve GPU stats: {e}")


def stop_gpu_logging():
    global STOP_GPU_LOGGER
    STOP_GPU_LOGGER = True


def start_gpu_logging(logger: logging.Logger, interval_seconds: int):
    """Logs GPU stats on an interval using gpustat in a background thread."""
    assert not STOP_GPU_LOGGER, "A gpu logger should not already be running"

    thread = threading.Thread(
        target=_log_gpu_stats,
        # Set as a daemon thread so it automatically is closed on shutdown.
        daemon=True,
        args=(logger, interval_seconds),
    )
    thread.start()


def _log_byte_rate(logger: logging.Logger, interval_seconds: int, file_path: Path):
    global STOP_BYTE_COUNT_LOGGER
    previous_byte_count = 0
    previous_time = time.time()
    is_zst = file_path.suffix == ".zst"

    while True:
        time.sleep(interval_seconds)
        if STOP_BYTE_COUNT_LOGGER:
            STOP_BYTE_COUNT_LOGGER = False
            return

        try:
            if is_zst:
                # This takes ~1 second to run on 5 million sentences.
                current_byte_count = 0
                cmd = ["zstd", "-dc", str(file_path)]
                with subprocess.Popen(
                    cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE
                ) as process:
                    for chunk in iter(lambda: process.stdout.read(8192), b""):
                        current_byte_count += len(chunk)
            else:
                # This is pretty much instantaneous.
                result = subprocess.run(
                    ["wc", "-c", str(file_path)], capture_output=True, text=True, check=True
                )
                current_byte_count = int(result.stdout.split()[0])

            bytes_added = current_byte_count - previous_byte_count

            current_secs = time.time()
            elapsed_secs = current_secs - previous_time
            byte_rate = bytes_added / elapsed_secs if bytes_added > 0 else 0

            logger.info(f"[bytes] Added: {bytes_added:,}")
            logger.info(f"[bytes] Total: {current_byte_count:,}")
            logger.info(f"[bytes] Rate: {byte_rate:,.2f} bytes/second")

            previous_byte_count = current_byte_count
            previous_time = time.time()
        except Exception as e:
            logger.error(f"Failed to monitor byte count: {e}")


def stop_byte_count_logger():
    global STOP_BYTE_COUNT_LOGGER
    STOP_BYTE_COUNT_LOGGER = True


def start_byte_count_logger(logger: logging.Logger, interval_seconds: int, file_path: Path):
    """
    Monitors the rate of bytes being added to a file, logging the number of bytes
    added per second over the interval.
    """

    assert not STOP_BYTE_COUNT_LOGGER, "A line count logger should not already be running"
    thread = threading.Thread(
        target=_log_byte_rate, args=(logger, interval_seconds, file_path), daemon=True
    )
    thread.start()


common/marian.py

"""
Common utilities related to working with Marian.
"""

from pathlib import Path
from typing import Union

import yaml


def get_combined_config(config_path: Path, extra_marian_args: list[str]) -> dict[str, any]:
    """
    Frequently we combine a Marian yml config with extra marian args when running
    training. To get the final value, add both here.
    """
    return {
        **yaml.safe_load(config_path.open()),
        **marian_args_to_dict(extra_marian_args),
    }


def marian_args_to_dict(extra_marian_args: list[str]) -> dict[str, Union[str, bool, list[str]]]:
    """
    Converts marian args, to the dict format. This will combine a decoder.yml
    and extra marian args.

    e.g. `--precision float16` becomes {"precision": "float16"}
    """
    decoder_config = {}
    if extra_marian_args and extra_marian_args[0] == "--":
        extra_marian_args = extra_marian_args[1:]

    previous_key = None
    for arg in extra_marian_args:
        if arg.startswith("--"):
            previous_key = arg[2:]
            decoder_config[previous_key] = True
            continue

        if not previous_key:
            raise Exception(
                f"Expected to have a previous key when converting marian args to a dict: {extra_marian_args}"
            )

        prev_value = decoder_config.get(previous_key)
        if prev_value is True:
            decoder_config[previous_key] = arg
        elif isinstance(prev_value, list):
            prev_value.append(arg)
        else:
            decoder_config[previous_key] = [prev_value, arg]

    return decoder_config


common/memory.py

import gc
import logging
import os
from typing import Optional

import psutil

from pipeline.common import format_bytes
from pipeline.common.logging import get_logger

_memory_logger: Optional[logging.Logger] = None
_memory_process: Optional[psutil.Process] = None
_memory_last_bytes: Optional[int] = None


def get_memory_string() -> str:
    """
    Get the current string representation of the memory usage.
    """
    global _memory_process
    global _memory_last_bytes

    # Lazily initial everything.
    if not _memory_process:
        _memory_process = psutil.Process(os.getpid())

    memory_info = _memory_process.memory_info()
    bytes = memory_info.rss

    if _memory_last_bytes:
        bytes_diff = bytes - _memory_last_bytes
        sign = ""
        if bytes_diff >= 0:
            sign = "+"
        string = f"{format_bytes(bytes)} ({sign}{format_bytes(bytes_diff)})"
    else:
        string = format_bytes(bytes)

    _memory_last_bytes = bytes

    return string


def log_memory(gc_collect=False) -> None:
    """
    Logs the memory usage of the current Python process.

    Args:
    - gc_collect - Perform a GC before measuring memory.
    """
    global _memory_logger
    if not _memory_logger:
        _memory_logger = get_logger("memory")

    if gc_collect:
        gc.collect()

    _memory_logger.info(get_memory_string())


continuation/corpus.py

"""
Continue the training pipeline with existing corpora.
"""
import argparse
from enum import Enum
from pathlib import Path
from pipeline.common.downloads import stream_download_to_file
from pipeline.common.logging import get_logger
from pipeline.common import arg_utils

logger = get_logger(__file__)


class Corpus(Enum):
    backtranslations = "backtranslations"
    original_parallel = "original-parallel"
    student_distillation = "student-distillation"

    def __str__(self):
        # Support for argparse choices.
        return self.name


def main() -> None:
    parser = argparse.ArgumentParser(
        description=__doc__,
        formatter_class=argparse.RawTextHelpFormatter,
    )
    parser.add_argument(
        "--src_locale", type=str, required=True, help="The source language for this corpus"
    )
    parser.add_argument(
        "--trg_locale", type=str, required=True, help="The target language for this file"
    )
    parser.add_argument("--corpus", type=Corpus, required=True, choices=list(Corpus))
    parser.add_argument(
        "--src_url", type=str, required=True, help="The source URL for this corpus"
    )
    parser.add_argument(
        "--trg_url", type=str, required=True, help="The target URL for this corpus"
    )
    parser.add_argument(
        "--tok_src_url",
        type=str,
        default="",
        help="The (optional) source URL for the tokenized corpus",
    )
    parser.add_argument(
        "--tok_trg_url",
        type=str,
        default="",
        help="The (optional) target URL for the tokenized corpus",
    )
    parser.add_argument(
        "--alignments_url",
        type=str,
        default="",
        help="The (optional) alignments file URL for this file",
    )
    parser.add_argument(
        "--artifacts", type=Path, help="The location where the dataset will be saved"
    )

    args = parser.parse_args()

    src_locale = arg_utils.ensure_string("--src_locale", args.src_locale)
    trg_locale = arg_utils.ensure_string("--trg_locale", args.trg_locale)
    corpus: Corpus = args.corpus
    src_url = arg_utils.ensure_string("--src_url", args.src_url)
    trg_url = arg_utils.ensure_string("--trg_url", args.trg_url)
    tok_src_url = arg_utils.handle_none_value(args.tok_src_url)
    tok_trg_url = arg_utils.handle_none_value(args.tok_trg_url)
    alignments_url = arg_utils.handle_none_value(args.alignments_url)

    artifacts: Path = args.artifacts

    if corpus == Corpus.backtranslations:
        file_name_part = "mono"
    elif corpus == Corpus.original_parallel:
        file_name_part = "corpus"
    elif corpus == Corpus.student_distillation:
        file_name_part = "corpus"
    else:
        raise ValueError(f'Unexpected corpus name: "{corpus}"')

    artifacts.mkdir(exist_ok=True)
    src_destination = artifacts / f"{file_name_part}.{src_locale}.zst"
    trg_destination = artifacts / f"{file_name_part}.{trg_locale}.zst"

    stream_download_to_file(src_url, src_destination)
    stream_download_to_file(trg_url, trg_destination)

    if tok_src_url or tok_trg_url or alignments_url:
        assert (
            tok_src_url and tok_trg_url and alignments_url
        ), "All three URLs must be provided for the tokenized corpus."

        alignments_destination = artifacts / f"{file_name_part}.aln.zst"
        tok_src_destination = artifacts / f"{file_name_part}.tok-icu.{src_locale}.zst"
        tok_trg_destination = artifacts / f"{file_name_part}.tok-icu.{trg_locale}.zst"
        stream_download_to_file(alignments_url, alignments_destination)
        stream_download_to_file(tok_src_url, tok_src_destination)
        stream_download_to_file(tok_trg_url, tok_trg_destination)


if __name__ == "__main__":
    main()


continuation/model.py

"""
Continue the training pipeline with an existing model.
"""
import argparse
from pathlib import Path
from pipeline.common.downloads import stream_download_to_file, location_exists
from pipeline.common.logging import get_logger
from pipeline.common import arg_utils

logger = get_logger(__file__)

potential_models = (
    "model.npz.best-chrf.npz",
    "model.npz",
    "model.npz.best-bleu-detok.npz",
    "model.npz.best-ce-mean-words.npz",
    "final.model.npz.best-chrf.npz",
    "model.npz.optimizer.npz",
)

potential_decoders = [
    "model.npz.best-chrf.npz.decoder.yml",
    "model.npz.best-bleu-detok.npz.decoder.yml",
    "model.npz.best-ce-mean-words.npz.decoder.yml",
    "final.model.npz.best-chrf.npz.decoder.yml",
    "model.npz.decoder.yml",
    "model.npz.progress.yml",
    "model.npz.yml",
]


def main() -> None:
    parser = argparse.ArgumentParser(
        description=__doc__,
        formatter_class=argparse.RawTextHelpFormatter,
    )
    parser.add_argument(
        "--src_locale", type=str, required=True, help="The source language for this model"
    )
    parser.add_argument(
        "--trg_locale", type=str, required=True, help="The target language for this model"
    )
    parser.add_argument("--url_prefix", type=str, required=True, help="The prefix for the URLs")
    parser.add_argument("--vocab_src", type=str, help="The source vocab file")
    parser.add_argument(
        "--vocab_trg", type=str, help="The target vocab file, potentially the same as the source"
    )
    parser.add_argument(
        "--best_model",
        type=str,
        required=True,
        help="The metric used to determine the best model, e.g. chrf, bleu, etc.",
    )
    parser.add_argument(
        "--artifacts", type=Path, help="The location where the models will be saved"
    )

    args = parser.parse_args()

    src_locale = arg_utils.ensure_string("--src_locale", args.src_locale)
    trg_locale = arg_utils.ensure_string("--trg_locale", args.trg_locale)
    url_prefix = arg_utils.ensure_string("--url_prefix", args.url_prefix)
    best_model = arg_utils.ensure_string("--best_model", args.best_model)
    src_vocab_url = arg_utils.handle_none_value(args.vocab_src)
    trg_vocab_url = arg_utils.handle_none_value(args.vocab_trg)
    artifacts: Path = args.artifacts

    assert artifacts

    model_out = artifacts / f"final.model.npz.{best_model}.npz"
    decoder_out = artifacts / f"final.model.npz.{best_model}.npz.decoder.yml"

    model_found = False
    for potential_model in potential_models:
        url = f"{url_prefix}/{potential_model}"
        logger.info(f"Checking to see if a model exists: {potential_model}")
        if location_exists(url):
            logger.info(f"Downloading it to: {model_out}")
            stream_download_to_file(url, model_out)
            model_found = True
            break
    assert model_found

    decoder_found = False
    for potential_decoder in potential_decoders:
        url = f"{url_prefix}/{potential_decoder}"
        logger.info(f"Checking to see if a decoder.yml exists: {potential_decoder}")
        if location_exists(url):
            logger.info(f"Downloading it to: {decoder_out}")
            stream_download_to_file(url, decoder_out)
            decoder_found = True
            break
    assert decoder_found

    # Prefer the vocab near the model.
    if not src_vocab_url or not trg_vocab_url:
        if location_exists(f"{url_prefix}/vocab.spm"):
            logger.info(f"A single vocab was found: {url_prefix}/vocab.spm")
            src_vocab_url = f"{url_prefix}/vocab.spm"
            trg_vocab_url = f"{url_prefix}/vocab.spm"
        elif location_exists(f"{url_prefix}/vocab.{src_locale}.spm") and location_exists(
            f"{url_prefix}/vocab.{trg_locale}.spm"
        ):
            logger.info(f"A split vocab was found: {url_prefix}/vocab.[locale].spm")
            src_vocab_url = f"{url_prefix}/vocab.{src_locale}.spm"
            trg_vocab_url = f"{url_prefix}/vocab.{trg_locale}.spm"
        else:
            raise ValueError(
                "Attempted to run model continuation but no vocab was found or provided. "
                'Add one to the config at "continuation.vocab.src" and '
                ' "continuation.vocab.trg"'
            )

    # TODO - Change to the other "if" branch when split vocab lands:
    # See: https://github.com/mozilla/translations/pull/1051
    if True:
        assert src_vocab_url == trg_vocab_url, "Split vocab is not supported yet."
        stream_download_to_file(src_vocab_url, artifacts / "vocab.spm")
    else:
        stream_download_to_file(src_vocab_url, f"vocab.{src_locale}.spm")
        stream_download_to_file(trg_vocab_url, f"vocab.{trg_locale}.spm")


if __name__ == "__main__":
    main()


continuation/requirements/continuation.in

zstandard==0.23.0
requests==2.31.0


continuation/vocab.py

"""
Continue the training pipeline with an existing vocab.
"""
import argparse
from pathlib import Path
from pipeline.common.downloads import stream_download_to_file
from pipeline.common.logging import get_logger
from pipeline.common import arg_utils

logger = get_logger(__file__)


def main() -> None:
    parser = argparse.ArgumentParser(
        description=__doc__,
        formatter_class=argparse.RawTextHelpFormatter,
    )
    parser.add_argument(
        "--src_locale", type=str, required=True, help="The source language for this vocab"
    )
    parser.add_argument(
        "--trg_locale", type=str, required=True, help="The target language for this vocab"
    )
    parser.add_argument("--src_url", type=str, required=True, help="The source URL for this vocab")
    parser.add_argument(
        "--trg_url",
        type=str,
        required=True,
        help="The target URL for this vocab, potentially the same as the source.",
    )
    parser.add_argument(
        "--artifacts", type=Path, help="The location where the vocab will be saved"
    )

    args = parser.parse_args()

    src_locale = arg_utils.ensure_string("--src_locale", args.src_locale)
    trg_locale = arg_utils.ensure_string("--trg_locale", args.trg_locale)
    src_url = arg_utils.ensure_string("--src_url", args.src_url)
    trg_url = arg_utils.ensure_string("--trg_url", args.trg_url)
    artifacts: Path = args.artifacts

    artifacts.mkdir(exist_ok=True)

    # TODO - Change to the other "if" branch when split vocab lands:
    # See: https://github.com/mozilla/translations/pull/1051
    if True:
        assert src_url == trg_url, "Split vocab is not supported yet."
        stream_download_to_file(src_url, artifacts / "vocab.spm")
    else:
        stream_download_to_file(src_url, f"vocab.{src_locale}.spm")
        stream_download_to_file(trg_url, f"vocab.{trg_locale}.spm")


if __name__ == "__main__":
    main()


data/analyze.py

#!/usr/bin/env python3

"""
Get the statistical distribution of a dataset.

Usage:

    python3 pipeline/data/analyze.py \
        --file_location data.en.zst
        --output ./artifacts
        --dataset "opus_NLLB/v1"
        --language en

For parallel corpora, add the arguments twice, separated by a `--`.
"""

import argparse
import gzip
import os
import sys
from typing import Optional

import matplotlib.pyplot as plt
import numpy as np
import zstandard
from matplotlib import ticker

# Ensure the pipeline is available on the path.
sys.path.append(os.path.join(os.path.dirname(os.path.abspath(__file__)), "../.."))

from pipeline.common.datasets import Dataset
from pipeline.common.downloads import (
    RemoteGzipLineStreamer,
    RemoteZstdLineStreamer,
)
from pipeline.common.logging import get_logger

logger = get_logger(__file__)


def get_line_streamer(file_location: str):
    """Streams in lines from remote locations, or from disk. Accepts zst, gz, and plain text."""
    if file_location.startswith("http://") or file_location.startswith("https://"):
        if file_location.endswith(".zst"):
            return RemoteZstdLineStreamer(file_location)
        # Assume gzip.
        return RemoteGzipLineStreamer(file_location)

    if file_location.endswith(".gz"):
        return gzip.open(file_location, "rt")
    if file_location.endswith(".zst"):
        return zstandard.open(file_location, "rt")
    return open(file_location, "rt")


def main(args: Optional[list[str]] = None) -> None:
    parser = argparse.ArgumentParser(
        description=__doc__,
        formatter_class=argparse.RawTextHelpFormatter,  # Preserves whitespace in the help text.
    )
    parser.add_argument(
        "--file_location", type=str, required=True, help="A url or file path for analyzing."
    )
    parser.add_argument(
        "--output_dir", type=str, required=True, help="The directory for the output."
    )
    parser.add_argument("--dataset", type=str, required=True, help="The name of the dataset")
    parser.add_argument(
        "--language",
        type=str,
        required=True,
        help="The dataset language, as a BCP-47 language tag",
    )
    # All the use of "--" to add more arguments.
    parser.add_argument("next_dataset_args", nargs=argparse.REMAINDER)

    parsed_args = parser.parse_args(args)

    # Defer parsing any options after "--", and recurse below if there are some.
    next_dataset_args: Optional[list[str]] = None
    if len(parsed_args.next_dataset_args):
        if parsed_args.next_dataset_args[0] != "--":
            print(next_dataset_args)
            raise Exception("Unexpected arguments. Use -- to pass in multiple datasets.")
        next_dataset_args = parsed_args.next_dataset_args[1:]

    logger.info(f"file_location: {parsed_args.file_location}")
    logger.info(f"output_dir: {parsed_args.output_dir}")
    logger.info(f"dataset: {parsed_args.dataset}")
    logger.info(f"language: {parsed_args.language}")

    dataset = Dataset(parsed_args.dataset)
    graph_prefix = f"{dataset.file_safe_name()}.{parsed_args.language}"

    # Compute the distributions for both the codepoints, and word size.
    codepoints_distribution = Histogram()
    word_distribution = Histogram()
    with get_line_streamer(parsed_args.file_location) as lines:
        for line in lines:
            codepoints_distribution.count(len(line))
            word_distribution.count(len(line.split()))

    plot_logarithmic_histogram(
        word_distribution,
        max_size=5_000,  # words
        title="\n".join(
            [
                "Word Count Distribution",
                f"{parsed_args.dataset} - {parsed_args.language}",
            ]
        ),
        x_axis_label="Words (log scale)",
        filename=os.path.join(parsed_args.output_dir, f"{graph_prefix}.distribution-words.png"),
    )

    plot_logarithmic_histogram(
        codepoints_distribution,
        max_size=10_000,  # codepoints
        title="\n".join(
            [
                "Codepoints per Sentence Distribution",
                f"{parsed_args.dataset} - {parsed_args.language}",
            ]
        ),
        x_axis_label="Codepoints (log scale)",
        filename=os.path.join(
            parsed_args.output_dir, f"{graph_prefix}.distribution-codepoints.png"
        ),
    )

    if next_dataset_args:
        # Apply the arguments again after "--".
        main(next_dataset_args)


class Histogram:
    """Computes a histogram based on counts."""

    def __init__(self) -> None:
        # The keys are the bins, the values are the counts.
        self.data: dict[int, int] = {}

    def count(self, count: int):
        if count not in self.data:
            self.data[count] = 0
        self.data[count] += 1

    def log_scale_bins(self, max_size: int, bin_count: int = 30) -> list[int]:
        """Converts the linear bins of the histogram into into logscale bins."""
        # Start with a few small value bins, since it's easy to start with some small fractional
        # values on a log scale.
        bins = [1.0, 2.0]
        for edge in np.logspace(np.log10(1), np.log10(max_size), bin_count):
            if edge > 2.0:
                bins.append(edge)
        return bins


def plot_logarithmic_histogram(
    histogram: Histogram, max_size: int, title: str, x_axis_label: str, filename: str
):
    """
    Converts a histogram of values into a logscale graph, where the x axis is logarithmic,
    and the y scale is linear. The x axis represents the bins of the histogram.
    """

    bins = np.array(histogram.log_scale_bins(max_size))

    # Plot a histogram with logarithmic bins.
    plt.title(title)
    plt.hist(histogram.data.keys(), bins=bins, weights=histogram.data.values(), alpha=0.7)

    plt.xlabel(x_axis_label)
    plt.xscale("log")
    plt.xticks(ticks=bins, labels=[f"{int(edge)}" for edge in bins], rotation="vertical")

    plt.ylabel("Frequency (linear)")
    plt.yscale("linear")
    plt.gca().yaxis.set_major_formatter(ticker.StrMethodFormatter("{x:,.0f}"))

    # Ensure no labels are cut off.
    plt.tight_layout()

    logger.info(f"Saving plot to: {filename}")
    plt.savefig(filename, dpi=150)
    plt.close()


if __name__ == "__main__":
    main()


data/cjk.py

"""
Chinese, Japanese, Korean (CJK) specific data importing code
"""
from enum import Flag
from pathlib import Path
from typing import Optional

import hanzidentifier
import opencc

from pipeline.common.datasets import Statistics
from pipeline.common.downloads import read_lines, write_lines


CJK_LANGS = ["zh", "ja", "ko"]


class ChineseType(Flag):
    none = 0
    simplified = 1
    traditional = 2


class ConversionStep(Statistics):
    """
    When converting data, count how many sentences were converted, and how many were visited.
    """

    def __init__(self, description: str, converted=0, dataset_path: Optional[Path] = None) -> None:
        super().__init__(dataset_path)
        self.description = description
        self.converted = converted
        self.visited = 0


class DatasetStatistics(Statistics):
    def __init__(self, dataset_path: Path, script: ChineseType) -> None:
        super().__init__(dataset_path)
        self.script = script
        self.script_conversion = ConversionStep(
            f"How many sentences in the dataset were converted to {script.name}",
        )


class ChineseConverter:
    def __init__(self):
        self.s2t = opencc.OpenCC("s2t.json")
        self.t2s = opencc.OpenCC("t2s.json")

    def convert_file(
        self, input_path: Path, output_path: Path, to: ChineseType
    ) -> DatasetStatistics:
        stats = DatasetStatistics(output_path, to)
        with write_lines(output_path) as out_file, read_lines(input_path) as lines:
            for line in lines:
                stats.script_conversion.visited += 1
                ch_type = self._detect(line)
                if ch_type in (ch_type.none, to):
                    new_line = line
                else:
                    new_line = self._convert_line(line, to)
                    stats.script_conversion.converted += 1
                out_file.write(new_line)
        return stats

    @staticmethod
    def _detect(text) -> ChineseType:
        res = hanzidentifier.identify(text)
        if res == hanzidentifier.SIMPLIFIED:
            return ChineseType.simplified
        if res == hanzidentifier.TRADITIONAL:
            return ChineseType.traditional
        if res in (hanzidentifier.BOTH, hanzidentifier.MIXED):
            return ChineseType.traditional | ChineseType.simplified
        return ChineseType.none

    def _convert_line(self, text: str, to: ChineseType) -> str:
        if to == ChineseType.simplified:
            return self.t2s.convert(text)
        elif to == ChineseType.traditional:
            return self.s2t.convert(text)
        raise ValueError(f"Unsupported type: {to}")


data/dataset_importer.py

#!/usr/bin/env python3
"""
Downloads a dataset and runs augmentation if needed

Example:
    python pipeline/data/dataset_importer.py \
        --type=corpus \
        --dataset=sacrebleu_aug-mix_wmt19 \
        --output_prefix=$(pwd)/test_data/augtest \
        --src=ru \
        --trg=en
"""

import argparse
import os
import random
import re
import shutil
import subprocess
import sys
from pathlib import Path
from typing import Dict, Iterable, List

from opustrainer.modifiers.noise import NoiseModifier
from opustrainer.modifiers.placeholders import PlaceholderTagModifier
from opustrainer.modifiers.surface import TitleCaseModifier, UpperCaseModifier
from opustrainer.modifiers.typos import TypoModifier
from opustrainer.types import Modifier

from pipeline.common.downloads import compress_file, decompress_file
from pipeline.data.cjk import ChineseConverter, ChineseType

random.seed(1111)


class CompositeModifier:
    """
    Composite modifier runs several modifiers one after another
    """

    def __init__(self, modifiers: List[Modifier]):
        self._modifiers = modifiers

    def __call__(self, batch: List[str]) -> Iterable[str]:
        for mod in self._modifiers:
            batch = list(mod(batch))

        return batch


MIX_PROB = 0.05  # 5% will be augmented in the mix
PROB_1 = 1.0  # 100% chance
PROB_0 = 0.0  # 0% chance
# use lower probabilities than 1 to add inline noise into the mix
# probability 1 adds way too much noise to a corpus
NOISE_PROB = 0.05
NOISE_MIX_PROB = 0.01


def get_typos_probs() -> Dict[str, float]:
    # select 4 random types of typos
    typos = set(random.sample(list(TypoModifier.modifiers.keys()), k=4))
    # set probability 1 for selected typos and 0 for the rest
    probs = {typo: PROB_1 if typo in typos else PROB_0 for typo in TypoModifier.modifiers.keys()}
    return probs


# See documentation for the modifiers in https://github.com/mozilla/translations/blob/main/docs/training/opus-trainer.md#supported-modifiers
modifier_map = {
    "aug-typos": lambda: TypoModifier(PROB_1, **get_typos_probs()),
    "aug-title": lambda: TitleCaseModifier(PROB_1),
    "aug-upper": lambda: UpperCaseModifier(PROB_1),
    "aug-noise": lambda: NoiseModifier(PROB_1),
    "aug-inline-noise": lambda: PlaceholderTagModifier(NOISE_PROB, augment=1),
    "aug-mix": lambda: CompositeModifier(
        [
            TypoModifier(MIX_PROB, **get_typos_probs()),
            TitleCaseModifier(MIX_PROB),
            UpperCaseModifier(MIX_PROB),
            NoiseModifier(MIX_PROB),
            PlaceholderTagModifier(NOISE_MIX_PROB, augment=1),
        ]
    ),
    "aug-mix-cjk": lambda: CompositeModifier(
        [
            NoiseModifier(MIX_PROB),
            PlaceholderTagModifier(NOISE_MIX_PROB, augment=1),
        ]
    ),
}


def run_cmd(cmd: List[str], env: Dict[str, str]):
    result = None
    # make sure to preserve the current process env vars
    env_vars = dict(os.environ)
    env_vars.update(env)
    try:
        result = subprocess.run(
            cmd,
            universal_newlines=True,
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
            check=False,
            env=env_vars,
        )
        result.check_returncode()
    except:
        if result:
            print(result.stdout)
        raise

    print(result.stdout)


def add_alignments(corpus: List[str]) -> List[str]:
    from simalign import SentenceAligner

    # We use unsupervised aligner here because statistical tools like fast_align require a large corpus to train on
    # This is slow without a GPU and is meant to operate only on small evaluation datasets

    # Use BERT with subwords and itermax as it has a higher recall and matches more words than other methods
    # See more details in the paper: https://arxiv.org/pdf/2004.08728.pdf
    # and in the source code: https://github.com/cisnlp/simalign/blob/master/simalign/simalign.py
    # This will download a 700Mb BERT model from Hugging Face and cache it
    aligner = SentenceAligner(model="bert", token_type="bpe", matching_methods="i")

    alignments = []
    for line in corpus:
        src_sent, trg_sent = line.split("\t")
        sent_aln = aligner.get_word_aligns(src_sent, trg_sent)["itermax"]
        aln_str = " ".join(f"{src_pos}-{trg_pos}" for src_pos, trg_pos in sent_aln)
        alignments.append(aln_str)

    corpus_tsv = [f"{sents}\t{aln}" for sents, aln in zip(corpus, alignments)]
    return corpus_tsv


# we plan to use it only for small evaluation datasets
def augment(output_prefix: str, aug_modifer: str, src: str, trg: str):
    """
    Augment corpus on disk using the OpusTrainer modifier
    """
    if aug_modifer not in modifier_map:
        raise ValueError(f"Invalid modifier {aug_modifer}. Allowed values: {modifier_map.keys()}")

    # file paths for compressed and uncompressed corpus
    uncompressed_src = f"{output_prefix}.{src}"
    uncompressed_trg = f"{output_prefix}.{trg}"
    compressed_src = f"{output_prefix}.{src}.zst"
    compressed_trg = f"{output_prefix}.{trg}.zst"

    corpus = read_corpus_tsv(compressed_src, compressed_trg, uncompressed_src, uncompressed_trg)

    if aug_modifer in ("aug-mix", "aug-inline-noise", "aug-mix-cjk"):
        # add alignments for inline noise
        # Tags modifier will remove them after processing
        corpus = add_alignments(corpus)

    modified = []
    for line in corpus:
        # recreate modifier for each line to apply randomization (for typos)
        modifier = modifier_map[aug_modifer]()
        modified += modifier([line])
    write_modified(modified, uncompressed_src, uncompressed_trg)


def read_corpus_tsv(
    compressed_src: str, compressed_trg: str, uncompressed_src: str, uncompressed_trg: str
) -> List[str]:
    """
    Decompress corpus and read to TSV
    """
    if os.path.isfile(uncompressed_src):
        os.remove(uncompressed_src)
    if os.path.isfile(uncompressed_trg):
        os.remove(uncompressed_trg)

    # Decompress the original corpus.
    decompress_file(compressed_src, keep_original=False)
    decompress_file(compressed_trg, keep_original=False)

    # Since this is only used on small evaluation sets, it's fine to load the entire dataset
    # and augmentation into memory rather than streaming it.
    with open(uncompressed_src) as f:
        corpus_src = [line.rstrip("\n") for line in f]
    with open(uncompressed_trg) as f:
        corpus_trg = [line.rstrip("\n") for line in f]

    corpus_tsv = [f"{src_sent}\t{trg_sent}" for src_sent, trg_sent in zip(corpus_src, corpus_trg)]
    return corpus_tsv


def write_modified(modified: List[str], uncompressed_src: str, uncompressed_trg: str):
    """
    Split the modified TSV corpus, write back and compress
    """
    modified_src = "\n".join([line.split("\t")[0] for line in modified]) + "\n"
    modified_trg = "\n".join([line.split("\t")[1] for line in modified]) + "\n"

    with open(uncompressed_src, "w") as f:
        f.write(modified_src)
    with open(uncompressed_trg, "w") as f:
        f.writelines(modified_trg)

    # compress corpus back
    compress_file(uncompressed_src, keep_original=False)
    compress_file(uncompressed_trg, keep_original=False)


def run_import(
    type: str,
    dataset: str,
    output_prefix: str,
    src: str,
    trg: str,
):
    current_dir = os.path.dirname(os.path.abspath(__file__))
    # these envs are standard across the pipeline

    if type == "corpus":
        # Parse a dataset identifier to extract importer, augmentation type and dataset name
        # Examples:
        # opus_wikimedia/v20230407
        # mtdata_EU-eac_forms-1-eng-lit
        # flores_aug-title_devtest
        # sacrebleu_aug-upper-strict_wmt19
        match = re.search(r"^(\w*)_(aug[a-z\-]*)?_?(.+)$", dataset)

        if not match:
            raise ValueError(
                f"Invalid dataset name: {dataset}. "
                f"Use the following format: <importer>_<name> or <importer>_<augmentation>_<name>."
            )

        importer = match.group(1)
        aug_modifer = match.group(2)
        name = match.group(3)

        no_aug_id = f"{importer}_{name}"

        print("Downloading parallel dataset")
        run_cmd(
            [os.path.join(current_dir, "download-corpus.sh"), no_aug_id, output_prefix],
            env={"SRC": src, "TRG": trg},
        )

        # TODO: convert everything to Chinese simplified for now
        # TODO: https://github.com/mozilla/firefox-translations-training/issues/896
        for lang in (src, trg):
            if lang == "zh":
                print("Converting the output file to Chinese Simplified")
                chinese_converter = ChineseConverter()
                stats = chinese_converter.convert_file(
                    Path(f"{output_prefix}.{lang}.zst"),
                    Path(f"{output_prefix}.converted.{lang}.zst"),
                    ChineseType.simplified,
                )
                shutil.move(f"{output_prefix}.converted.{lang}.zst", f"{output_prefix}.{lang}.zst")
                print(
                    f"Converted {stats.script_conversion.converted} lines from {stats.script_conversion.visited} to Chinese Simplified"
                )
                stats.save_json()

        if aug_modifer:
            print("Running augmentation")
            augment(output_prefix, aug_modifer, src=src, trg=trg)

    elif type == "mono":
        raise ValueError("Downloading mono data is not supported yet")
    else:
        raise ValueError(f"Invalid dataset type: {type}. Allowed values: mono, corpus")


def main() -> None:
    print(f"Running with arguments: {sys.argv}")
    parser = argparse.ArgumentParser(
        description=__doc__,
        # Preserves whitespace in the help text.
        formatter_class=argparse.RawTextHelpFormatter,
    )

    parser.add_argument("--type", metavar="TYPE", type=str, help="Dataset type: mono or corpus")
    parser.add_argument(
        "--src",
        metavar="SRC",
        type=str,
        help="Source language",
    )
    parser.add_argument(
        "--trg",
        metavar="TRG",
        type=str,
        help="Target language",
    )
    parser.add_argument(
        "--dataset",
        metavar="DATASET",
        type=str,
        help="Full dataset identifier. For example, sacrebleu_aug-upper-strict_wmt19 ",
    )
    parser.add_argument(
        "--output_prefix",
        metavar="OUTPUT_PREFIX",
        type=str,
        help="Write output dataset to a path with this prefix",
    )

    args = parser.parse_args()
    print("Starting dataset import and augmentation.")
    run_import(args.type, args.dataset, args.output_prefix, args.src, args.trg)
    print("Finished dataset import and augmentation.")


if __name__ == "__main__":
    main()


data/download-corpus.sh

#!/bin/bash
##
# Downloads parallel dataset
#

set -x
set -euo pipefail

[[ -z "${SRC}" ]] && echo "SRC is empty"
[[ -z "${TRG}" ]] && echo "TRG is empty"


dataset=$1
output_prefix=$2

echo "###### Downloading dataset ${dataset}"

cd "$(dirname "${0}")"

dir=$(dirname "${output_prefix}")
mkdir -p "${dir}"

name=${dataset#*_}
type=${dataset%%_*}

# Choose either the .sh or .py script.
if [[ -f "importers/corpus/${type}.py" ]]; then
  script="python3 importers/corpus/${type}.py"
else
  script="bash importers/corpus/${type}.sh"
fi

${script} "${SRC}" "${TRG}" "${output_prefix}" "${name}"

echo "###### Done: Downloading dataset ${dataset}"


data/download-mono.py

#!/usr/bin/env python3
"""
Downloads a monolingual dataset, shuffles it, and truncates it to a maximum amount of sentences.

Kinds:
   taskcluster/kinds/dataset/kind.yml

Example usage:

    pipeline/data/download-mono.py                  \\
        --dataset news-crawl_news.2021              \\
        --language en                               \\
        --max_sentences 100000000                   \\
        --artifacts $TASK_WORKDIR/artifacts

Artifacts:

    artifacts
    └── news.2021.en.zst
"""

import argparse
import os
import shutil
from contextlib import ExitStack
from pathlib import Path
from typing import Optional

from importers.mono.hplt import HpltDownloader

from pipeline.common.datasets import Dataset, shuffle_with_max_lines
from pipeline.common.downloads import (
    get_download_size,
    read_lines,
    write_lines,
)
from pipeline.common.logging import get_logger
from pipeline.data.cjk import ChineseConverter, ChineseType

CURRENT_FOLDER = os.path.dirname(os.path.abspath(__file__))
IMPORTERS_PATH = os.path.abspath(os.path.join(CURRENT_FOLDER, "mono"))

logger = get_logger(__file__)


def main(args_list: Optional[list[str]] = None) -> None:
    parser = argparse.ArgumentParser(
        description=__doc__,
        formatter_class=argparse.RawTextHelpFormatter,  # Preserves whitespace in the help text.
    )
    parser.add_argument("--dataset", type=str, help="The key for the dataset")
    parser.add_argument("--language", type=str, help="The BCP 47 language tag of the dataset")
    parser.add_argument(
        "--max_sentences", type=int, help="The maximum number of sentences to retain"
    )
    parser.add_argument(
        "--hplt_min_doc_score",
        type=float,
        help="The minimum document score to filter datasets that include this metric",
        default=5.0,
    )
    parser.add_argument(
        "--hplt_max_characters",
        type=int,
        help="The maximum length of the output segments. ",
        default=600,
    )
    parser.add_argument(
        "--hplt_merge_lines",
        type=bool,
        help="Whether to accumulate lines of the same document in one output segment until `hplt_max_characters` is reached.",
        default=False,
    )
    parser.add_argument(
        "--artifacts", type=Path, help="The location where the dataset will be saved"
    )
    args = parser.parse_args(args_list)

    dataset = Dataset(args.dataset)

    file_destination: Path = args.artifacts / f"{dataset.file_safe_name()}.{args.language}.zst"

    logger.info(f"Dataset: {args.dataset}")
    logger.info(f"Language: {args.language}")
    logger.info(f"HPLT Max Sentences: {args.max_sentences}")
    logger.info(f"HPLT Minimum Document Score Threshold: {args.hplt_min_doc_score}")
    logger.info(f"HPLT Merge Lines: {args.hplt_merge_lines}")
    logger.info(f"Artifacts: {args.artifacts}")
    logger.info(f"File Destination: {file_destination}")

    if not os.path.exists(args.artifacts):
        os.makedirs(args.artifacts)

    if dataset.importer == "hplt":
        if dataset.name != "mono/v2.0":
            raise ValueError("Only HPLT v2.0 is supported")
        HpltDownloader(
            language=args.language,
            hplt_min_doc_score=args.hplt_min_doc_score,
            max_characters=args.hplt_max_characters,
            max_lines=args.max_sentences,
            file_destination=file_destination,
            merge_lines=args.hplt_merge_lines,
        ).download()

        return

    url = None
    if dataset.importer == "url":
        url = dataset.name
    elif dataset.importer == "news-crawl":
        url = f"http://data.statmt.org/news-crawl/{args.language}/{dataset.name}.{args.language}.shuffled.deduped.gz"
        logger.info("Downloading WMT newscrawl monolingual data")
        logger.info(url)
    elif dataset.importer == "opus":
        url = f"https://object.pouta.csc.fi/OPUS-{dataset.name}/mono/{args.language}.txt.gz"
        logger.info("Downloading OPUS monolingual data")
        logger.info(url)
    else:
        raise Exception(f'Unsupported importer "{dataset.importer}"')

    logger.info(f"URL: {url}")

    with ExitStack() as stack:
        outfile = stack.enter_context(write_lines(file_destination))
        lines = stack.enter_context(read_lines(url))

        for line in shuffle_with_max_lines(
            line_stream=lines,
            seed=dataset.name,
            max_lines=args.max_sentences,
            total_byte_size=get_download_size(url),
        ):
            outfile.write(line)

    # TODO: convert everything to Chinese simplified for now
    # TODO: https://github.com/mozilla/firefox-translations-training/issues/896
    if args.language == "zh":
        logger.info("Converting the output file to Chinese Simplified")
        chinese_converter = ChineseConverter()
        converted_path = file_destination.with_suffix(".converted.zst")
        stats = chinese_converter.convert_file(
            file_destination, converted_path, ChineseType.simplified
        )
        shutil.move(converted_path, file_destination)
        print(
            f"Converted {stats.script_conversion.converted} lines from {stats.script_conversion.visited} to Chinese Simplified"
        )
        stats.save_json()


if __name__ == "__main__":
    main()


data/importers/corpus/flores.sh

#!/bin/bash
##
# Downloads flores dataset
# Dataset type can be "dev" or "devtest"
#

set -x
set -euo pipefail

echo "###### Downloading flores corpus"

src=$1
trg=$2
output_prefix=$3
dataset=$4

WGET="${WGET:-wget}" # This can be overridden by tests.

tmp="$(mktemp -d)/flores/${dataset}"
mkdir -p "${tmp}"

${WGET} -O "${tmp}/flores101_dataset.tar.gz" "https://dl.fbaipublicfiles.com/flores101/dataset/flores101_dataset.tar.gz"
tar -xzf "${tmp}/flores101_dataset.tar.gz" -C "${tmp}" --no-same-owner

flores_code() {
  code=$1

  if [ "${code}" == "zh" ] || [ "${code}" == "zh-Hans" ]; then
    flores_code="zho_simpl"
  elif [ "${code}" == "zh-Hant" ]; then
    flores_code="zho_trad"
  else
    flores_code=$(python3 -c "from mtdata.iso import iso3_code; print(iso3_code('${code}', fail_error=True))")
  fi

  echo "${flores_code}"
}

src_flores=$(flores_code "${src}")
trg_flores=$(flores_code "${trg}")

zstdmt -c "${tmp}/flores101_dataset/${dataset}/${src_flores}.${dataset}" > "${output_prefix}.${src}.zst"
zstdmt -c "${tmp}/flores101_dataset/${dataset}/${trg_flores}.${dataset}" > "${output_prefix}.${trg}.zst"

rm -rf "${tmp}"

echo "###### Done: Downloading flores corpus"


data/importers/corpus/mtdata.sh

#!/bin/bash
##
# Downloads a dataset using mtdata
#

set -x
set -euo pipefail

echo "###### Downloading mtdata corpus"

src=$1
trg=$2
output_prefix=$3
dataset=$4

tmp="$(dirname "${output_prefix}")/mtdata/${dataset}"
mkdir -p "${tmp}"

src_iso=$(python3 -c "from mtdata.iso import iso3_code; print(iso3_code('${src}', fail_error=True))")
trg_iso=$(python3 -c "from mtdata.iso import iso3_code; print(iso3_code('${trg}', fail_error=True))")

mtdata get -l "${src}-${trg}" -tr "${dataset}" -o "${tmp}"

find "${tmp}"

cat "${tmp}/train-parts/${dataset}.${src_iso}" | zstdmt -c > "${output_prefix}.${src}.zst"
cat "${tmp}/train-parts/${dataset}.${trg_iso}" | zstdmt -c > "${output_prefix}.${trg}.zst"

rm -rf "${tmp}"

echo "###### Done: Downloading mtdata corpus"


data/importers/corpus/opus.sh

#!/bin/bash
##
# Downloads corpus using opus
#

set -x
set -euo pipefail

echo "###### Downloading opus corpus"

src=$1
trg=$2
output_prefix=$3
dataset=$4

WGET="${WGET:-wget}" # This can be overridden by tests.

name=${dataset%%/*}
name_and_version="${dataset//[^A-Za-z0-9_- ]/_}"

tmp="$(dirname "${output_prefix}")/opus/${name_and_version}"
mkdir -p "${tmp}"

archive_path="${tmp}/${name}.txt.zip"

${WGET} -O "${archive_path}" "https://object.pouta.csc.fi/OPUS-${dataset}/moses/${src}-${trg}.txt.zip" ||
  ${WGET} -O "${archive_path}" "https://object.pouta.csc.fi/OPUS-${dataset}/moses/${trg}-${src}.txt.zip"
unzip -o "${archive_path}" -d "${tmp}"

for lang in ${src} ${trg}; do
  zstdmt -c "${tmp}/${name}.${src}-${trg}.${lang}" > "${output_prefix}.${lang}.zst" ||
    zstdmt -c "${tmp}/${name}.${trg}-${src}.${lang}" > "${output_prefix}.${lang}.zst"
done

rm -rf "${tmp}"


echo "###### Done: Downloading opus corpus"


data/importers/corpus/sacrebleu.sh

#!/bin/bash
##
# Downloads corpus using sacrebleu
#

set -x
set -euo pipefail

echo "###### Downloading sacrebleu corpus"

src=$1
trg=$2
output_prefix=$3
dataset=$4

{
  set +e

  sacrebleu                         \
    --test-set "${dataset}"         \
    --language-pair "${src}-${trg}" \
    --echo src                      \
  | zstdmt -c > "${output_prefix}.${src}.zst"

  sacrebleu                         \
    --test-set "${dataset}"         \
    --language-pair "${src}-${trg}" \
    --echo ref                      \
  | zstdmt -c > "${output_prefix}.${trg}.zst"

  status=$?

  set -e
}

if [ $status -ne 0 ]; then
  echo "The first import failed, try again by switching the language pair direction."

  sacrebleu                         \
    --test-set "${dataset}"         \
    --language-pair "${trg}-${src}" \
    --echo src                      \
    | zstdmt -c > "${output_prefix}.${trg}.zst"

  sacrebleu                         \
    --test-set "${dataset}"         \
    --language-pair "${trg}-${src}" \
    --echo ref                      \
    | zstdmt -c > "${output_prefix}.${src}.zst"
fi


echo "###### Done: Downloading sacrebleu corpus"


data/importers/corpus/url.py

#!/usr/bin/env python3
"""
Import data from a url.

Example usage:

pipeline/data/importers/corpus/url.py                                                       \\
  en                                                                       `# src`           \\
  ru                                                                       `# trg`           \\
  artifacts/releng-translations-dev_data_custom-en-ru_zip                  `# output_prefix` \\
  https://storage.google.com/releng-translations-dev/data/custom-en-ru.zip `# url`
"""

import argparse

from pipeline.common.downloads import stream_download_to_file
from pipeline.common.logging import get_logger

logger = get_logger(__file__)


def main() -> None:
    parser = argparse.ArgumentParser(
        description=__doc__,
        formatter_class=argparse.RawTextHelpFormatter,  # Preserves whitespace in the help text.
    )
    parser.add_argument("src", type=str)
    parser.add_argument("trg", type=str)
    parser.add_argument("output_prefix", type=str)
    parser.add_argument("url", type=str)

    args = parser.parse_args()

    src_file = args.url.replace("[LANG]", args.src)
    trg_file = args.url.replace("[LANG]", args.trg)
    src_dest = f"{args.output_prefix}.{args.src}.zst"
    trg_dest = f"{args.output_prefix}.{args.trg}.zst"

    logger.info(f"src:           {args.src}")
    logger.info(f"trg:           {args.trg}")
    logger.info(f"output_prefix: {args.output_prefix}")
    logger.info(f"src_dest:      {src_dest}")
    logger.info(f"trg_dest:      {trg_dest}")

    stream_download_to_file(src_file, src_dest)
    stream_download_to_file(trg_file, trg_dest)


if __name__ == "__main__":
    main()


data/importers/mono/hplt.py

import json
import random
from contextlib import ExitStack
from dataclasses import dataclass
from pathlib import Path
import icu

from pipeline.common.datasets import (
    CountingStep,
    FilteringStep,
    Statistics,
    WeakStringSet,
)
from pipeline.common.downloads import location_exists, read_lines, write_lines
from pipeline.common.logging import get_logger
from pipeline.common.memory import log_memory

logger = get_logger(__name__)

random.seed(38947598475)


@dataclass
class HPLTDocument:
    """
    A structured type for the HPLT document entry in a jsonl file.
    https://hplt-project.org/datasets/v2.0
    """

    def __init__(self, **json):
        self.lang = json["lang"]
        self.doc_scores = json["doc_scores"]
        self.seg_langs = json["seg_langs"]
        # The sentences in the text, which were separated by newlines.
        self.lines = json["text"].split("\n")

    # The list of detected document languages where the first language is most probable.
    # For example: [zho_Hans, zho_Hant, eng_Latn]
    lang: list[str]
    # The list of document scores from web-docs-scorer where the first score is the overall document score (WDS_score) followed by 8 subscores.
    # All the scores are from 0 to 10.
    # See https://github.com/pablop16n/web-docs-scorer/
    # For example, [8.3, 10, 10, 9.9, 10, 10, 10, 4, 0]
    doc_scores: list[float]
    # The detected language for each line (segment).
    # For example: [yue_Hant, zho_Hans, zho_Hans, zho_Hant, unk, ... ]
    seg_langs: list[str]
    # All of the text, split by newlines.
    lines: list[str]


class FilteringStatistics(Statistics):
    """
    Gather statistics about the filtering process.
    """

    def __init__(self, dataset_path: Path) -> None:
        super().__init__(dataset_path)
        self.shards = FilteringStep(
            "How many shards were sampled from. Each shard contains a subset of the "
            "total datasets available.",
        )
        self.visited_lines = FilteringStep(
            "How many lines were visited and kept from the HPLT documents.",
        )
        self.document_count = CountingStep(
            "How many documents were visited. This can help represent data diversity.",
        )
        self.duplicate_lines = CountingStep(
            "Of the collected lines, this counts how many were duplicates and discarded.",
        )
        self.final_lines = CountingStep(
            "How many lines were actually written.",
        )
        self.filtered_doc_locale = CountingStep(
            "How many lines were filtered based on document locale.",
        )
        self.filtered_line_locale = CountingStep(
            "How many lines were filtered based on line locales.",
        )
        self.filtered_doc_score = CountingStep(
            "How many lines were filtered based on document scores.",
        )
        self.filtered_too_long = CountingStep(
            "How many lines were filtered based on length.",
        )

    def count_shards_visited(self, *_args):
        self.shards.filtered -= 1
        self.shards.kept += 1


def get_hplt_locale(lang_iso6931: str) -> str:
    """
    Converts language in ISO-693-1 format to the HPLT format.
    For example, ru -> rus_Cyrl
    """
    # icu return Kore by default which is a mix of Hang and Hani
    if lang_iso6931 == "ko":
        return "kor_Hang"
    locale = icu.Locale(lang_iso6931)
    # add default script
    locale = icu.Locale.addLikelySubtags(locale)
    hplt_locale = f"{locale.getISO3Language()}_{locale.getScript()}"
    return hplt_locale


def get_hplt_map_url(hplt_locale: str) -> str:
    return f"https://data.hplt-project.org/two/cleaned/{hplt_locale}_map.txt"


def language_has_hplt_support(language: str) -> bool:
    hplt_locale = get_hplt_locale(language)
    hplt_map = get_hplt_map_url(hplt_locale)
    return location_exists(hplt_map)


def load_shuffled_shard_urls(hplt_locale: str) -> list[str]:
    """
    Download the list of shards, e.g.
    https://data.hplt-project.org/two/cleaned/rus_Cyrl/1.jsonl.zst
    https://data.hplt-project.org/two/cleaned/rus_Cyrl/2.jsonl.zst
    ...
    https://data.hplt-project.org/two/cleaned/rus_Cyrl/10.jsonl.zst
    """

    url = get_hplt_map_url(hplt_locale)
    logger.info(f"Downloading shard list: {url}")

    with read_lines(url) as lines:
        shard_urls = []
        for line in lines:
            shard_urls.append(line.strip())
    random.Random(url).shuffle(shard_urls)

    logger.info(f"Available shards for {hplt_locale}:")
    for lines in shard_urls:
        logger.info(f" - {lines}")
    return shard_urls


class HpltDownloader:
    """
    Downloads and filters the HPLT dataset.
    https://hplt-project.org/datasets/v2.0

    Parameters:
     - language: The BCP 47 language code to filter the documents.
     - hplt_min_doc_score: The minimum score a document must have to be included in the final dataset.
     - max_characters: The maximum number of characters to merge sentences in the document before writing if enabled.
                       Also filters lines that are too long.
     - max_lines: The maximum number of lines to include in the final dataset.
     - file_destination: The destination path where the final dataset will be written.
     - merge_lines: Whether to accumulate line of the same document in one segment until max_characters is reached.
    """

    def __init__(
        self,
        language: str,
        hplt_min_doc_score: float,
        max_characters: int,
        max_lines: int,
        file_destination: Path,
        merge_lines: bool,
    ) -> None:
        self.merge_lines = merge_lines
        self.max_lines = max_lines
        self.max_characters = max_characters
        self.hplt_min_doc_score = hplt_min_doc_score
        self.hplt_locale = get_hplt_locale(language)
        self.accumulated_text = ""
        self.cumulative_char_count = 0
        self.visited_lines = 0
        self.file_destination = file_destination
        self.stats = FilteringStatistics(file_destination)
        self.strings_seen = WeakStringSet()
        self.stack = ExitStack()
        self.outfile = self.stack.enter_context(write_lines(file_destination))

    def close(self):
        self.stack.close()

    def download(self):
        try:
            self._run_download()
        finally:
            self.close()

    def _run_download(self):
        logger.info(f"Using HPLT locale {self.hplt_locale}")
        shuffled_shard_urls = load_shuffled_shard_urls(self.hplt_locale)
        self.stats.shards.filtered = len(shuffled_shard_urls)

        # The shard URLs are shuffled, and then streamed into the read_lines iterator.
        # This iterator can work over multiple documents. The first document is loaded,
        # and then the documents in the shard are read in order from that shard. After
        # the first shard is read, the iterator continues with the next shards until
        # enough fluent sentences are collected. At this point the remaining shards
        # will not be visited.
        document_stream = self.stack.enter_context(
            read_lines(shuffled_shard_urls, on_enter_location=self.stats.count_shards_visited)
        )

        for document_json in document_stream:
            self.stats.document_count.value += 1
            document = HPLTDocument(**json.loads(document_json))
            overall_doc_score = document.doc_scores[0]
            doc_lang = document.lang[0]

            self._maybe_write_accumulated_text()

            # HPLT 2.0 uses document level scores
            if overall_doc_score < self.hplt_min_doc_score:
                self.stats.filtered_doc_score.value += 1
                continue

            # We want only documents written primarily in the target language
            if doc_lang != self.hplt_locale:
                self.stats.filtered_doc_locale.value += 1
                continue

            # Visit the lines in the document.
            for line_locale, line in zip(document.seg_langs, document.lines):
                self.visited_lines += 1
                self._process_line(line_locale, line)
                if self.visited_lines % 5_000_000 == 0:
                    logger.info(f"Visited {self.visited_lines:,} lines")
                    logger.info(f"Kept {self.stats.visited_lines.kept:,}.")
                    logger.info(
                        f"Wrote {self.stats.final_lines.value:,} out of {self.max_lines:,}."
                    )
                    log_memory()

                if self.stats.final_lines.value == self.max_lines:
                    break

            if self.stats.final_lines.value == self.max_lines:
                break

            self._maybe_write_accumulated_text()

        self.stats.visited_lines.filtered = self.visited_lines - self.stats.visited_lines.kept
        logger.info(f"Wrote {self.stats.final_lines.value:,} lines to: {self.file_destination}")
        stat_path = self.stats.save_json()
        logger.info(f"Saved filtering stats to: {stat_path}")

    def _process_line(self, line_locale: str, line: str):
        # Line locale does not match expected locale, filter
        if line_locale != self.hplt_locale:
            self.stats.filtered_line_locale.value += 1
            self._maybe_write_accumulated_text()
            return

        char_count = len(line)
        # Filter long segments
        if char_count > self.max_characters:
            self.stats.filtered_too_long.value += 1
            self._maybe_write_accumulated_text()
            return

        # Just write the current line if merging is disabled
        if not self.merge_lines:
            self.accumulated_text = line
            self.stats.visited_lines.kept += 1
            self._maybe_write_accumulated_text()
            return

        # Text accumulation mode starts here

        self.stats.visited_lines.kept += 1

        # Determine if this sentence should be added to the previous one or
        # written out as a new line.
        if self.cumulative_char_count + char_count + 1 > self.max_characters:
            # This line would be too long, write it out.
            self._maybe_write_accumulated_text()

        self.cumulative_char_count += char_count
        # Collect this line to write.
        if self.accumulated_text:
            self.accumulated_text = f"{self.accumulated_text} {line}"
            # count the whitespace
            self.cumulative_char_count += 1
        else:
            self.accumulated_text = line

    def _maybe_write_accumulated_text(self):
        """
        Since the loop below is building up paragraphs of text, we only want to write
        out a line when enough text has been accumulated. The paragraph should be
        written out when either the text gets too long, or the next line is discarded.
        """

        self.cumulative_char_count = 0
        if self.accumulated_text:
            if self.accumulated_text in self.strings_seen:
                self.stats.duplicate_lines.value += 1
            else:
                self.outfile.write(self.accumulated_text + "\n")
                self.stats.final_lines.value += 1
                self.strings_seen.add(self.accumulated_text)
            self.accumulated_text = ""


data/requirements/analyze.in

matplotlib==3.8.3
numpy==1.26.4
requests==2.31.0


data/requirements/data.in

opustrainer==0.3
simalign==0.4
mtdata==0.4.1
psutil==6.0.0
hanzidentifier==1.2.0
OpenCC==1.1.9
sacrebleu==2.4.2
PyICU==2.8.1


eval/eval.py

#!/usr/bin/env python3
"""
Evaluate a trained model with both the BLEU and chrF metrics.

Kinds:
   taskcluster/kinds/evaluate/kind.yml
   taskcluster/kinds/evaluate-quantized/kind.yml
   taskcluster/kinds/evaluate-teacher-ensemble/kind.yml

Example usage:

    $VCS_PATH/pipeline/eval/eval.py
        --src               en                                                 \\
        --trg               ca                                                 \\
        --marian_config     fetches/final.model.npz.best-chrf.npz.decoder.yml  \\
        --models            fetches/final.model.npz.best-chrf.npz              \\
        --dataset_prefix    fetches/wmt09                                      \\
        --artifacts_prefix  artifacts/wmt09                                    \\
        --model_variant     gpu                                                \\
        --workspace         12000                                              \\
        --gpus              4

Artifacts:

For instance for a artifacts_prefix of: "artifacts/wmt09":

  artifacts
  ├── wmt09.en             The source sentences
  ├── wmt09.ca             The target output
  ├── wmt09.ca.ref         The original target sentences
  ├── wmt09.log            The Marian log
  ├── wmt09.metrics        The BLEU and chrF score
  └── wmt09.metrics.json   The BLEU and chrF score in json format

Fetches:

For instance for a value of: "fetches/wmt09":
  fetches
  ├── wmt09.en.zst
  └── wmt09.ca.zst
"""


import argparse
import json
import os
import subprocess
from textwrap import dedent, indent
from typing import Optional

from sacrebleu.metrics.bleu import BLEU, BLEUScore
from sacrebleu.metrics.chrf import CHRF, CHRFScore

from pipeline.common.downloads import decompress_file
from pipeline.common.logging import get_logger

logger = get_logger("eval")
try:
    import wandb
    from translations_parser.publishers import METRIC_KEYS, WandB
    from translations_parser.utils import metric_from_tc_context
    from translations_parser.wandb import (
        add_wandb_arguments,
        get_wandb_publisher,
        list_existing_group_logs_metrics,
    )

    WANDB_AVAILABLE = "TASKCLUSTER_PROXY_URL" in os.environ
except ImportError as e:
    print(f"Failed to import tracking module: {e}")
    WANDB_AVAILABLE = False


def run_bash_oneliner(command: str):
    """
    Runs multi-line bash with comments as a one-line command.
    """
    command_dedented = dedent(command)

    # Remove comments and whitespace.
    lines = [
        line.strip() for line in command_dedented.split("\n") if line and not line.startswith("#")
    ]
    command = " \\\n".join(lines)

    logger.info("-----------------Running bash in one line--------------")
    logger.info(indent(command_dedented, "  "))
    logger.info("-------------------------------------------------------")
    return subprocess.check_call(command, shell=True)


def main(args_list: Optional[list[str]] = None) -> None:
    parser = argparse.ArgumentParser(
        description=__doc__,
        formatter_class=argparse.RawTextHelpFormatter,  # Preserves whitespace in the help text.
    )
    parser.add_argument(
        "--artifacts_prefix",
        type=str,
        help="The location where the translated results will be saved",
    )
    parser.add_argument(
        "--dataset_prefix", type=str, help="The evaluation datasets prefix, used in the form."
    )
    parser.add_argument("--src", type=str, help='The source language, e.g "en".')
    parser.add_argument("--trg", type=str, help='The target language, e.g "ca".')
    parser.add_argument("--marian", type=str, help="The path the to marian binaries.")
    parser.add_argument("--marian_config", type=str, help="The marian yaml config for the model.")
    parser.add_argument(
        "--quantized",
        action="store_true",
        help="Use a quantized model. This requires the browsermt fork of Marian",
    )
    parser.add_argument(
        "--models",
        type=str,
        help="The Marian model (or models if its an ensemble) to use for translations",
    )
    parser.add_argument(
        "--vocab",
        required=False,
        type=str,
        help="The path to a vocab file (optional)",
    )
    parser.add_argument(
        "--shortlist",
        required=False,
        type=str,
        help="The path to a lexical shortlist (optional)",
    )
    parser.add_argument("--workspace", type=str, help="The preallocated MB for the workspace")
    parser.add_argument(
        "--gpus",
        required=False,
        type=str,
        help="Which GPUs to use (only for the gpu model variant)",
    )
    parser.add_argument(
        "--model_variant", type=str, help="The model variant to use, (gpu, cpu, quantized)"
    )

    # Add Weight & Biases CLI args when module is loaded
    if WANDB_AVAILABLE:
        add_wandb_arguments(parser)

    args = parser.parse_args(args_list)

    src = args.src
    trg = args.trg
    dataset_prefix = args.dataset_prefix
    artifacts_prefix = args.artifacts_prefix

    artifacts_dir = os.path.dirname(artifacts_prefix)
    source_file_compressed = f"{dataset_prefix}.{src}.zst"
    source_file = f"{artifacts_prefix}.{src}"
    target_file_compressed = f"{dataset_prefix}.{trg}.zst"
    target_file = f"{artifacts_prefix}.{trg}"
    target_ref_file = f"{artifacts_prefix}.{trg}.ref"
    marian_decoder = f'"{args.marian}"/marian-decoder'
    marian_log_file = f"{artifacts_prefix}.log"
    language_pair = f"{src}-{trg}"
    metrics_file = f"{artifacts_prefix}.metrics"
    metrics_json = f"{artifacts_prefix}.metrics.json"

    # Configure Marian for the different model variants.
    marian_extra_args = []
    if args.model_variant == "quantized":
        marian_extra_args = ["--int8shiftAlphaAll"]
    elif args.model_variant == "gpu":
        if not args.workspace:
            raise Exception("The workspace size was not provided")
        marian_extra_args = [
            '--workspace', args.workspace,
            '--devices', args.gpus,
        ]  # fmt: skip
    elif not args.model_variant == "cpu":
        raise Exception(f"Unsupported model variant {args.model_variant}")

    if args.vocab:
        # Pass in the vocab twice as it's shared between the source and the target.
        marian_extra_args = [*marian_extra_args, "--vocabs", args.vocab, args.vocab]

    if args.shortlist:
        # The final "false" argument tells Marian not to verify the correctness of the shortlist.
        marian_extra_args = marian_extra_args + ["--shortlist", args.shortlist, "false"]

    logger.info("The eval script is configured with the following:")
    logger.info(f" >          artifacts_dir: {artifacts_dir}")
    logger.info(f" > source_file_compressed: {source_file_compressed}")
    logger.info(f" >            source_file: {source_file}")
    logger.info(f" >            target_file: {target_file}")
    logger.info(f" >        target_ref_file: {target_ref_file}")
    logger.info(f" >         marian_decoder: {marian_decoder}")
    logger.info(f" >        marian_log_file: {marian_log_file}")
    logger.info(f" >          language_pair: {language_pair}")
    logger.info(f" >           metrics_file: {metrics_file}")
    logger.info(f" >           metrics_json: {metrics_json}")
    logger.info(f" >      marian_extra_args: {marian_extra_args}")
    logger.info(f" >                   gpus: {args.gpus}")

    logger.info("Ensure that the artifacts directory exists.")
    os.makedirs(artifacts_dir, exist_ok=True)

    logger.info("Save the original target sentences to the artifacts")

    decompress_file(target_file_compressed, keep_original=False, decompressed_path=target_ref_file)

    run_bash_oneliner(
        f"""
        # Decompress the source file, e.g. $fetches/wmt09.en.zst
        zstdmt -dc "{source_file_compressed}"

        # Tee the source file into the artifacts directory, e.g. $artifacts/wmt09.en
        | tee "{source_file}"

        # Take the source and pipe it in to be decoded (translated) by Marian.
        | {marian_decoder}
            --models {args.models}
            --config {args.marian_config}
            --quiet
            --quiet-translation
            --log {marian_log_file}
            {" ".join(marian_extra_args)}

        # The translations be "tee"ed out to the artifacts, e.g. $artifacts/wmt09.ca
        | tee "{target_file}"
        """
    )

    with open(target_ref_file, "r") as file:
        target_ref_lines = file.readlines()
    with open(target_file, "r") as file:
        target_lines = file.readlines()
    with open(source_file, "r") as file:
        source_lines = file.readlines()

    compute_bleu = BLEU(trg_lang=trg)
    compute_chrf = CHRF()

    logger.info("Computing the BLEU score.")
    bleu_score: BLEUScore = compute_bleu.corpus_score(target_lines, [target_ref_lines])
    bleu_details = json.loads(
        bleu_score.format(signature=compute_bleu.get_signature().format(), is_json=True)
    )

    logger.info("Computing the chrF score.")
    chrf_score: CHRFScore = compute_chrf.corpus_score(target_lines, [target_ref_lines])
    chrf_details = json.loads(
        chrf_score.format(signature=compute_chrf.get_signature().format(), is_json=True)
    )

    # The default comet model.
    # It should match the model used in https://github.com/mozilla/firefox-translations-models/
    comet_model_name = "Unbabel/wmt22-comet-da"

    if os.environ.get("COMET_SKIP"):
        comet_score = "skipped"
        print("COMET_SKIP was set, so the COMET score will not be computed.")
    else:
        logger.info("Loading COMET")
        import comet

        # COMET_MODEL_DIR allows tests to place the model in a data directory
        comet_checkpoint = comet.download_model(
            comet_model_name, saving_directory=os.environ.get("COMET_MODEL_DIR")
        )
        comet_model = comet.load_from_checkpoint(comet_checkpoint)
        comet_data = []
        for source, target, target_ref in zip(source_lines, target_lines, target_ref_lines):
            comet_data.append({"src": source, "mt": target, "ref": target_ref})
        # GPU information comes in the form of a list of numbers, e.g. "0 1 2 3". Split these to
        # get the GPU count.
        gpu_count = len(args.gpus.split(" "))
        if os.environ.get("COMET_CPU"):
            gpu_count = 0  # Let tests override the CPU count.
        comet_mode = "cpu" if gpu_count == 0 else "gpu"
        logger.info(f'Computing the COMET score with "{comet_model_name}" using the {comet_mode}')

        comet_results = comet_model.predict(comet_data, gpus=gpu_count)
        # Reduce the precision.
        comet_score = round(comet_results.system_score, 4)

    metrics = {
        "bleu": {
            "score": bleu_details["score"],
            # Example details:
            # {
            #     "name": "BLEU",
            #     "score": 0.4,
            #     "signature": "nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.0.0",
            #     "verbose_score": "15.6/0.3/0.2/0.1 (BP = 0.823 ratio = 0.837 hyp_len = 180 ref_len = 215)",
            #     "nrefs": "1",
            #     "case": "mixed",
            #     "eff": "no",
            #     "tok": "13a",
            #     "smooth": "exp",
            #     "version": "2.0.0"
            # }
            "details": bleu_details,
        },
        "chrf": {
            "score": chrf_details["score"],
            # Example details:
            # {
            #     "name": "chrF2",
            #     "score": 0.64,
            #     "signature": "nrefs:1|case:mixed|eff:yes|nc:6|nw:0|space:no|version:2.0.0",
            #     "nrefs": "1",
            #     "case": "mixed",
            #     "eff": "yes",
            #     "nc": "6",
            #     "nw": "0",
            #     "space": "no",
            #     "version": "2.0.0"
            # }
            "details": chrf_details,
        },
        "comet": {
            "score": comet_score,
            "details": {
                "model": comet_model_name,
                "score": comet_score,
            },
        },
    }

    logger.info(f"Writing {metrics_json}")
    with open(metrics_json, "w") as file:
        file.write(json.dumps(metrics, indent=2))

    logger.info(f'Writing the metrics in the older "text" format: {metrics_file}')
    with open(metrics_file, "w") as file:
        file.write(f"{bleu_details['score']}\n" f"{chrf_details['score']}\n" f"{comet_score}\n")

    if WANDB_AVAILABLE:
        metric = metric_from_tc_context(
            chrf=chrf_details["score"], bleu=bleu_details["score"], comet=comet_score
        )

        run_client = get_wandb_publisher(  # noqa
            project_name=args.wandb_project,
            group_name=args.wandb_group,
            run_name=args.wandb_run_name,
            taskcluster_secret=args.taskcluster_secret,
            artifacts=args.wandb_artifacts,
            publication=args.wandb_publication,
        )
        if run_client is None:
            # W&B publication may be direclty disabled through WANDB_PUBLICATION
            return

        logger.info(f"Publishing metrics to Weight & Biases ({run_client.extra_kwargs})")
        run_client.open()
        run_client.handle_metrics(metrics=[metric])
        run_client.close()

        # Publish an extra row on the group_logs summary run
        group_logs_client = WandB(  # noqa
            project=run_client.wandb.project,
            group=run_client.wandb.group,
            name="group_logs",
            suffix=run_client.suffix,
        )
        logger.info("Adding metric row to the 'group_logs' run")
        group_logs_client.open()

        # Restore existing metrics data
        data = list_existing_group_logs_metrics(group_logs_client.wandb)
        data.append(
            [
                run_client.wandb.group,
                run_client.wandb.name,
                metric.importer,
                metric.dataset,
                metric.augmentation,
            ]
            + [getattr(metric, attr) for attr in METRIC_KEYS]
        )
        group_logs_client.wandb.log(
            {
                "metrics": wandb.Table(
                    columns=[
                        "Group",
                        "Model",
                        "Importer",
                        "Dataset",
                        "Augmenation",
                        *METRIC_KEYS,
                    ],
                    data=data,
                )
            }
        )
        group_logs_client.close()


if __name__ == "__main__":
    main()


eval/requirements/eval.in

sacrebleu[ja,ko]==2.4.2
unbabel-comet==2.2.2


quantize/export.sh

#!/bin/bash
##
# Export the quantized model to bergamot translator format.
#
# This script requires the browsermt fork of Marian for the int8shiftAlphaAll mode.
# https://github.com/browsermt/marian-dev
# https://github.com/browsermt/students/tree/master/train-student#5-8-bit-quantization

set -x
set -euo pipefail

echo "###### Exporting a quantized model"

test -v SRC
test -v TRG
test -v BMT_MARIAN

model_dir=$1
shortlist=$2
vocab=$3
output_dir=$4

mkdir -p "${output_dir}"

model="${output_dir}/model.${SRC}${TRG}.intgemm.alphas.bin"
cp "${model_dir}/model.intgemm.alphas.bin" "${model}"
pigz "${model}"

shortlist_bin="${output_dir}/lex.50.50.${SRC}${TRG}.s2t.bin"
"${BMT_MARIAN}"/marian-conv \
  --shortlist "${shortlist}" 50 50 0 \
  --dump "${shortlist_bin}" \
  --vocabs "${vocab}" "${vocab}"
pigz "${shortlist_bin}"

vocab_out="${output_dir}/vocab.${SRC}${TRG}.spm"
cp "${vocab}" "${vocab_out}"
pigz "${vocab_out}"


echo "### Export is completed. Results: ${output_dir}"

echo "###### Done: Exporting a quantized model"


quantize/quantize.sh

#!/bin/bash
##
# Runs quantization of the student model.
#

set -x
set -euo pipefail

echo "###### Quantizing a model"

test -v BMT_MARIAN
test -v BIN
test -v SRC
test -v TRG

model=$1
vocab=$2
shortlist=$3
devtest_src=$4
output_dir=$5

cd "$(dirname "${0}")"

res_model="${output_dir}/model.intgemm.alphas.bin"
mkdir -p "${output_dir}"
cp "${vocab}" "${output_dir}"

echo "### Decoding a sample test set in order to get typical quantization values"
test -s "${output_dir}/quantmults" ||
  "${BMT_MARIAN}"/marian-decoder \
    --models "${model}" \
    --vocabs "${vocab}" "${vocab}" \
    --config "decoder.yml" \
    --input "${devtest_src}" \
    --output "${output_dir}/output.${TRG}" \
    --shortlist "${shortlist}" false \
    --quiet \
    --quiet-translation \
    --log "${output_dir}/cpu.output.log" \
    --dump-quantmult \
    2>"${output_dir}/quantmults"

echo "### Quantizing"
test -s "${output_dir}/model.alphas.npz" ||
  "${BMT_MARIAN}"/../scripts/alphas/extract_stats.py \
    "${output_dir}/quantmults" \
    "${model}" \
    "${output_dir}/model.alphas.npz"

echo "### Converting"
test -s "${res_model}" ||
  "${BMT_MARIAN}"/marian-conv \
    --from "${output_dir}/model.alphas.npz" \
    --to "${res_model}" \
    --gemm-type intgemm8

echo "### The result models is saved to ${res_model}"

echo "###### Done: Quantizing a model"


quantize/requirements/quantize.in

numpy


setup/compile-extract-lex.sh

#!/bin/bash
##
# Installs and compiles alignment tools
#

set -x
set -euo pipefail

echo "###### Compiling extract-lex"

test -v BIN

build_dir=$1
threads=$2

mkdir -p "${BIN}"
mkdir -p "${build_dir}"
cd "${build_dir}"
cmake ..
make -j "${threads}"
cp extract_lex "${BIN}"


echo "###### Done: Compiling extract-lex"


setup/compile-fast-align.sh

#!/bin/bash
##
# Installs and compiles alignment tools
#

set -x
set -euo pipefail

echo "###### Compiling fast align"

test -v BIN

build_dir=$1
threads=$2


mkdir -p "${BIN}"
mkdir -p "${build_dir}"
cd "${build_dir}"
cmake ..
make -j "${threads}"
cp fast_align atools "${BIN}"

echo "###### Done: Compiling fast align"


setup/compile-marian.sh

#!/bin/bash
##
# Installs and compiles marian
#

set -x
set -euo pipefail

echo "###### Compiling marian"

marian_dir=$1
threads=${2}
use_gpu=${3:-true}
extra_args=( "${@:4}" )

mkdir -p "${marian_dir}"
cd "${marian_dir}"

if [ "${use_gpu}" == "true" ]; then
  # this is a production version that runs on GPU
  test -v CUDA_DIR
  cmake .. -DUSE_SENTENCEPIECE=on -DUSE_FBGEMM=on -DCOMPILE_CPU=on -DCMAKE_BUILD_TYPE=Release \
    -DCUDA_TOOLKIT_ROOT_DIR="${CUDA_DIR}" "${extra_args[@]}"
else
  # this is a CPU version that we use for testing
  cmake .. -DUSE_SENTENCEPIECE=on -DUSE_FBGEMM=on -DCOMPILE_CPU=on -DCMAKE_BUILD_TYPE=Release \
    -DCOMPILE_CUDA=off -DCOMPILE_SERVER=on "${extra_args[@]}"
fi

make -j "${threads}"

echo "###### Done: Compiling marian"


setup/compile-preprocess.sh

#!/bin/bash
##
# Installs and compiles alignment tools
#

set -x
set -euo pipefail

echo "###### Compiling preprocess"

test -v BIN

build_dir=$1
threads=$2

mkdir -p "${build_dir}"
cd "${build_dir}"
cmake .. -DBUILD_TYPE=Release
make -j "${threads}"
cp bin/dedupe "${BIN}"

echo "###### Done: Compiling preprocess"


setup/install-deps.sh

#!/bin/bash
##
# Installs system dependencies
#

set -x
set -euo pipefail

echo "######### Installing dependencies"

apt-get update

echo "### Installing extra dependencies"
apt-get install -y pigz htop wget unzip parallel bc git

echo "### Installing marian dependencies"
apt-get install -y build-essential libboost-system-dev libprotobuf10 \
  protobuf-compiler libprotobuf-dev openssl libssl-dev libgoogle-perftools-dev

echo "### Installing Intel MKL"
wget -qO- 'https://apt.repos.intel.com/intel-gpg-keys/GPG-PUB-KEY-INTEL-SW-PRODUCTS-2023.PUB' | apt-key add -
sh -c 'echo deb https://apt.repos.intel.com/mkl all main > /etc/apt/sources.list.d/intel-mkl.list'
apt-get update
apt-get install -y intel-mkl-64bit-2020.0-088

echo "### Installing fast_align dependencies "
apt-get install -y libgoogle-perftools-dev libsparsehash-dev libboost-all-dev

echo "######### Done: Installing dependencies"


setup/install-kenlm.sh

#!/bin/bash
##
# Installs and compiles kenlm
#

set -x
set -euo pipefail

echo "###### Installing kenlm"

test -v BIN

kenlm=$1
threads=$2

cd "${kenlm}"
mkdir -p build
cd build
mkdir "${BIN}/kenlm"
cmake .. -DKENLM_MAX_ORDER=7 -DCMAKE_INSTALL_PREFIX:PATH="${BIN}/kenlm"
make -j "${threads}" install
cd ..

python -m pip install . --install-option="--max_order 7"

echo "###### Done: Installing kenlm"


train/requirements/train.in

opustrainer==0.3
gpustat==1.1.1


train/spm-vocab.sh

#!/bin/bash
##
# Train the SentencePiece vocabulary model. This outputs a ".spm" binary file, and the
# ".vocab" file which is a human readable list of the vocabulary. The vocab file is
# what is used to tokenize text input for the machine learning model. The vocab that
# is generated is a mix of the source and target languages.
#
# Docs:
#   docs/vocab-size.md
#
# Kinds:
#   taskcluster/ci/train-vocab/kind.yml
#
# Example usage:
#
#   export MARIAN=$MOZ_FETCHES_DIR                 && \
#   spm-vocab.sh                                      \
#       fetches/corpus.en.zst  `# merged_corpus_src`  \
#       fetches/corpus.ca.zst  `# merged_corpus_trg`  \
#       artifacts/vocab.spm    `# vocab_output`       \
#       10000000               `# sample_size`        \
#       auto                   `# threads`            \
#       32000                  `# vocab_size`

set -x
set -euo pipefail

if [[ -z "${MARIAN}" ]]; then
    echo "Error: The MARIAN environment variable was not provided. This is required as"
    echo "the path to the spm_train binary."
    exit 1
fi

# The name of the source corpus, e.g. "fetches/corpus.en.zst".
merged_corpus_src=$1
# The name of the target corpus, e.g. "fetches/corpus.ca.zst".
merged_corpus_trg=$2
# Where the vocab file will be output, e.g. "artifacts/vocab.spm"
vocab_output=$3
# The maximum number of sentences to train on, e.g. 10000000
sample_size=$4
# The thread count, either "auto" or an int.
num_threads=$5
# The size of the final vocab. Defaults to 32000.
vocab_size=${6:-None}

if [ "$vocab_size" == "None" ]; then
  vocab_size=32000
fi

if (( vocab_size % 8 != 0 )); then
  echo "Error: vocab_size must be a multiple of 8 (https://github.com/mozilla/translations/issues/249)"
  exit 1
fi

if [ "$num_threads" = "auto" ]; then
  num_threads=$(nproc)
fi

vocab_dir=$(dirname "${vocab_output}")
mkdir -p "${vocab_dir}"

zstdmt -dc "${merged_corpus_src}" >"${vocab_dir}/data.src.txt"
zstdmt -dc "${merged_corpus_trg}" >"${vocab_dir}/data.trg.txt"

# The input arguments are available here:
#   https://github.com/google/sentencepiece/blob/master/doc/options.md
#
# https://github.com/hplt-project/OpusTrainer/tree/main#generating-vocabulary-and-tags-before-training
# byte_fallback - decomposes unknown pieces into UTF-8 bytes
# user_defined_symbols - placeholders
"${MARIAN}/spm_train" \
  --bos_id=-1 \
  --eos_id=0 \
  --unk_id=1 \
  --user_defined_symbols="__source__,__target__,__done__,__start__,__end__" \
  --model_prefix="${vocab_dir}/vocab" \
  --vocab_size="${vocab_size}" \
  --input="${vocab_dir}/data.src.txt,${vocab_dir}/data.trg.txt" \
  --input_sentence_size="${sample_size}" \
  --shuffle_input_sentence=true \
  --byte_fallback \
  --num_threads "${num_threads}"

rm "${vocab_dir}/data.src.txt" "${vocab_dir}/data.trg.txt"

mv "${vocab_dir}/vocab.model" "${vocab_output}"


train/train.py

"""
Run training using Marian and OpusTrainer.
"""

import argparse
from contextlib import ExitStack
from enum import Enum
import os
from pathlib import Path
import random
import shutil
import tempfile
from typing import Any, Generator, Optional

from pipeline.common.downloads import read_lines, write_lines
from pipeline.common.logging import get_logger
from pipeline.common.command_runner import apply_command_args, run_command_pipeline

logger = get_logger(__file__)
train_dir = Path(__file__).parent


CJK_LANGS = ["zh", "ja", "ko"]


class ModelType(Enum):
    student = "student"
    teacher = "teacher"
    backward = "backward"


class TrainingType(Enum):
    finetune = "finetune"
    train = "train"


class StudentModel(Enum):
    none = "None"
    tiny = "tiny"
    base = "base"


class TeacherMode(Enum):
    none = "None"
    one_stage = "one-stage"
    two_stage = "two-stage"


class BestModelMetric(Enum):
    chrf = "chrf"
    ce_mean_words = "ce-mean-words"
    bleu_detok = "bleu-detok"
    # These are also available in Marian, but not used here:
    #   cross_entropy = "cross-entropy"
    #   perplexity = "perplexity"
    #   valid_script = "valid-script"
    #   translation = "translation"
    #   bleu = "bleu"
    #   bleu_segmented = "bleu-segmented"


def build_dataset_tsv(
    dataset_prefix: str,
    src: str,
    trg: str,
    alignments_file: Optional[Path] = None,
) -> Path:
    """
    Takes as input a dataset prefix, and combines the datasets into a TSV, removing
    the original files. If an alignments file is provided, any empty alignments will
    be discarded.

    For instance:
        Prefix:
          - path/to/corpus

        Files used:
          - path/to/corpus.en.zst
          - path/to/corpus.fr.zst
          - path/to/corpus.aln.zst

        And then builds:
          - path/to/corpus.enfr.tsv
    """
    src_path = Path(f"{dataset_prefix}.{src}.zst")
    trg_path = Path(f"{dataset_prefix}.{trg}.zst")
    # OpusTrainer supports only tsv and gzip
    # TODO: pigz is not installed on the generic Taskcluster worker, so we use datasets in decompressed mode for now
    tsv_path = Path(f"{dataset_prefix}.{src}{trg}.tsv")  # .gz

    with ExitStack() as stack:
        tsv_outfile = stack.enter_context(write_lines(tsv_path))
        src_lines: Generator[str, Any, Any] = stack.enter_context(read_lines(src_path))
        trg_lines: Generator[str, Any, Any] = stack.enter_context(read_lines(trg_path))

        logger.info(f"Generating tsv dataset: {tsv_path}")

        if alignments_file:
            logger.info(f"Using alignments file: {alignments_file}")

            aln_lines: Generator[str, Any, Any] = stack.enter_context(
                read_lines(f"{alignments_file}")
            )
            empty_alignments = []

            for src_line, trg_line, aln_line in zip(src_lines, trg_lines, aln_lines):
                if aln_line.strip():
                    tsv_outfile.write(
                        f"{src_line.strip()}\t{trg_line.strip()}\t{aln_line.strip()}\n"
                    )
                else:
                    # do not write lines with empty alignments to TSV, Marian will complain and skip those
                    empty_alignments.append((src_line, trg_line))

            if empty_alignments:
                logger.info(f"Number of empty alignments is {len(empty_alignments)}")
                logger.info("Sample of empty alignments:")
                random.shuffle(empty_alignments)
                for src_line, trg_line in empty_alignments[:50]:
                    logger.info(f"  src: {src_line.strip()}")
                    logger.info(f"  trg: {trg_line.strip()}")

        else:
            for src_line, trg_line in zip(src_lines, trg_lines):
                tsv_outfile.write(f"{src_line.strip()}\t{trg_line.strip()}\n")

    logger.info("Freeing up disk space after TSV merge.")
    logger.info(f"Removing {src_path}")
    src_path.unlink()
    logger.info(f"Removing {trg_path}")
    trg_path.unlink()
    if alignments_file:
        logger.info(f"Removing {alignments_file}")
        alignments_file.unlink()

    return tsv_path


def get_log_parser_command():
    if shutil.which("parse_tc_logs") is None:
        logger.info("Weight & Biases publication script is not available.")
        return ["cat"]

    if "TEST_ARTIFACTS" in os.environ:
        logger.info("Weight & Biases publication is disabled for unit tests.")
        return ["cat"]

    logger.info("Weight & Biases publication is available.")
    return ["parse_tc_logs", "--from-stream", "--publish-group-logs", "--verbose"]


class TrainCLI:
    def __init__(self, args: Any, temp_dir: Path) -> None:
        self.temp_dir = temp_dir
        self.vocab: Path = args.vocab
        self.src: str = args.src
        self.trg: str = args.trg
        self.seed: int = args.seed
        self.train_set_prefixes: list[str] = args.train_set_prefixes.split(",")
        self.alignments_files: list[Path] = [
            Path(path) for path in args.alignments.split(",") if path != "None"
        ]
        self.validation_set_prefix: str = args.validation_set_prefix
        self.artifacts: Path = args.artifacts
        self.model_type: ModelType = args.model_type
        self.student_model: StudentModel = args.student_model
        self.teacher_mode: TeacherMode = args.teacher_mode
        self.training_type: TrainingType = args.training_type
        self.best_model_metric: BestModelMetric = args.best_model_metric
        self.extra_marian_args: list[str] = args.extra_marian_args
        self.marian_bin = args.marian_dir / "marian"
        self.gpus = args.gpus
        self.workspace = args.workspace
        self.config_variables = {
            "vocab": self.vocab,
            "src": self.src,
            "trg": self.trg,
            "seed": self.seed,
        }
        self.opustrainer_config = self.artifacts / "config.opustrainer.yml"

    def log_config(self):
        logger.info("Running train.py with the following settings:")
        logger.info(f" - temp_dir: {self.temp_dir}")
        logger.info(f" - vocab: {self.vocab}")
        logger.info(f" - src: {self.src}")
        logger.info(f" - trg: {self.trg}")
        logger.info(f" - seed: {self.seed}")
        logger.info(f" - train_set_prefixes: {self.train_set_prefixes}")
        logger.info(f" - alignments_files: {self.alignments_files}")
        logger.info(f" - validation_set_prefix: {self.validation_set_prefix}")
        logger.info(f" - artifacts: {self.artifacts}")
        logger.info(f" - model_type: {self.model_type.value}")
        logger.info(f" - student_model: {self.student_model.value}")
        logger.info(f" - teacher_mode: {self.teacher_mode.value}")
        logger.info(f" - training_type: {self.training_type.value}")
        logger.info(f" - best_model_metric: {self.best_model_metric}")
        logger.info(f" - extra_marian_args: {self.extra_marian_args}")
        logger.info(f" - marian_bin: {self.marian_bin}")
        logger.info(f" - gpus: {self.gpus}")
        logger.info(f" - workspace: {self.workspace}")
        logger.info(f" - opustrainer_config: {self.opustrainer_config}")

    def validate_args(self) -> None:
        if self.extra_marian_args and self.extra_marian_args[0] != "--":
            logger.error(" ".join(self.extra_marian_args))
            raise Exception("Expected the extra marian args to be after a --")

        # Validate input values.
        if not self.vocab.exists():
            raise Exception("Could not find the path to the vocab.")

        if not self.marian_bin.exists():
            raise Exception(f"Marian binary could not be found {self.marian_bin}")

        for alignment_file in self.alignments_files:
            if not alignment_file.exists():
                raise Exception(f"Alignment file could not be found {alignment_file}")

    def build_datasets(self) -> None:
        # Start by building the training datasets, e.g.
        #
        #  corpus.enfr.tsv from:
        #   - fetches/corpus.en.zst
        #   - fetches/corpus.fr.zst
        #   - fetches/corpus.aln.zst
        #
        #  mono.enfr.tsv from:
        #   - fetches/mono.en.zst
        #   - fetches/mono.fr.zst
        #   - fetches/mono.aln.zst

        for index, dataset_prefix in enumerate(self.train_set_prefixes):
            alignments = None
            if self.alignments_files:
                alignments = self.alignments_files[index]
                self.config_variables[f"dataset{index}"] = build_dataset_tsv(
                    dataset_prefix, self.src, self.trg, alignments
                )
            else:
                self.config_variables[f"dataset{index}"] = build_dataset_tsv(
                    dataset_prefix, self.src, self.trg
                )

        # Then build out the validation set, for instance:
        #
        # devset.enfr.tsv from:
        #  - fetches/devset.en.zst
        #  - fetches/devset.fr.zst
        self.validation_set = build_dataset_tsv(self.validation_set_prefix, self.src, self.trg)

    def generate_opustrainer_config(
        self,
    ):
        """
        Generate an OpusTraininer config that points to the current datasets and language
        options.
        """

        config_suffix = "cjk.yml" if self.src in CJK_LANGS or self.trg in CJK_LANGS else "yml"

        if self.model_type == ModelType.teacher:
            teacher_mode = self.teacher_mode.value
            if teacher_mode == TeacherMode.none.value:
                raise ValueError("Teacher mode was not properly set, as it was set to none")

            config_input = (
                train_dir
                / f"configs/opustrainer/{self.model_type.value}.{teacher_mode}.{config_suffix}"
            )
        else:
            config_input = (
                train_dir / f"configs/opustrainer/{self.model_type.value}.{config_suffix}"
            )

        with open(config_input, "rt", encoding="utf-8") as file:
            config_text = file.read()

        logger.info(f"Applying OpusTrainer config variables: {config_input}")
        for key, value in self.config_variables.items():
            logger.info(f" - {key}: {value}")

        with self.opustrainer_config.open("wt", encoding="utf-8") as file:
            config_text = config_text.format(**self.config_variables)
            file.write(config_text)

    def get_opustrainer_cmd(self):
        return [
            "opustrainer-train",
            *apply_command_args(
                {
                    "config": self.opustrainer_config,
                    "log-file": self.artifacts / "opustrainer.log",
                    "log-level": "INFO",
                }
            ),
        ]

    def get_marian_cmd(self):
        all_model_metrics = ["chrf", "ce-mean-words", "bleu-detok"]
        validation_metrics = [
            # Place the best model metric first.
            self.best_model_metric.value,
            # And then the rest of the metrics should follow.
            *[m for m in all_model_metrics if m != self.best_model_metric.value],
        ]

        # Take off the "--" from beginning of the list.
        extra_args = self.extra_marian_args[1:]

        if "USE_CPU" not in os.environ:
            # We run a CPU version of Marian in tests and it does not work with these arguments.
            extra_args.append("--sharding")
            extra_args.append("local")

        if self.model_type == ModelType.student:
            if self.student_model == StudentModel.none:
                raise ValueError("Student configuration is not provided")
            model_name = f"student.{self.student_model.value}"
        else:
            model_name = self.model_type.value

        return [
            str(self.marian_bin),
            *apply_command_args(
                {
                    "model": self.artifacts / "model.npz",
                    "config": [
                        train_dir / f"configs/model/{model_name}.yml",
                        train_dir
                        / f"configs/training/{self.model_type.value}.{self.training_type.value}.yml",
                    ],
                    "tempdir": self.temp_dir / "marian-tmp",
                    "vocabs": [self.vocab, self.vocab],
                    "workspace": self.workspace,
                    "devices": self.gpus.split(" "),
                    "valid-metrics": validation_metrics,
                    "valid-sets": str(self.validation_set),
                    "valid-translation-output": self.artifacts / "devset.out",
                    "valid-log": self.artifacts / "valid.log",
                    "log": self.artifacts / "train.log",
                    "shuffle": "batches",
                    "seed": str(self.seed),
                    "no-restore-corpus": None,
                    "valid-reset-stalled": None,
                    "sync-sgd": None,
                    "quiet-translation": None,
                    "overwrite": None,
                    "keep-best": None,
                    "tsv": None,
                }
            ),
            *extra_args,
        ]

    def run_training(self):
        """
        OpusTrainer pipes augmented data into Marian. Marian handles the training and
        outputs the progress as in its log. The final part of the pipeline is the log
        parser which parses the streamed logs and reports the results to W&B.
        """
        run_command_pipeline(
            [
                [
                    # OpusTrainer controls the marian commands.
                    *self.get_opustrainer_cmd(),
                    *self.get_marian_cmd(),
                ],
                get_log_parser_command(),
            ],
            pipe_stderr=True,
            logger=logger,
        )

        shutil.copy(
            self.artifacts / f"model.npz.best-{self.best_model_metric.value}.npz",
            self.artifacts / f"final.model.npz.best-{self.best_model_metric.value}.npz",
        )
        shutil.copy(
            self.artifacts / f"model.npz.best-{self.best_model_metric.value}.npz.decoder.yml",
            self.artifacts
            / f"final.model.npz.best-{self.best_model_metric.value}.npz.decoder.yml",
        )


def main() -> None:
    parser = argparse.ArgumentParser(
        description=__doc__,
        # Preserves whitespace in the help text.
        formatter_class=argparse.RawTextHelpFormatter,
    )

    parser.add_argument(
        "--model_type",
        type=ModelType,
        choices=ModelType,
        required=True,
        help="The type of model to train",
    )
    parser.add_argument(
        "--student_model",
        type=StudentModel,
        choices=StudentModel,
        required=False,
        default=StudentModel.tiny,
        help="Type of student model",
    )
    parser.add_argument(
        "--training_type",
        type=TrainingType,
        choices=TrainingType,
        help="Type of teacher training",
    )
    parser.add_argument(
        "--gpus",
        type=str,
        required=True,
        help='The indexes of the GPUs to use on a system, e.g. --gpus "0 1 2 3"',
    )
    parser.add_argument(
        "--marian_dir",
        type=Path,
        required=True,
        help="Path to Marian binary directory. This allows for overriding to use the browser-mt fork.",
    )
    parser.add_argument(
        "--workspace",
        type=str,
        required=True,
        help="The amount of Marian memory (in MB) to preallocate",
    )
    parser.add_argument("--src", type=str, help="Source language")
    parser.add_argument("--trg", type=str, help="Target language")
    parser.add_argument(
        "--train_set_prefixes",
        type=str,
        help="Comma separated prefixes to datasets for curriculum learning",
    )
    parser.add_argument("--validation_set_prefix", type=str, help="Prefix to validation dataset")
    parser.add_argument("--artifacts", type=Path, help="Where to save the model artifacts")
    parser.add_argument("--vocab", type=Path, help="Path to vocab file")
    parser.add_argument(
        "--best_model_metric",
        type=BestModelMetric,
        help="Multiple metrics are gathered, but only the best model for a given metric will be retained",
    )
    parser.add_argument(
        "--alignments",
        type=str,
        help="Comma separated alignment paths corresponding to each training dataset, or 'None' to train without alignments",
    )
    parser.add_argument("--seed", type=int, help="Random seed")
    parser.add_argument(
        "--teacher_mode",
        type=TeacherMode,
        choices=TeacherMode,
        help="Teacher mode",
    )
    parser.add_argument(
        "extra_marian_args",
        nargs=argparse.REMAINDER,
        help="Additional parameters for the training script",
    )

    with tempfile.TemporaryDirectory() as temp_dir:
        train_cli = TrainCLI(parser.parse_args(), Path(temp_dir))
        train_cli.log_config()
        train_cli.validate_args()
        train_cli.build_datasets()
        train_cli.generate_opustrainer_config()
        train_cli.run_training()


if __name__ == "__main__":
    main()


translate/collect.sh

#!/bin/bash
#
# Collects chunked translation data of the form "file.N.out" where N is a number.
# The datasets are chunked earlier in the pipeline by split-{corpus,mono}.sh so that
# tasks can work on smaller sets of data to better parallelize the work. After processing,
# any chunked data is reassembled with this script.
#
# Example tasks running on chunked data (before this script):
#   extract-best-en-ca-1/10
#   translate-corpus-en-ca-1/10
#   translate-mono-src-en-ca-1/10
#   translate-mono-trg-en-ca-1/10
#
# Kinds:
#   taskcluster/ci/collect-mono-trg/kind.yml
#   taskcluster/ci/collect-mono-src/kind.yml
#   taskcluster/ci/collect-corpus/kind.yml
#
# Example usage:
#
#   pipeline/translate/collect.sh    \
#      fetches                       \
#      artifacts/mono.en.zst         \
#      $MOZ_FETCHES_DIR/mono.ca.zst

set -x
set -euo pipefail

# The directory with chunks of the form "fetches/file.N.out", where N is a number.
chunks_dir=$1
# The full file name path to the output compressed file, e.g. "artifacts/mono.en.zst"
output_path=$2
# The path to the monolingual data to compare against, e.g. "$MOZ_FETCHES_DIR/mono.hu.zst"
mono_path=$3


echo "### Collecting translations"

find "${chunks_dir}" -name '*.out' |  # For example, finds "fetches/file.1.out", "fetches/file.2.out", etc.
  sort -t '.' -k2,2n |                      # Sort by the number in "file.1.out", e.g. 1 here.
  xargs cat |                               # Combine all of these files together.
  zstdmt >"${output_path}"

echo "### Comparing number of sentences in source and artificial target files"

src_len=$(zstdmt -dc "${mono_path}" | wc -l)
trg_len=$(zstdmt -dc "${output_path}" | wc -l)

if [ "${src_len}" != "${trg_len}" ]; then
  echo "### Error: length of ${mono_path} ${src_len} is different from ${output_path} ${trg_len}"
  exit 1
fi


translate/extract_best.py

#!/usr/bin/env python3
# -*- coding: utf-8 -*-

from __future__ import absolute_import, division, print_function

import argparse
import collections
import math
import re
import sys


def main():
    args = parse_args()

    if args.metric == "bleu":
        score_function = compute_bleu
    elif args.metric == "sacrebleu":
        global sacrebleu
        import sacrebleu

        score_function = compute_sacrebleu
    elif args.metric == "chrf":
        global sacrebleu
        import sacrebleu

        score_function = compute_chrf
    else:
        sys.stderr.write("Unrecognized metric: {}\n".format(args.metric))
        pass

    if args.toolkit == "marian":
        marian_best_bleu(args, score_function)
    elif args.toolkit == "t2t":
        t2t_best_bleu(args, score_function)
        pass


def t2t_best_bleu(args, score_function):
    for i, ref_line in enumerate(args.references):
        refs = ref_line.strip().split("\n")
        if args.debpe:
            refs = [re.sub(r"@@ +", "", r) for r in refs]
            pass
        texts = next(args.nbest).strip().split("\t")
        if args.debpe:
            texts = [re.sub(r"@@ +", "", t) for t in texts]
            pass
        refs = [r.split() for r in refs]
        scores = [score_function(refs, t.split()) for t in texts]
        best_txt = texts[scores.index(max(scores))]

        args.output.write("{}\n".format(best_txt))
        if args.debug:
            sys.stderr.write("{}: {}\n".format(i, scores))
            pass
        if i % 100000 == 0 and i > 0:
            sys.stderr.write("[{}]\n".format(i))
            pass
        pass


def marian_best_bleu(args, score_function):
    prev_line = None
    for i, ref_line in enumerate(args.references):
        refs = ref_line.strip().split("\n")
        if args.debpe:
            refs = [re.sub(r"@@ +", "", r) for r in refs]

        texts = []
        while True:
            if prev_line:
                # CTranslate2 can output empty text, for example:
                # 10181 ||| .GDFMAKERPROJECTファイルを開くには?
                # 10181 ||| .GDMAKERPROJECTファイルを開くには?
                # 10181 ||| .GDFMAKERPROJECTファイルを開くには?
                # 10181 ||| .GDFMakerPROJECTファイルを開くには?
                # 10181 ||| .GDFAKERPROJECTファイルを開くには?
                # 10181 ||| .GDMakerPROJECTファイルを開くには?
                # 10181 ||| .GDFMAKERPROJECTファイルを開くには。
                # 10181 |||
                # Marian also outputs scores, for example:
                # 0 ||| Реформа, направленная на выдвижение условий, идет слишком медленно. ||| F0= -9.21191 F1= -11.53 ||| -1.22059
                fields = prev_line.rstrip("\n").split(" ||| ")
                if len(fields) == 1:
                    # handle "10181 |||"
                    fields = fields[0].split()[0], ""

                idx = int(fields[0])
                if idx == i:
                    texts.append(fields[1])
                else:
                    break

            prev_line = next(args.nbest, None)
            if not prev_line:
                break

        if args.debpe:
            texts = [re.sub(r"@@ +", "", t) for t in texts]
        refs = [r.split() for r in refs]
        scores = [score_function(refs, t.split()) for t in texts]
        best_txt = texts[scores.index(max(scores))]

        args.output.write("{}\n".format(best_txt))
        if args.debug:
            sys.stderr.write("{}: {}\n".format(i, scores))

        if i % 100000 == 0 and i > 0:
            sys.stderr.write("[{}]\n".format(i))


def compute_chrf(references, translation):
    hypo = " ".join(translation)
    refs = [" ".join(r) for r in references]
    return sacrebleu.sentence_chrf(hypo, refs).score


def compute_sacrebleu(references, translation):
    hypo = " ".join(translation)
    refs = [" ".join(r) for r in references]
    return sacrebleu.sentence_bleu(hypo, refs).score


def compute_bleu(references, translation, max_order=4):
    precisions = get_ngram_precisions(references, translation, max_order)
    if min(precisions) > 0:
        p_log_sum = sum((1.0 / max_order) * math.log(p) for p in precisions)
        geo_mean = math.exp(p_log_sum)
    else:
        geo_mean = 0

    bp = get_brevity_penalty(references, translation)
    return geo_mean * bp


def get_brevity_penalty(references, translation):
    reference_length = min(len(r) for r in references)
    translation_length = len(translation)
    ratio = float(translation_length) / reference_length
    if ratio > 1.0 or ratio == 0.0:
        bp = 1.0
    else:
        bp = math.exp(1 - 1.0 / ratio)
    return bp


def get_ngram_precisions(references, translation, max_order=4):
    matches_by_order = [0] * max_order
    possible_matches_by_order = [0] * max_order

    merged_ref_ngram_counts = collections.Counter()
    for reference in references:
        merged_ref_ngram_counts |= get_ngrams(reference, max_order)
    translation_ngram_counts = get_ngrams(translation, max_order)
    overlap = translation_ngram_counts & merged_ref_ngram_counts
    for ngram in overlap:
        matches_by_order[len(ngram) - 1] += overlap[ngram]
    for order in range(1, max_order + 1):
        possible_matches = len(translation) - order + 1
        if possible_matches > 0:
            possible_matches_by_order[order - 1] += possible_matches

    precisions = [0] * max_order
    for i in range(0, max_order):
        # smoothing
        if matches_by_order[i] == 0 and possible_matches_by_order[i] == 0:
            precisions[i] = 0.0
        else:
            precisions[i] = (matches_by_order[i] + 1.0) / (possible_matches_by_order[i] + 1.0)
    return precisions


def get_ngrams(segment, max_order):
    ngram_counts = collections.Counter()
    for order in range(1, max_order + 1):
        for i in range(0, len(segment) - order + 1):
            ngram = tuple(segment[i : i + order])
            ngram_counts[ngram] += 1
    return ngram_counts


def parse_args():
    from argparse import FileType

    parser = argparse.ArgumentParser()
    parser.add_argument("-i", "--nbest", type=FileType("r"), default=sys.stdin)
    parser.add_argument("-r", "--references", type=FileType("r"), required=True)
    parser.add_argument("-o", "--output", type=FileType("w"), default=sys.stdout)
    parser.add_argument("-m", "--metric", default="bleu")
    parser.add_argument("--debpe", action="store_true")
    parser.add_argument("-d", "--debug", action="store_true")
    parser.add_argument("-t", "--toolkit", default="marian", help="Toolkit: 'marian' or 't2t'")
    return parser.parse_args()


if __name__ == "__main__":
    main()


translate/merge-corpus.sh

#!/bin/bash
##
# Merges datasets with shuffling.
#
# Kinds:
#   taskcluster/kinds/merge-translated/kind.yml

set -x
set -euo pipefail

test -v BIN

# https://stackoverflow.com/questions/41962359/shuffling-numbers-in-bash-using-seed
# Deterministic shuffling
get_seeded_random()
{
  seed="$1"
  openssl enc -aes-256-ctr -pass pass:"$seed" -nosalt \
    </dev/zero 2>/dev/null
}


echo "###### Merging datasets"

src1=$1
src2=$2
trg1=$3
trg2=$4
res_src=$5
res_trg=$6

tmp_dir="$(dirname "${res_src}")/tmp"
mkdir -p "${tmp_dir}"

cat <(zstdmt -dc "${src1}") <(zstdmt -dc "${src2}") | zstdmt >"${tmp_dir}/original.src.zst"
cat <(zstdmt -dc "${trg1}") <(zstdmt -dc "${trg2}") | zstdmt >"${tmp_dir}/original.trg.zst"

# De-duplicating uses dedupe from: https://github.com/kpu/preprocess
#
# This utility deduplicates based on the 64-bit hash of the entire sentence pair, or
# on one side of the sentence pair. The first encountered line is kept, duplicates later
# on are removed.
#
#  Deduplication settings:
#    -f [ --fields ] arg (=1-) Fields to use for key like cut -f
#    -d [ --delim ] arg (=	)   Field delimiter
#    -p [ --parallel ] arg     Filter parallel data using four files: in_en in_fr
#                              out_en out_fr
#  Deduplicate lines in a file: ./dedupe <in >out
#  Deduplicate parallel data, removing if either side is non-unique ./bin/dedupe -p in_en in_fr out_en out_fr

echo "#### Deduplicating"
paste <(zstdmt -dc "${tmp_dir}/original.src.zst") <(zstdmt -dc "${tmp_dir}/original.trg.zst") |
  shuf --random-source=<(get_seeded_random 42) |
  ${BIN}/dedupe |
  zstdmt > "${tmp_dir}/all.zst"

zstdmt -dc "${tmp_dir}/all.zst" | cut -f1 | zstdmt > "${res_src}"
zstdmt -dc "${tmp_dir}/all.zst" | cut -f2 | zstdmt > "${res_trg}"

src_len=$(zstdmt -dc "${res_src}" | wc -l)
trg_len=$(zstdmt -dc "${res_trg}" | wc -l)
if [ "${src_len}" != "${trg_len}" ]; then
  echo "Error: length of ${res_src} ${src_len} is different from ${res_trg} ${trg_len}"
  exit 1
fi

rm -rf "${tmp_dir}"

echo "###### Done: Merging datasets"


translate/requirements/extract_best.in

sacrebleu[ja,ko]==2.4.2


translate/requirements/splitter.in

requests==2.31.0


translate/requirements/translate-ctranslate2.in

ctranslate2==4.3.1
sentencepiece==0.2.0
gpustat==1.1.1


translate/splitter.py

#!/usr/bin/env python3
"""
Splits a dataset to chunks. Generates files in format file.00.zst, file.01.zst etc.

Example:
    python splitter.py \
        --output_dir=test_data \
        --num_parts=10 \
        --output_suffix=.ref \
        test_data/corpus.en.zst
"""

import argparse
import os
from contextlib import ExitStack
from typing import Optional

from pipeline.common.downloads import count_lines, read_lines, write_lines
from pipeline.common.logging import get_logger

logger = get_logger(__file__)


def split_file(mono_path: str, output_dir: str, num_parts: int, output_suffix: str = ""):
    """
    Split a file into fixed number of chunks.

    For instance with:

        mono_path     = "corpus.en.zst"
        output_dir    = "artifacts"
        num_parts     = 20
        output_suffix = ".ref"

    Outputs:
        .
        ├── corpus.en.zst
        └── artifacts
            ├── file.1.ref.zst
            ├── file.2.ref.zst
            ├── file.3.ref.zst
            ├── ...
            └── file.20.ref.zst
    """
    os.makedirs(output_dir, exist_ok=True)

    total_lines = count_lines(mono_path)
    lines_per_part = (total_lines + num_parts - 1) // num_parts
    logger.info(f"Splitting {mono_path} to {num_parts} chunks x {total_lines:,} lines")

    line_writer = None
    line_count = 0
    file_index = 1

    with read_lines(mono_path) as lines:
        with ExitStack() as chunk_stack:
            for line in lines:
                if not line_writer or line_count >= lines_per_part:
                    # The current file is full or doesn't exist, start a new one.
                    if line_writer:
                        chunk_stack.close()

                    chunk_name = f"{output_dir}/file.{file_index}{output_suffix}.zst"
                    logger.info(f"Writing to file chunk: {chunk_name}")
                    line_writer = chunk_stack.enter_context(write_lines(chunk_name))
                    file_index += 1
                    line_count = 0

                line_writer.write(line)
                line_count += 1

    logger.info("Done writing to files.")


def main(args: Optional[list[str]] = None) -> None:
    parser = argparse.ArgumentParser(
        description=__doc__,
        formatter_class=argparse.RawTextHelpFormatter,  # Preserves whitespace in the help text.
    )
    parser.add_argument("mono_path", type=str, help="Path to the compressed monolingual dataset")
    parser.add_argument("--output_dir", type=str, help="Output directory to store split files")
    parser.add_argument("--num_parts", type=int, help="Number of parts to split the file into")
    parser.add_argument(
        "--output_suffix", type=str, help="A suffix for output files, for example .ref", default=""
    )

    parsed_args = parser.parse_args(args)

    split_file(
        mono_path=parsed_args.mono_path,
        output_dir=parsed_args.output_dir,
        num_parts=parsed_args.num_parts,
        output_suffix=parsed_args.output_suffix,
    )


if __name__ == "__main__":
    main()


translate/translate.py

"""
Translate a corpus using either Marian or CTranslate2.
"""

import argparse
from enum import Enum
from glob import glob
import os
from pathlib import Path
import sys
import tempfile

from pipeline.common.command_runner import apply_command_args, run_command
from pipeline.common.datasets import compress, decompress
from pipeline.common.downloads import count_lines, is_file_empty, write_lines
from pipeline.common.logging import (
    get_logger,
    start_gpu_logging,
    start_byte_count_logger,
    stop_gpu_logging,
    stop_byte_count_logger,
)
from pipeline.common.marian import get_combined_config
from pipeline.translate.translate_ctranslate2 import translate_with_ctranslate2

logger = get_logger(__file__)

DECODER_CONFIG_PATH = Path(__file__).parent / "decoder.yml"


class Decoder(Enum):
    marian = "marian"
    ctranslate2 = "ctranslate2"


class Device(Enum):
    cpu = "cpu"
    gpu = "gpu"


def get_beam_size(extra_marian_args: list[str]):
    return get_combined_config(DECODER_CONFIG_PATH, extra_marian_args)["beam-size"]


def run_marian(
    marian_dir: Path,
    models: list[Path],
    vocab: str,
    input: Path,
    output: Path,
    gpus: list[str],
    workspace: int,
    is_nbest: bool,
    extra_args: list[str],
):
    config = Path(__file__).parent / "decoder.yml"
    marian_bin = str(marian_dir / "marian-decoder")
    log = input.parent / f"{input.name}.log"
    if is_nbest:
        extra_args = ["--n-best", *extra_args]

    logger.info("Starting Marian to translate")

    run_command(
        [
            marian_bin,
            *apply_command_args(
                {
                    "config": config,
                    "models": models,
                    "vocabs": [vocab, vocab],
                    "input": input,
                    "output": output,
                    "log": log,
                    "devices": gpus,
                    "workspace": workspace,
                }
            ),
            *extra_args,
        ],
        logger=logger,
        env={**os.environ},
    )


def main() -> None:
    parser = argparse.ArgumentParser(
        description=__doc__,
        # Preserves whitespace in the help text.
        formatter_class=argparse.RawTextHelpFormatter,
    )

    parser.add_argument(
        "--input", type=Path, required=True, help="The path to the text to translate."
    )
    parser.add_argument(
        "--models_glob",
        type=str,
        required=True,
        nargs="+",
        help="A glob pattern to the Marian model(s)",
    )
    parser.add_argument(
        "--artifacts", type=Path, required=True, help="Output path to the artifacts."
    )
    parser.add_argument("--nbest", action="store_true", help="Whether to use the nbest")
    parser.add_argument(
        "--marian_dir", type=Path, required=True, help="The path the Marian binaries"
    )
    parser.add_argument("--vocab", type=Path, help="Path to vocab file")
    parser.add_argument(
        "--gpus",
        type=str,
        required=True,
        help='The indexes of the GPUs to use on a system, e.g. --gpus "0 1 2 3"',
    )
    parser.add_argument(
        "--workspace",
        type=str,
        required=True,
        help="The amount of Marian memory (in MB) to preallocate",
    )
    parser.add_argument(
        "--decoder",
        type=Decoder,
        default=Decoder.marian,
        help="Either use the normal marian decoder, or opt for CTranslate2.",
    )
    parser.add_argument(
        "--device",
        type=Device,
        default=Device.gpu,
        help="Either use the normal marian decoder, or opt for CTranslate2.",
    )
    parser.add_argument(
        "extra_marian_args",
        nargs=argparse.REMAINDER,
        help="Additional parameters for the training script",
    )

    args = parser.parse_args()

    # Provide the types for the arguments.
    marian_dir: Path = args.marian_dir
    input_zst: Path = args.input
    artifacts: Path = args.artifacts
    models_globs: list[str] = args.models_glob
    models: list[Path] = []
    for models_glob in models_globs:
        for path in glob(models_glob):
            models.append(Path(path))
    postfix = "nbest" if args.nbest else "out"
    output_zst = artifacts / f"{input_zst.stem}.{postfix}.zst"
    vocab: Path = args.vocab
    gpus: list[str] = args.gpus.split(" ")
    extra_marian_args: list[str] = args.extra_marian_args
    decoder: Decoder = args.decoder
    is_nbest: bool = args.nbest
    device: Device = args.device

    # Do some light validation of the arguments.
    assert input_zst.exists(), f"The input file exists: {input_zst}"
    assert vocab.exists(), f"The vocab file exists: {vocab}"
    if not artifacts.exists():
        artifacts.mkdir()
    for gpu_index in gpus:
        assert gpu_index.isdigit(), f'GPUs must be list of numbers: "{gpu_index}"'
    assert models, "There must be at least one model"
    for model in models:
        assert model.exists(), f"The model file exists {model}"
    if extra_marian_args and extra_marian_args[0] != "--":
        logger.error(" ".join(extra_marian_args))
        raise Exception("Expected the extra marian args to be after a --")

    logger.info(f"Input file: {input_zst}")
    logger.info(f"Output file: {output_zst}")

    # Taskcluster can produce empty input files when chunking out translation for
    # parallelization. In this case skip translating, and write out an empty file.
    if is_file_empty(input_zst):
        logger.info(f"The input is empty, create a blank output: {output_zst}")
        with write_lines(output_zst) as _outfile:
            # Nothing to write, just create the file.
            pass
        return

    if decoder == Decoder.ctranslate2:
        translate_with_ctranslate2(
            input_zst=input_zst,
            artifacts=artifacts,
            extra_marian_args=extra_marian_args,
            models_globs=models_globs,
            is_nbest=is_nbest,
            vocab=[str(vocab)],
            device=device.value,
            device_index=[int(n) for n in gpus],
        )
        return

    # The device flag is for use with CTranslate, but add some assertions here so that
    # we can be consistent in usage.
    if device == Device.cpu:
        assert (
            "--cpu-threads" in extra_marian_args
        ), "Marian's cpu should be controlled with the flag --cpu-threads"
    else:
        assert (
            "--cpu-threads" not in extra_marian_args
        ), "Requested a GPU device, but --cpu-threads was provided"

    # Run the training.
    with tempfile.TemporaryDirectory() as temp_dir_str:
        temp_dir = Path(temp_dir_str)
        input_txt = temp_dir / input_zst.stem
        output_txt = temp_dir / output_zst.stem

        decompress(input_zst, destination=input_txt, remove=True, logger=logger)

        five_minutes = 300
        if device == Device.gpu:
            start_gpu_logging(logger, five_minutes)
        start_byte_count_logger(logger, five_minutes, output_txt)

        run_marian(
            marian_dir=marian_dir,
            models=models,
            vocab=vocab,
            input=input_txt,
            output=output_txt,
            gpus=gpus,
            workspace=args.workspace,
            is_nbest=is_nbest,
            # Take off the initial "--"
            extra_args=extra_marian_args[1:],
        )

        stop_gpu_logging()
        stop_byte_count_logger()

        compress(output_txt, destination=output_zst, remove=True, logger=logger)

        input_count = count_lines(input_txt)
        output_count = count_lines(output_zst)
        if is_nbest:
            beam_size = get_beam_size(extra_marian_args)
            expected_output = input_count * beam_size
            assert (
                expected_output == output_count
            ), f"The nbest output had {beam_size}x as many lines ({expected_output} vs {output_count})"
        else:
            assert (
                input_count == output_count
            ), f"The input ({input_count} and output ({output_count}) had the same number of lines"


if __name__ == "__main__":
    try:
        main()
    except RuntimeError as e:
        # On GCP instances, we occasionally find that a GPU is not found even
        # when it has been requested. Exiting with a unique error code in these
        # cases allows us to automatically retry such tasks in Taskcluster.
        if len(e.args) > 0 and "no CUDA-capable device is detected" in e.args[0]:
            logger.exception("couldn't find GPU, exiting with 9002")
            sys.exit(9002)


translate/translate_ctranslate2.py

"""
Translate a corpus with a teacher model (transformer-based) using CTranslate2. This is useful
to quickly synthesize training data for student distillation as CTranslate2 is ~2 times faster
than Marian. For a more detailed analysis see: https://github.com/mozilla/translations/issues/931

https://github.com/OpenNMT/CTranslate2/
"""

from typing import Any, TextIO
from enum import Enum
from glob import glob
from pathlib import Path

import ctranslate2
import sentencepiece as spm
from ctranslate2.converters.marian import MarianConverter

from pipeline.common.downloads import read_lines, write_lines
from pipeline.common.logging import (
    get_logger,
    start_gpu_logging,
    start_byte_count_logger,
    stop_gpu_logging,
    stop_byte_count_logger,
)
from pipeline.common.marian import get_combined_config


def load_vocab(path: str):
    logger.info("Loading vocab:")
    logger.info(path)
    sp = spm.SentencePieceProcessor(path)

    return [sp.id_to_piece(i) for i in range(sp.vocab_size())]


# The vocab expects a .yml file. Instead directly load the vocab .spm file via a monkey patch.
if not ctranslate2.converters.marian.load_vocab:
    raise Exception("Expected to be able to monkey patch the load_vocab function")
ctranslate2.converters.marian.load_vocab = load_vocab

logger = get_logger(__file__)


class Device(Enum):
    gpu = "gpu"
    cpu = "cpu"


class MaxiBatchSort(Enum):
    src = "src"
    none = "none"


def get_model(models_globs: list[str]) -> Path:
    models: list[Path] = []
    for models_glob in models_globs:
        for path in glob(models_glob):
            models.append(Path(path))
    if not models:
        raise ValueError(f'No model was found with the glob "{models_glob}"')
    if len(models) != 1:
        logger.info(f"Found models {models}")
        raise ValueError("Ensemble training is not supported in CTranslate2")
    return Path(models[0])


class DecoderConfig:
    def __init__(self, extra_marian_args: list[str]) -> None:
        super().__init__()
        # Combine the two configs.
        self.config = get_combined_config(Path(__file__).parent / "decoder.yml", extra_marian_args)

        self.mini_batch_words: int = self.get_from_config("mini-batch-words", int)
        self.beam_size: int = self.get_from_config("beam-size", int)
        self.precision = self.get_from_config("precision", str, "float32")
        if self.get_from_config("fp16", bool, False):
            self.precision = "float16"

    def get_from_config(self, key: str, type: any, default=None):
        value = self.config.get(key, default)
        if value is None:
            raise ValueError(f'"{key}" could not be found in the decoder.yml config')
        if isinstance(value, type):
            return value
        if type != str and isinstance(value, str):
            return type(value)
        raise ValueError(f'Expected "{key}" to be of a type "{type}" in the decoder.yml config')


def write_single_translation(
    _index: int, tokenizer_trg: spm.SentencePieceProcessor, result: Any, outfile: TextIO
):
    """
    Just write each single translation to a new line. If beam search was used all the other
    beam results are discarded.
    """
    line = tokenizer_trg.decode(result.hypotheses[0])
    outfile.write(line)
    outfile.write("\n")


def write_nbest_translations(
    index: int, tokenizer_trg: spm.SentencePieceProcessor, result: Any, outfile: TextIO
):
    """
    Match Marian's way of writing out nbest translations. For example, with a beam-size of 2 and
    collection nbest translations:

    0 ||| Translation attempt
    0 ||| An attempt at translation
    1 ||| The quick brown fox jumped
    1 ||| The brown fox quickly jumped
    ...
    """
    for hypothesis in result.hypotheses:
        line = tokenizer_trg.decode(hypothesis)
        outfile.write(f"{index} ||| {line}\n")


def translate_with_ctranslate2(
    input_zst: Path,
    artifacts: Path,
    extra_marian_args: list[str],
    models_globs: list[str],
    is_nbest: bool,
    vocab: list[str],
    device: str,
    device_index: list[int],
) -> None:
    model = get_model(models_globs)
    postfix = "nbest" if is_nbest else "out"

    tokenizer_src = spm.SentencePieceProcessor(vocab[0])
    if len(vocab) == 1:
        tokenizer_trg = tokenizer_src
    else:
        tokenizer_trg = spm.SentencePieceProcessor(vocab[1])

    if extra_marian_args and extra_marian_args[0] != "--":
        logger.error(" ".join(extra_marian_args))
        raise Exception("Expected the extra marian args to be after a --")

    decoder_config = DecoderConfig(extra_marian_args[1:])

    ctranslate2_model_dir = model.parent / f"{Path(model).stem}"
    logger.info("Converting the Marian model to Ctranslate2:")
    logger.info(model)
    logger.info("Outputing model to:")
    logger.info(ctranslate2_model_dir)

    converter = MarianConverter(model, vocab)
    converter.convert(ctranslate2_model_dir, quantization=decoder_config.precision)

    if device == "gpu":
        translator = ctranslate2.Translator(
            str(ctranslate2_model_dir), device="cuda", device_index=device_index
        )
    else:
        translator = ctranslate2.Translator(str(ctranslate2_model_dir), device="cpu")

    logger.info("Loading model")
    translator.load_model()
    logger.info("Model loaded")

    output_zst = artifacts / f"{input_zst.stem}.{postfix}.zst"

    num_hypotheses = 1
    write_translation = write_single_translation
    if is_nbest:
        num_hypotheses = decoder_config.beam_size
        write_translation = write_nbest_translations

    def tokenize(line):
        return tokenizer_src.Encode(line.strip(), out_type=str)

    five_minutes = 300
    if device == "gpu":
        start_gpu_logging(logger, five_minutes)
    start_byte_count_logger(logger, five_minutes, output_zst)

    index = 0
    with write_lines(output_zst) as outfile, read_lines(input_zst) as lines:
        for result in translator.translate_iterable(
            # Options for "translate_iterable":
            # https://opennmt.net/CTranslate2/python/ctranslate2.Translator.html#ctranslate2.Translator.translate_iterable
            map(tokenize, lines),
            max_batch_size=decoder_config.mini_batch_words,
            batch_type="tokens",
            # Options for "translate_batch":
            # https://opennmt.net/CTranslate2/python/ctranslate2.Translator.html#ctranslate2.Translator.translate_batch
            beam_size=decoder_config.beam_size,
            return_scores=False,
            num_hypotheses=num_hypotheses,
        ):
            write_translation(index, tokenizer_trg, result, outfile)
            index += 1

    stop_gpu_logging()
    stop_byte_count_logger()
